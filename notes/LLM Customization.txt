 [ Inaudible Remarks ]
 If you have any trouble, please once again reach out.
 So we have a -- you see those cups on the table.
 There's a blue cup and a red cup.
 They're not for drinking.
 If you have a question or something isn't working,
 please place your red cup on the table and one
 of the teas will come and find you.
 So once the issue is resolved, switch it back to blue.
 Lunch will be served between 11.30 and 1.30 and David will let us know
 when it's time for us to go have lunch.
 [ Inaudible Remarks ]
 Towards the end of the day, we will appreciate it
 if you could fill out the chart survey, it will take more than ten minutes.
 We'll let you know at 4.30 when you do that.
 So you will have access to all the course materials
 through the DLF platform for six months.
 But does -- when your GPU resources are not unlimited,
 so you'll have a limited time --
 a limited total cumulative time on the GPUs.
 You can go back and review the contents later tonight, tomorrow, this week,
 at home and review the materials as much as we do.
 One thing we do ask is when you're not working on the labs,
 so for instance, when we take a break for lunch or at the end of the day,
 please shut down your labs.
 So these are running on the clouds,
 so they're going to keep running unless you actually switch them on.
 >> We'd like to thank the DLF platform team
 and our cloud sponsor, Microsoft.
 All of our DLF sessions and workshops at GTC are powered by NVIDIA GPU
 Excellentex systems from Microsoft Azure.
 So please take a moment before we start to make sure your phones are silenced.
 And when you're doing that, I'm going to hand it off to David,
 before I start the workshop.
 >> Thank you so much, Sid.
 Round of applause for Sid.
 Thanks to all our PAs.
 And for yourselves, I mean, just keep it going, that's fine.
 Thank you all so much for being here.
 This is one of my favorite things to do,
 and it has been painful for us to not be able to do this in person for now,
 five years since the last time that we were able to have GTC in person.
 So I know that we've been doing virtual conferences.
 Maybe some of y'all are familiar with the deep running resources
 from your work or from the public sessions we hold.
 But it really is no better way to kick off a week
 that is hopefully going to be full of a lot of exciting announcements,
 exciting learning and growth for all of us.
 Myself and the other NVIDIA employees in the room,
 we're dedicating their time and volunteer to be with you today.
 We're all just so, so excited to be here.
 And being in this room today is really the best way to kick that off.
 So congrats, y'all are awesome, already in my book for that.
 Again, my name is David Williams.
 I am a solutions architect at NVIDIA.
 So that means I'm a sort of first point of contact engineer
 for investment in the financial services industry.
 So I work with payments accountants and banking accounts,
 adopting deep learning, artificial intelligence.
 That's the sort of role that all of our wonderful TAs,
 some of them might have.
 Some of them are other roles inside of NVIDIA.
 And certainly, we have a really incredible wide range of experience,
 expertise, industry, and expertise around the industry.
 So the thing I can ask the most of you is don't be shy.
 Use those cups.
 We do have a tiny use of cup system because there's a lot of y'all,
 so as much as I would love to have a [INAUDIBLE] conversation,
 that has really helped us out.
 To use those cups, turn them to red and you have a question.
 Either technically, about like, this doesn't work,
 or I don't understand this, or if you just have an interest, right?
 If there's something you want to learn, then we're interested in your learning.
 Have that conversation, or change my answer, change my answer.
 There's probably a lot of things you might want to know.
 Let me know about the answer, but that's how we all learn together, OK?
 So cool.
 Again, David Williams, flag me down whenever.
 I'll also be floating around.
 This is instructions you've been probably looking at.
 Now is definitely the time to follow them if you haven't done so yet.
 I'm glad to see that a lot of folks, almost everybody,
 has their own computer.
 I don't know if we have more friends or not.
 Does anybody not have a computer, and is kind of worried about this right now?
 Yeah.
 Well, I understand if you're having issues with yours or something like that,
 we may or may not be able to do something about that.
 Please start off.
 You've logged onto the Wi-Fi there.
 I will say some things that will make your experience smoother today.
 It seems like we've at least had a few folks, myself included,
 have a little issue with our courses page.
 It seems that it's mostly resolved by using an incognito window or clearing
 services.
 So if you get like a weird 404 error or something, try refreshing first.
 But that's probably what it is.
 I think you'll also probably have a better experience
 if you're not on your work VPN.
 One, sometimes that fiddles with our course materials
 that we're going to be logging on and getting access to.
 But also, it'll probably help you be more checked in and plugged in
 if you're not worried about what emails you have responded to yet.
 I know that's always something I have to train myself.
 So once you're on the Wi-Fi, you should be able to go to this URL,
 courses.invidia.com/vli-event.
 That's going to be an entry point to our GPU sessions today.
 There'll be a big box that says event code.
 First, I would say probably go to the top right and log in.
 You'll have an video account.
 I think everybody should, because I think
 you have to make one to register for the conference.
 But if not, make one and request an email you have access to and log in.
 And then that'll probably try to drop you to our dashboard page.
 I would say it's easiest to then, once you're logged in,
 go back to courses.invidia.com/vli-event.
 And then you should be logged in in the top right.
 You can enter our course code today.
 And once you've entered that code, you now have
 enrolled your account into this course.
 You'll have access to a GPU-based machine.
 Or actually, kind of not actually today, we'll talk about it.
 But you have access to the course machines today
 for an eight-hour session.
 And then you'll notice that there's another timer that's at 16 hours.
 Your total access to this course over the next six months can total up to 16.
 So you want to just fully take it again, at the same pacing
 that I'm going to take you through today, you can do that.
 You can log in two or three times to kind of refresh on things
 or just take the parts you thought were the most interesting.
 We'll talk about how you can download materials.
 You want to make it as easy as possible for you to take what you learned today,
 take it back with you to your jobs and your interests and stuff like that.
 So I'll quickly demonstrate what that should look like.
 So you should be able to go to something like this, right?
 Log in, operate if you're not.
 You are logged in.
 All you do is enter into that event code.
 Bucks, GTC 24, W62596_W.
 Our TAs all have little sheets of paper.
 If I move away from this, if anybody is just now walking in
 and still sending things out and is going to take your seconds
 to get to this point, use that cup.
 TAs will come check in with you, give you the event code.
 That will be good.
 Okay.
 Thank you, TAs, for being with us today.
 We appreciate it.
 I'll make sure we get some folks stepping up in the front.
 So we hit our max number.
 So we're trying to increase it now.
 So just be patient.
 This is a new course that we're unveiling and running
 for the first time here today.
 But we're all going to learn a little bit together.
 When you get into that course, you'll probably see something
 that looks a little bit more like this.
 And hopefully you can work this one out.
 You'll look right here to get started.
 We should see these slides, but also, down here at the bottom,
 the start button.
 So because we wanted to give everybody the same experience,
 all of you today should be running on...
 I'm going to go through the DLIT events.
 After you enter the event, you're just going to basically
 put onto a pre-made instance that has all of the...
 Maybe start over and then close this window.
 If you shut down this instance, your work is going to go away.
 This course is wonderfully built, in fact, by Josh.
 It's wonderfully built so that every...
 So it's not like you have to restart or do the entire...
 But if you like to download materials and you want to see
 your completed outputs, all of the code and the outputs
 of what you've been running, you probably do want to do that.
 We'll address that later today about the best way
 to download the materials.
 So we're just going to leave everything running for the eight
 hours we have going on today, and then we'll all close down
 all of our instances when we're done.
 Yep.
 So some slight changes here.
 If you can't resist, I would hold off on that start button.
 We've got 1,000 of you all.
 They're spinning off all these instances.
 It's always a challenge.
 So they're working on it.
 That's what it's called.
 We have a brilliant platform team.
 We will get everybody access to everything we're going to say.
 It might just be a business.
 So if you're already in on a big deal or you're already
 waiting on it on a big deal, we'll
 make sure everybody gets in.
 We're going to do that in a minute.
 [INTERPOSING VOICES]
 And you should all have access to these sites in English.
 Andy, let me try the dashboard.
 [INTERPOSING VOICES]
 OK, so we already have it.
 Should I click Start?
 [INTERPOSING VOICES]
 So thank you for being patient.
 Unfortunately, we are 29 minutes in and only just
 got talking about what we're learning today.
 But we won't get all caught up, right?
 That's always a challenge.
 So this is a course called Efficient LLM Customization.
 This is a really cool course.
 We're going to be here all day learning about large language
 models and, especially, how can we
 alleviate some of the most pressing concerns
 or add some really cool new capabilities to these models
 that I'm hoping--
 or most of us are at least somewhat familiar with
 by this point--
 big models like GDT, OKI models, MaMa, those sorts of things.
 So that's the focus of today.
 I'll talk about what sort of prior knowledge
 we're kind of expecting here in a second.
 I'd say I just mostly want this to be the most helpful
 and impactful for you all.
 So again, use those past questions.
 If I start getting sort of the same questions or cool little
 tidbits, I'll share it with the entire group.
 Talk to your neighbors as well.
 Introduce yourselves, figure out why somebody else is here
 and why I have to go there, hoping you can get out of it.
 That's some of the best things to get out of GDT.
 So we're going to be talking about those large language
 models and what I mean by customization.
 In particular, we're covering a set of approaches
 called Perimeter Efficient Find Domain.
 That's really the form we're going to be talking about.
 When we finally build applications, build a couple
 of really cool applications together, leveraging this concept
 we're calling the virtuous cycle of model customization,
 but how we can leverage these Perimeter Efficient Find
 Chaining techniques and also synthetic data generation
 to do more cool Perimeter Efficient Find Chaining
 techniques that can really supercharge our models.
 So I mentioned Heft is in our core course concept today.
 In particular, the two methods we're
 talking about are called P-Tuning and LoRa.
 It's for me to gauge here.
 Can you raise your hand if you think you have--
 I'm not going to ask you to explain yourself
 in front of the group.
 Raise your hand if you think you would
 be able to tell me what a P-Tuning or a LoRa model is,
 like one of you.
 Perfect.
 That is exactly what I hoped for.
 We got about 10%, 20% of you all who at least made me
 have a little prior knowledge.
 There's still plenty for you to learn and build
 and have fun with today.
 Keep that in mind, the vast majority of us, this is new.
 You're going to all learn together.
 And I mentioned synthetic data generation is--
 I mean, really, that's core to all of these models.
 These are models at their core that are generative models.
 But we're going to particularly apply it
 in that virtual cycle [INAUDIBLE]
 And I hope that the core thing you really pull away--
 yeah, learning an API is great.
 Yeah, building a particular tool is great.
 Really, it's about the core fundamental concepts.
 I want you to walk away here knowing
 I would customize a model because I
 want it to do something different.
 Or I want it to do the same stuff, but maybe smaller
 or lighter or more efficient.
 Or maybe I want to do the same stuff it already does now,
 but I really hate having to type in 500
 tokens to make it do that.
 This is the starting point into your journey
 of going to going and building more applications
 with this pretty much critical core technology
 of generative AI.
 So we're going to learn.
 It comes with us today, it comes with us the entire week.
 Pre-wrestling-- broadly speaking,
 I'd say this is sort of an intermediate-ish course,
 a beginner-intermediate course.
 I'm going to assume that everybody in this room
 at least has a solid enough understanding of deep learning.
 We just don't have time to go back through.
 But remember, core basics--
 I have data, I have a big mathematical statistical model.
 All that model is built of trainable weights
 that influence the outcome.
 So I put in data on one side, I get an outcome on the other side.
 We go forward pass through the model,
 calculate out some random, terrible answer.
 Use calculus to calculate how we want to change our model
 and back propagate backwards to update all of those weights,
 make our model just a tiny little bit better.
 We do that process of data correct, data correct,
 millions and not billions of times over a massive data set.
 That's the-- if you were like, wow, that sounds cool.
 I don't know any of that.
 Slide one of us down, maybe we could pop you
 over to the fundamental sports.
 There's also lots of ways to learn
 [INAUDIBLE]
 Secondly, LLM fundamentals.
 There's this model, 2017, called Transformer
 at an encoder-decoder structure primarily defined
 by a new layer type or a relatively novel layer
 that I called Tension.
 Tension is all about finding connections in data.
 And so it was incredibly powerful for taking text as inputs
 and doing either the creation of embeddings and encoding,
 say, like, I have text and I'm going to vectorize that
 and get a really, really deep understanding
 of the meanings of those words.
 And then a decoder path, a generative path,
 predicting, hey, if I have a string of tokens,
 what's the next, what's the next, right?
 And that's all I'm going to go into today.
 I hope you might have a little bit of background
 in that area.
 These models we talked about, originally GPT and MERT,
 the decoder and encoder paths.
 I've been the foundation of all of this generative AI work.
 So there's another defining this course
 if you want to go deeper in that architecture.
 I think, hopefully, the minimum you need to know
 is just be aware of what a generative model is
 and the general process by which we generate tokens,
 which is, you know, I have some set of input tokens,
 they get a string that gets turned into tokens
 or a tokenizer, I go through an embedding,
 it goes through a long-language model,
 the output of more tokens.
 Thanks for bearing with me on a very high level flash forward.
 The last one I would say
 is probably the least actual requirement.
 We originally were building this as sort of two parts
 to a course.
 I actually don't know what the status of that one was.
 I would say we're going to cover enough terms in here, right?
 But that's sort of one of the earliest ways
 when I customize these models.
 We're going to cover that a little bit in detail as well.
 Python, our two tools we're using today are Nemo and Python.
 So talk to the second first.
 Python is our language.
 That's the language of deep learning today.
 You don't have to be a Python wizard,
 but would probably help to be a little comfortable
 if you have any questions.
 Ask our team.
 We also have solutions all the way through.
 So I think you'd still get something out of this course,
 you just might be a little bit more
 reliant on this.
 The main tool is called NVIDIA Nemo.
 How many people here have heard of PyTorch?
 Cool?
 How about Hygenface?
 Cool?
 How about Nemo?
 Yeah!
 Well, you're in the right place.
 We're going to learn a lot about it today.
 We're especially learning about a service interaction,
 but Nemo is NVIDIA's end-to-end large language model frame.
 Now Hygenface is a awesome library for LLOs,
 it's probably where most of my customers get started,
 it's where I go.
 Nemo takes that same idea of a,
 I'm going on PyTorch, that's doing my course,
 he's learning a model anyway.
 And then Nemo adds in a more distributed framework,
 called Nemo Framework,
 that lets you do cool things like model parallelism,
 making these really big, massive models
 in the easiest possible way.
 Nemo is end-to-end, so it has data processing in it,
 like data processing library tools,
 to do exactly the same work that you want to do
 when you run a VPC model, or a virtual style model,
 or anything like that.
 It has inference tools to easily deploy that
 in the most efficient, optimal manner.
 That's kind of what Nemo is,
 and you'll probably hear about that a thousand more times.
 That's sort of our build of like,
 hey, once somebody kind of graduates,
 from just pulling down and trying something
 with Hygenface, and they really need to start doing
 building custom models, and deploying them,
 and building them at that, like,
 trillion-gram scale, that's kind of what we target.
 The other aspect of Nemo, that we are using today,
 is called Nemo Service.
 So, we also nicely integrate all of that code,
 up into an API, and service face-off.
 So, much like similarly, you probably touch OpenAI,
 and then release DOM there, and do things in the chat GPT,
 or some other tool before that, right?
 Art, or Gemini, or Google.
 Nemo kind of starts at that point, too,
 that you can just go find that pre-hosted model,
 submit your prompts and tokens, and get responses back.
 But we're building this for enterprise, right?
 So, we have even plugs and tools to then pull that down
 into a container that runs on them.
 So, what we're using today,
 the reason I kind of was a little coy,
 and normally we have a GPU attached to your instances,
 but you don't need one today,
 is that everything we're doing
 is something you could actually do on your laptop
 if you wanted to go set everything up,
 because we're leveraging a much more powerful model
 that would ever run on a single GPU.
 We're gonna be calling out to the Google AI test.
 And, you know, we're using the API interaction with it.
 Of course, there's a GUI and things like that.
 And, hint, hint, there may be some very exciting announcements
 about this, this week.
 That's all we do.
 Cool, fundamentals.
 So, here are seven, that's seven if I count right,
 seven things you can do with a language model, right?
 We probably all started with that first one.
 Going on with an AI, going to whatever service,
 and just sort of typing up, you know?
 Just type in, "Hey, how are you doing?
 "Hey, tell me a funny joke."
 See what these language generative models
 that are so good, you know,
 statistically determining what are the tokens
 that you are most likely to come out with access
 in response to a prompt, right?
 I'm gonna predict that one-by-one generation
 and then return and do it in such an incredibly human,
 natural way, right?
 So, that's probably where all this started.
 We just started typing and sort of just prompted
 to see what we could get.
 The next thing, this thing is a little, a little slower.
 You gotta, that's right, now excellent.
 Or minding me, if you're having trouble
 seeing them in the back of the room,
 you should have all these slides on there
 and you can load a little bit closer.
 So, the next thing there, intentionally modifying
 the generation parameters.
 But now, I'm still kind of just typing,
 but at least some services let you control
 all these funny parameters
 that give you just different access, right?
 One of the most easy ones to understand.
 Say I write tokens to generate.
 Oh man, I just asked the yes or no question,
 it gave me like 200 words.
 That's not really what I was looking for,
 so I'm gonna set a limit.
 Or I'm gonna set it in here.
 Some of the ones we'll talk about today
 and use are called temperature, topK, topP.
 These are randomness kind of things, right?
 These methods basically say, you know,
 there is one way you could run your large language model.
 At the end of a language model is a soft max layer.
 And all that does is it says,
 "Hey, here's all the types of tokens
 "I think might come next."
 And then it says, "Give me the most popular one."
 The most probable one, right?
 Well, we can change that math to give you
 a higher likelihood of choosing the second,
 or third, or fourth most likely outcome.
 Why would you wanna do that?
 I don't know, I mean, it depends.
 It depends on your use case, right?
 Maybe I'm building a very fixed chatbot
 and then I don't want it to give me very specific answers,
 so I want it to kind of be greedy
 and I want it to be very replicable.
 So I wouldn't use them.
 But maybe I'm building something like a customer service
 agent or something that I want to be more flexible,
 something that's creative, right?
 We talk about those creativity aspects a lot.
 You would increase a field like temperature, topK,
 to say, "Hey, you know, give me a slightly higher chance
 "of choosing something other than the first one."
 And maybe you'll get some intuition around that,
 but as you'll see, if you run some of these,
 there's some of these cells that will run
 and you'll get the same answer as your partner,
 'cause whoever's next to you,
 you'll get the same answer over and over again.
 That means you're probably in like a greedier mode,
 'cause it's just re-predicting the same
 most probable solutions.
 But, if you don't like that, you'll experiment
 with training now, or increasing the temperature,
 increasing the temperature.
 Intentional iterative prompt engineering.
 So I mentioned the promise that we're gonna talk
 about this a little bit, but this is the core concept
 of I'm not changing my model, I'm not training anything yet,
 but I'm going to change what I put into the model
 in some intentional way to try and get
 a different type of response.
 You probably maybe even already did this
 a little unintentionally, that you typed in something.
 It didn't give you quite what you were looking for.
 You just passed it a little bit different, right?
 And maybe you get it that time.
 Well, you can do prompt engineering
 in that sort of unintentional, flowy way,
 but you can also even structure your prompts
 to turn a general generative model
 into a sentiment analysis model, or a summarizing model.
 Purely by changing what we put in.
 You don't have to actually do any real data science.
 So we'll do a little bit of that.
 Few-shot prompt engineering is probably
 maybe the most common concept you might have heard of that,
 or is like a good one to get wrapped around.
 These generative models are so powerful,
 that I can give them examples at time of running,
 and they will take that into account.
 We'll see an example of like, if I want to say,
 what is the capital of mine from the state of South Carolina?
 I could say, well, what's the capital of Texas?
 Answer, awesome.
 Austin, and then what's the capital of North Carolina?
 Answer, please finish that prompt,
 and it has an example of the type of material I'm looking for.
 So that's few-shot, you could even have one shot
 and it's just, I provided one example.
 Two-shot, or few-shot is I provided two or three examples.
 What's the problem with this?
 This is a lot of tokens, right?
 And if you've used open AI, they charge by the token, right?
 But it's charged by the context of the token.
 So, that's one of the things we'll talk about when I say,
 we're going to customize to try and get things smaller.
 That's one of the dimensions we're going to try to do.
 And then highlighted the most, in the brightest color,
 in the green there, is path two.
 Grammar-efficient, fine-tuning.
 This is the first little, big entry point
 into actually updating or adding some weights to our model,
 or training, doing a training through fine-tuning process
 to provide it new functionality.
 Supervised fine-tuning actually came first,
 and that was, you know, this core concept
 to all of these transformer models.
 What makes them so powerful is we have this beautiful
 vision between pre-training and fine-tuning.
 We found that in the domain of natural language
 at the very least, that there was immense value
 to being able to train a model in an unsupervised
 or a semi-supervised, self-supervised method
 on the entire internet, and then possibly large amount
 of data for any human to ever create language for.
 So, that pre-training step, already done
 way down there at the bottom, and then fine-tuning,
 was like, okay, I can make a labeled data set
 of what I want my model to do.
 I just only want to make 1,000 examples,
 or 10,000 examples, or 50,000 examples.
 I can't do entire internet scale.
 This core divide between supervised fine-tuning
 and pre-training is what has enabled these models
 to get so big, so powerful, because bigger models means
 I need more data to efficiently train it,
 and if you couldn't do that, you didn't figure out
 how to do self-supervised or unsupervised learning.
 But, supervised fine-tuning, pre-training,
 we're not really doing that today.
 That's kind of, you know, a little architecture basics
 you might use on your model.
 So, prompting, how would I write a prompt?
 Prompt engineering, we can kind of walk ourselves
 through this process here, right?
 Where I'm gonna think about what I want to do,
 what are the constraints I want to do it under,
 and then, what would I really like it to do if I have to.
 So, you might start and say, in this case,
 I think we're doing the summarization task.
 Think about what's the task I'm trying to solve.
 And, you just start typing, right?
 You might type, say, before you get to this point,
 summarize, type, type, type, type, type,
 whatever you're trying to summarize in 10 words.
 And that is what I typed into in a lot of scripts, right?
 Now, this is using what's called a Python F string,
 a formatted string, and so the only nice, cool thing
 is when we're programmatically doing this.
 I can have a variable to fill in that.
 Right?
 Well, we can look at these prompt templates,
 and ideally, a prompt template, right?
 This is human readable language going into a model.
 Ideally, at least, a naive template makes sense to us
 as humans, too, right?
 So, I can see that somebody who wrote this prompt template
 wants to summarize.
 That's the task they're trying to solve.
 Question and answer, similarly.
 You have, like, hey, here's a backup context.
 Here's a Wikipedia article that I've pulled,
 and here's a question I might have about that.
 Please prompt for the answers.
 So, I have some prompt templates.
 That's what's feeding into our model,
 and hopefully defines, like, what is the core use case
 I'm trying to solve for?
 Constraints are now, what do we need that model to do?
 Right, under, you know, obviously solve our problem,
 but then what's the conditions under which
 it would be helpful to us?
 So, for example, just to build it into whatever system
 you're building, you might have an absolute max latency.
 This is not useful to me if I can't answer this question
 within three seconds, or three milliseconds, or however.
 Right?
 It needs to be accuracy, sorry, I just jumped over.
 Latency, but it needs to be accurate, right?
 It doesn't really do me any good if I already have
 a sentiment analysis chatbot that's doing 80% work correctly.
 I need it to be better than that.
 We might have constraints on deployment.
 As much as I'm sure we all wish, money is not imaginary,
 and usually, the more efficient we are with our GPU
 resources, the happier everybody is, right?
 So, this might be my constraints about what I have
 to run on, or how much money I have to spend, right?
 The amount of tokens being sent to these LLM services
 is another element of that as well.
 Anything that charges by the token,
 that's going to be constantly (mumbles)
 So, I have hard constraints, basically,
 about this is what I need to do, accuracy,
 and what time frame, and what cost.
 And there's also, like, eh, you know,
 it'd be nice if we had some extra things.
 I would make a lot of people happy
 if I could get the best possible performance.
 A lot of times, we talk about how language models
 are so cool, because it's not even just taking, like,
 something you're already doing,
 and you can do it a little bit better.
 It's like doing stuff so much better
 that you're now doing something
 you never really thought possible.
 And so, what are the things we've never tried
 to solve with data science that that would be?
 Maybe it would be nice if we can run something
 at 70 billion parameters, the total total weight size
 of our language model.
 Maybe 70 billion is possible,
 but boy, it would be nice if we saw it.
 I'd make my boss, make my IT team that good.
 Okay.
 Maybe I'm running a bunch of one thing already,
 and just adding another one for every single use case
 would be, you know, that's a lot to start to be track of.
 Most of my customers I talk to,
 the number of things they want to use
 our language model for is the number of times.
 You know, a lot of those may be unsolvable,
 that may be one of the good things,
 but still, if you talk about hundreds or even dozens
 of different distinct copies of these models running,
 you know, it sure seems like a waste
 if there was somebody who could do this.
 And then again, you know, maybe if I had fewer input tokens
 to my pump, I'm either saving money,
 a harder environment, or you know,
 it would be nicer for it to be faster.
 The smaller our pump is, the faster (mumbles)
 So, the long engineering process in more than a slide,
 and you start by writing something naive right there, right?
 What's the capital of Cal?
 And then, you're a good programmer,
 so you're like, ah, you know, it's silly for me
 to keep hitting delete and entering delete.
 I'm gonna iterate over a data set,
 so you turn it into that F string I mentioned, right?
 What's the capital of?
 Variable state.
 We'll use this a lot today.
 When Python executes, it will just place that in
 and what actually gets handed off to the next step
 of execution will just be a properly formatted string
 with whatever the value of state was at that point.
 And then, you shut in.
 I'm not getting my capital predictor
 working quite well enough yet,
 so I'm gonna now say, well, here's that example of,
 here's a question, here's the capital part for me,
 here's the one for Sacramento.
 Now, that's the capital of this other thing.
 I can help basically provide a new behavior to a model
 without having to touch the weights at all.
 I've turned a general purpose prompter into a capital
 in server one and one and one, right?
 And then, finally, we're gonna do this a lot today
 is we're doing this in Python.
 Not really helps to wrap that up.
 It is object atlantic as you can.
 So, you see here that this first prompt in,
 first prompt to bullet function
 is just basically taking a variable,
 returning that variable, right, so we can do it.
 That's it.
 So, way back on that side, that had the seven things.
 That was prompt engineering.
 You're now all experts at it.
 I'm sure you'll get to touch it a little bit.
 What if that's not working?
 What if I'm trying every one of those techniques
 and it's just kind of not quite doing something new?
 I'm trying to ask for a pretty challenging thing, right?
 And it's just, it's not working for me.
 Well, maybe past, in general,
 will be that next step up the accuracy ladder
 of what you need to customer in that model.
 So, it might be just straight up better
 at what you're trying to do.
 Alternatively, we see people turning to these methods
 to increase the accuracy of a smaller model.
 So, maybe one of those other constraints
 is how we want to optimize.
 And so, we're going to say, yeah, that's that case of
 I'm running the 70 billion model, but if I could run a
 - 13 billion?
 - I would be really happy.
 And so, we're going to try and sort of transition downward
 in our model hierarchy.
 And then, maybe that last case of like,
 for latency reasons, for cost reasons, I just won't,
 I hate, you know, the downside of this is
 look at how much longer our prompts got
 with only like two steps here.
 We went from, you know, probably five, seven tokens to 30.
 And you can imagine the bigger your shots are,
 the bigger your prompts are.
 Our prompts have already got large.
 Maybe we don't want to do that.
 So, that's why we would consider this next step called math.
 Maths at a high level.
 All of these techniques have the same sort of flow to them.
 You first, at least need some programmatic way
 to control an input prompt to a model.
 So, we're going to create that comp template,
 defining our math.
 And then, we need to create some amount of training data.
 This is a fine-tuning method we need labeled data.
 We can do this, certainly, you know,
 what's nice here is that HEP is the smallest
 of the fine-tuning methods.
 So, that means it needs the least amount of data.
 So, we can probably get by with like a few hundred,
 a thousand examples.
 That's still probably not how you want to spend today,
 sitting around and labeling things yourselves.
 But you certainly can do that in an organizational effort.
 You know, NVIDIA certainly sends that around all the time
 where somebody wants to do something new
 or they kind of send out a Google survey
 asking me to fill out and create some data for somebody.
 Right?
 That's pretty achievable with HEP.
 Of course, if what you want to do has already been done,
 always get a decrypt from that.
 You can just use an existing dataset.
 And then, what we're most excited about today,
 what we're actually going to be doing is I present to you
 that it is so much easier to just go to a bigger model,
 use it for a very short period of time,
 and hopefully that behavior is correct
 in a way that my model, this my smaller model
 that I'm actually trying to run isn't.
 So, I can use synthetic data generation from LLMs
 to then create those labeled datasets
 and ultimately feed that into my model.
 I am no lawyer, but read your license terms.
 That behavior, for obvious reasons,
 GPT, the people who make money off of GPT-4
 wouldn't be terribly happy if you went and built a model
 that was almost as good
 and now you're not locked into their service.
 So, most of the big model service providers
 kind of are discouraging of that.
 They do not want that.
 But, everything you're doing today is on NEMO service.
 And then, of course, now I have basically a function, right?
 An input function that can iterate
 over some amount of data that has some labels.
 Keep learning.
 We're gonna do some training process.
 We're gonna iterate over this dataset,
 feed it in, run a change, something.
 It's called parameter efficient
 in that hopefully what we're changing
 is less than the 13 billion or 70 billion total weights.
 So, that we'll talk about in more specifically
 what exact amount of weights we need.
 RAG, I actually already had a question today.
 This is one of our favorite topics,
 one of those powerful topics.
 There's actually an entire DLI
 that you can take for free, self-paced, I think,
 whenever, not BTC, that's what it's like.
 About RAG, that's how important it is.
 RAG answers the question,
 well, you talk about these models that are being trained.
 You say trained on the entire internet
 or even trained on this particular dataset.
 Does that mean that there's a cough in their intelligence?
 If I train this model today, March 17th,
 heck, how many say heck, by the way?
 I see you're all wearing green.
 We're all safe.
 But yeah, today, March 17th, I train a model.
 Strictly speaking, that model does not know
 anything that happens beyond that training.
 So, if some huge event comes out tomorrow
 and I try to ask a question about it,
 it doesn't have anything to base that knowledge on.
 It might be very good at making educational guesses,
 but sometimes, and we have guardrails
 in a lot of these services now that kind of
 can intuit if you're asking about something
 that happened beyond when they were trained
 and don't even tell you that, right?
 So, RAG answers, how do I get around this problem?
 How do I have up-to-date information?
 It also addresses the hallucinating problem, right?
 If you use these models and you start asking them
 to prompt tokens, and it's pretty easy
 to get them to say some wild things.
 You might have those temperature top case settings, right?
 They can lie to you, they can give you things
 that are half-truthy or true based on one source,
 not another source, right?
 A lot of things can be challenged there.
 So, RAG, retrieval, augmentative generation,
 it's a technique to get around this.
 And very short, right?
 Normally, right, I just write in my prompt,
 it goes to the language model, I get tokens out.
 Instead, we want to have some knowledge base set apart,
 that we would like to be the context
 or basis of our generation.
 To make those easily searchable and comparable,
 it's kind of hard to compare long strings of documents.
 Instead, we use a vector base.
 We'll use common embedding models,
 packed ones also go from transformers,
 they're really good in coding, right?
 But embedding models, to turn, to chunk up
 all of these documents, turn them into vectors,
 and store all those vectors in a database.
 And ideally, those vectors have some connections, right?
 I've embedded my model in such a way that a model,
 a article about dog grooming is more similar
 to one about cat grooming than it is to one about
 wargamer for you.
 So, we vectorize all of these documents,
 put them into a database, and when my prompt comes in,
 I also pass it through that same embedding model,
 I now have a vector that corresponds to the input prompt,
 I have a vector for all of my knowledge base articles,
 and I can do a big old comparison.
 I can say, which of these knowledge articles
 give me a rank list of the most probable things
 that relate to what I'm talking about?
 And then, what we actually prompt our language model with
 is that context.
 We say, hey, here's a knowledge article
 that I think is relevant to what you're talking about,
 and here is the question I'm asking.
 And you can see how that's way, way more efficient,
 way more accurate than just saying, hey,
 dig deep into your trainable weights on the entire internet
 when you were training and trying to remember.
 It's kind of like doing reading comprehension
 versus memorizing and remembering
 from when you read the book last week.
 Yeah, but I actually haven't done that.
 RAG, massively powerful technique.
 This is how almost all of my customers
 are starting their journey as well,
 because they have these knowledge base articles
 that are key to these cases they want to solve.
 Can you do RAG with Pest?
 Absolutely.
 You see, nothing I'm talking about RAG,
 that's all kind of before we prompt the model.
 And so you can train your LLM
 to do something new and cool and more powerful,
 and also then add RAG on.
 I encourage you to take that DLi course
 if you're deeply interested in it and consider it really good.
 Virtuous cycle.
 So I mentioned the third option.
 We're doing time tuning, we need to create a data set,
 how are we gonna do it?
 We can do it manually, we can find some of the else's.
 If I can't do either of those things,
 then I'm gonna probably do something like this.
 We're gonna write some of our hand seeded prompts.
 You have to at least write a few,
 write that prompt template,
 and figure out a way that I can generate even more of those.
 So that prompt template is really our key to this.
 I can now basically iterate over data set
 and create 100, 1000 prompts on that.
 We're gonna then pass those synthetic prompts
 into a big LLM that does not meet our constraints,
 is beyond what we want to spend or deploy,
 and we're gonna get really high quality data sets.
 So as long as there's some LLM out there
 that you have access to that does work for your problem,
 well, we now have a label data set.
 We're gonna convince ourselves that that's possible,
 we're gonna see those outputs and go like,
 cool, this does work with these really large models,
 they just don't fit for my particular use case.
 And now, that's my data set,
 because I can be synthetically generated labels
 to go along with my prompt set sort of creatively.
 And then, that's our data set,
 we find you a smaller model with all of those,
 and our model just kind of gets lifted up.
 It's probably never gonna be better than our larger model,
 right, that's kind of an inherent limitation there,
 but now I can run a 13 billion parameter model
 where I used to have to rely on the set.
 (typing)
 This also, I think that's farther on in p-tuning,
 lets you, as mentioned, right, it makes it better,
 is one element of optimization,
 but also lets you do something new
 that might actually require fine tuning examples
 in a way that a common, just general purpose generator
 doesn't.
 There we go.
 We're nearing the end of having to listen to me non-stop.
 I will jump in and out.
 Any questions?
 This is one time I will open up for hands here.
 Any questions about that material?
 Yes, here in the front.
 (faintly speaking)
 Great question.
 So the question here was, we have reg,
 one of the parameters you kind of set,
 similar to how topk actually works
 for token generation as well,
 you can set like, hey, you know,
 I want the number one most best match article
 to be put into my LLM along with my actual question prompt.
 I could set it to be three, I could set it to be five.
 You know, it'll be a rank ordered list
 of what they can get most likely matching that embedding,
 but then how many do I put in?
 And yeah, prohibitive is maybe a good word for that.
 You imagine we tune in rag, we tune the chunk size.
 So you could say, you might wanna carve up my documents
 into paragraphs or into entire documents or sentences.
 So maybe if you have a smaller chunk size,
 you could have a higher topk.
 You can be a little bit more prescriptive.
 But in any way you slice it,
 the more stuff you put into that LLM up front,
 the slower and more costly it's gonna be.
 Great.
 And I think this is a perfect example here
 of the type of interaction we want when I ask.
 So I have a prize cycle up here.
 If you want, you can do it now or you can do it later.
 But we have about 10 little NVIDIA items
 that we might ask some actual specific questions through.
 We got some good stuff.
 Got some like mugs and a hat and some socks
 that are, I promise, fresh and clean and not worn.
 So since you were so brave to ask our first question
 and also had the luckiness to get called by me,
 if you don't mind, whenever we're doing stuff,
 feel free to go grab something.
 I'll pepper those out throughout the rest of the day
 as we ask some key questions, okay guys?
 So we'll make sure to give it all away.
 Yeah?
 (audience member speaking off-mic)
 Yeah, so our question here is Pest, how do we track all this?
 Yeah, you will see that we're using the service.
 So our service has customization IDs.
 And basically, you don't create an entire new copy
 of a model, that wouldn't be very efficient,
 and that's kind of against Pest.
 Pest implies that there's a very small amount of weights
 that were actually changed.
 And then that just gets stored up in our service
 and we get given a customization ID that basically says,
 hey, these are the P tuning weights
 that were trained on this process.
 And these were the LoRa weights
 that were trained on this process.
 And you can say, run the same llama, 70 billion,
 or whichever model with these P tuning weights that were run.
 Is that the answer?
 - Yeah, yeah.
 (audience member speaking off-mic)
 - Yeah.
 (audience member speaking off-mic)
 - Yep, yep.
 (audience member speaking off-mic)
 - Yeah, we don't, I think, you know,
 I'm hearing basically questions on experiment tracking,
 model tracking, keeping in mind, hey, I did this,
 this is what inputs were used to make this customized version
 of the model, and tracking all of that.
 We don't quite go that full on in here,
 but the service has some other capabilities
 that we're not using, and you can kind of see all
 the customizations, and then people commonly
 use other tools, like weights and biases
 and other construction tools.
 I'm gonna take one more question,
 you right here in the yellow, since I pointed at you already.
 What do you have for me?
 - So, in RAG, you say that we are basically
 embedding the model into that vector database
 we're generating, where do you think the sweet spot is
 between sort of updating that vector database
 and that process of training that, effectively,
 versus just putting that into the prompt?
 - Yeah, questions here about, like, RAG,
 sort of bouncing, now we have more things to touch, right?
 We can improve, yes, basically, you know,
 you have an embedding model that makes potentially
 better vector embeddings of your documents
 and of your input prompts.
 That's one thing you can improve.
 You can also improve the LLM itself, right?
 And, I don't know that I have a good answer for that,
 but it's like, this is perfect, ask me a question,
 it's already stumped me a little bit,
 and I think you just kind of do both.
 And you keep improving and you keep improving,
 and maybe if you find your limitation,
 like, as I can, most of the time when you do RAG,
 you can see that context, you can kind of see
 what was provided in human data through the samples,
 and you're like, that's not good,
 that's not what I was talking about at all.
 And then you probably should go focus on the embedding.
 It is, you know, you're getting good prompts
 and context into the model, and you know,
 it's just more about the generation aspect
 than the LLM, and doing these sorts of methods
 might make you think, oh, maybe I should do
 a fine tuning process, maybe I should embed my model
 as more like core language model generation.
 So, I would love, I love all the questions already,
 you know, I'm killing it.
 We're gonna be getting to a sort of Amazon version here
 in another about 10 minutes, right?
 And so, that's exactly the sort of stuff
 we'd love to talk to you about.
 Like I said, there's so many more people here
 who know even more, and I know way more than I do
 about these techniques, and we'd love to have
 those conversations with you.
 So, write down your questions.
 I promise we will try to get to everything
 by the end of the day.
 10.09, we are flying here.
 So, at this point, you can jump out of these slides,
 and hopefully, we put that start button.
 I checked our Slack, I'm hoping they figured this out.
 It might take a minute if these instances,
 our potlabs, we call them, have spun up.
 I'd say don't refresh the page, please.
 That would just kind of bump you back a little bit.
 It might take a minute if they've spun up,
 it might take about five or six minutes
 if they're still having some issues.
 If they are having issues here,
 I have the slides on my machine, because we're gonna look
 at one last set of slides here and talk about
 what is our problem.
 Sorry, I should clear another video.
 How many people here are, this is also really helpful
 for us, how many people here are here like with a group
 from your company, okay?
 How many people here are here for work,
 but became like solo and were part of that life?
 Awesome, gosh, y'all are so motivated.
 And then people who are just here for personal interest,
 for hobbies, for learning something new,
 yeah, this is a passion in yours, awesome.
 Yeah, I'd love to see it.
 This is something I've been doing,
 I've been working in the area for about seven years,
 and I just barely made the cutoff,
 where I came in from doing hardware, computer engineering,
 parallel architectures, learning CUDA,
 our programming language in high school
 and my graduate studies.
 And then I think, within about six months
 of when I joined in 2017, we were kind of out,
 so I was like, okay, everybody we hired
 should probably know what deep learning is.
 I was very, very thankful to have missed that cutoff.
 Great, so hopefully if you're like me
 and your hot logs work just fine, you can click launch here.
 Does anybody need the event code again?
 I'll throw it up for two seconds.
 If you were coming in late, again,
 our TAs have all of this information,
 maybe it's just better to use the cups there.
 Again, you should already be back on the stage,
 you can follow them along with the slides.
 And then, there's that event code down at the bottom.
 But please ask our TAs for help
 if any of this isn't working.
 Most of us, however, hopefully it's working great.
 We're gonna jump right in.
 Get cozy, this is our home for the day.
 We have a nice little intro.
 Um.
 Intro set of instructions that are gonna tell you
 what you're actually building.
 So if anybody has not used Jupyter before,
 super easy interactive webpage.
 You can do exactly what I do.
 What's nice about it is it just has
 these Python coding cells in a spurt.
 And you execute those and it effectively maintains
 and builds to the running Python context
 that we're gonna be working on.
 So it's kind of like I'm running a Python file step by step.
 So I can define import libraries,
 I can define variables, like this word, this string.
 And I can call function to give you,
 this is what we're ultimately building,
 is an email responder.
 Should be widely applicable to basically
 every company in the world that has these.
 Wouldn't it be nice, instead of having
 to take up our human cycles and experience
 with just sending kind of more emails.
 We could generate them, but generate them
 to specifically reply to what a customer
 actually wants to know.
 So this respond to email function is effectively
 one of the things we're gonna build.
 You can see here that a language model
 generated this response directly in response
 to all this information that it pulled out.
 It understood it was an unhappy customer.
 It understood in few details what they were asking about.
 And it used our 43 billion grand to continue tracking.
 And then, maybe even at the end,
 wouldn't it be nice to give this a little personal touch?
 Train it on your customer style of your company,
 whatever makes sense to you.
 Are you a formal company, an informal company?
 Or like in this example, are you a pirate company?
 Who wants to respond saying, "I'm a pirate company."
 That's sort of the tools we're gonna build today.
 Let's talk through how we're gonna do it.
 If you come down here in this first notebook,
 there's more slides right there.
 I promise these ones are shorter,
 but please go ahead and use this time
 to at least get to this point to make sure
 that you're technically able to follow,
 and then we'll jump into actual slides,
 or actual work on the character.
 Yes, sorry, again, these slides can be found
 in this first notebook called Workshop Intro.
 And you just scroll all the way down,
 there's a LLM Utils slide loader cell.
 So that's your test to make sure
 that you can do everything you need to do.
 You run that cell through like hitting Shift+Enter,
 or there's a little run button up here,
 give me, I know that's Jupiter 101,
 and then they should load.
 Yep, so, ultimately, we wanna craft a response.
 That's the thing we're after.
 We wanna make a respond,
 and we can think of that as a response string, right?
 You just wanna create that string
 and hand it off to something else
 that will actually generate an email for us.
 We worked earlier.
 Oh, maybe I'll just, there we go.
 Yep, so we wanna generate those email responses.
 So, what's our input data is some other customer email,
 and then this tool, this function called respond
 or respond email generates those string
 we talked about, right?
 Well, what would that respond function probably look like?
 It's gonna take that input of a customer email output.
 We saw, in that output of that first notebook,
 all these things it's doing.
 We're gonna kinda help our model along,
 as much as we love how powerful generative models are,
 and they can do a lot of intuition themselves.
 Sometimes, it does make sense to do
 some detailed extraction in the process.
 So, we're gonna build a sentiment tool.
 We saw that they pulled out the negative
 or positive value of the model or of the input.
 We're gonna extract a name, a product, a location,
 some other key entity recognition.
 We're gonna actually do the response writing,
 so call the generate function of our email search.
 And then, maybe we're gonna rewrite it
 in that persona, that pirate persona.
 Probably most of us don't wanna write our emails as pirates,
 but that's just a very clear working like,
 oh yeah, it works, because it's written like a pirate,
 but you can imagine most of it's more like,
 hey, here's a bunch of our customer form emails
 in a style we've been developing for years with our customers.
 We want it to be written like that.
 Well, how would we build that sentiment?
 And these are just basically functions we want
 inside of the response function.
 A sentiment function, we're gonna try and use
 an eight million parameter GPT model.
 That's small enough that it can probably
 run on most GPUs, an inference mode especially.
 And sentiment, pretty easy thing.
 You need something, you need one token back, do it that.
 So, what we need in that function to do it
 is we need an actual model that we,
 our company can do that task,
 and then we need that sentiment prompt template.
 That's gonna create our prompts.
 We're not gonna be just typing in sentiment,
 you know, you need some way to process a customer email
 and automatically create the prompt template.
 That's how we're gonna get sentiment.
 We are going to use a path method
 to train that sentiment column, right?
 So, to do the path method to train
 that, you know, here's the green model.
 I know the colors are a little harder here,
 maybe easier on your screen, but the green is our model.
 Here's what that model looks like.
 Well, that model was trained on GPT eight billion
 and some sort of data set, some test data set,
 and so one makes up a test data set.
 We need a bunch of prompts and a bunch of responses.
 Similarly, a main extractor.
 Eight billion for a model that this is gonna be
 an extracting, tuned, or optimized model
 and a template for doing that extraction process, right?
 We're gonna do that process for that.
 We're gonna do that for extracting products.
 We're gonna do it for that.
 Notice, though, that these names are gonna change.
 For all three of these, we're gonna try and leverage
 at least, you know, extract the name, extract the queue,
 extract the product, extract the location.
 Maybe we could use the same model for all three of those.
 You'll have an example of how we can do multi-task model,
 right?
 Extract the location.
 Same thing.
 And then, the writer, that isn't tough.
 We're gonna mostly use GPT-43.
 You see, all of these different things we're doing,
 we didn't have to use the most expensive,
 challenging model for it.
 We can save that for the most critical part of our workflow,
 the part that's gonna generate a really nice, long code name.
 So we just ultimately need, how do I take in all of that data
 and build a template for it?
 Yes?
 (student mumbles)
 (student mumbles)
 I mean, the problem is, there's a lot of it, right?
 We're gonna actually kick ourselves off
 with a separate dataset other than this, called PubMed,
 that's going to effectively be our initial guesswork
 at which method is gonna be needed for each of these.
 (student mumbles)
 So, and then finally, persona.
 We need some model that knows how to rewrite text
 into a persona, and how do we do that, right?
 Well, here we go, here we go, right?
 I don't know why this is slightly different,
 but the same thing.
 I mean, this is really just highlighting, again,
 this path method in a little bit more detail here,
 that for that persona data, we just need that dataset
 from someplace to do that path method.
 This is that virtuous cycle bit,
 where instead of all of these other ones,
 maybe we will find a way to get prompts and responses
 for other models easily, for a simple task.
 But, when we talk about something as complex as
 the label for my persona model is persona written emails,
 that sounds like a big pain to find a bunch
 of pirate emails laying around.
 So, I'm going to use GPT-43B to, again, take in my prompts,
 generate some really high quality synthetic data responses,
 and then I can use that to create my dataset,
 and then that's how we are able to use
 only an eight-building criminal model for that complex task.
 This will all make sense.
 I'm trying to give you the entire picture,
 and it's about to get even a little bit more complicated.
 This is just what we're going to go to at the end of the day.
 So, I promise you, you can look back at these slides
 and be like, oh yeah, I did do all of this.
 That's our model called response.
 Now, not to leave you a little bit sad,
 but we don't even have emails to give you that.
 So, we're going to actually first do that.
 (laughs)
 That's going to be a process.
 We're going to kind of do this actually first project
 to generate emails to get comfortable with prompting
 and generating these models and interacting with the service.
 So, we're going to generate all of these emails
 that we're ultimately going to be trying
 to build responses for, too.
 Now, of course, to generate emails,
 it would be nice if I had some details
 about a list dimension of here's all of the names
 that people might be writing in,
 here's all of the products they might be talking about,
 here's some label that says
 this is a happy email or some kind of upset email, right?
 So, we're actually going to do a project before a project.
 And go ahead and the project before the project.
 So, an email detail, like this emailer function
 right there in the very top middle,
 that's another function that takes in details,
 uses a GPT model to generate those customer emails
 we're going to train at the end of it.
 Those customer email details, like I mentioned,
 is just a list, a Python list of names,
 and products, and moods, and location.
 The things that we want to join on.
 So, your very, very first, well, second or third thing
 of this part of the project will be
 generating those lists of things as well.
 We're going to try and do basically
 as much pure prompting and path methods
 that we're never going to sit down
 and, like, try to do anything manually.
 So, we're going to create a list generator effectively.
 That can just list out, hey, give me 10 names.
 Give me 10 product names.
 That's all we're going to do.
 So, that is another example we're going to do of a path.
 We're going to say, hey, this list generator,
 I'd really like, it's such a simple thing,
 I feel like I could probably do it with a small model,
 but maybe I need to help that model along,
 I'm going to use some bigger models
 to create my training data set.
 Rather than just sitting there just typing out names.
 Okay, so, again, building back up.
 We're going to actually start by building the list generator.
 That's going to give us customer email details.
 We're going to take those details,
 write a bunch of customer emails automatically,
 and then we're going to figure out
 how can we also write a response to that.
 (audience mumbling)
 What'd I ask?
 Can somebody give me, show a hand,
 maybe in the very back left corner,
 I don't think I've gone over there yet.
 Somebody tell me why you would maybe use
 a pep method over prompt engineering.
 Any guesses?
 It's okay if you're wrong, you're still going to get a prize.
 But can I get a hand from up here?
 Yeah, for sure.
 (audience mumbling)
 Beautiful, that is one reason
 we could use pep over prompting now.
 Some sort of fine-tuning process, that's going to mean
 I don't have to put as much stuff into my model,
 and that's good, it's awesome.
 So please, you and my friend up in the front,
 as we're getting started here,
 make your way to the front corner and pick out a prize.
 Another method, right?
 You want to use peps to do something new,
 use a smaller model, smaller context, any of those.
 That's what we're trying to build to.
 You're going to see that all of our products
 kind of have that tie-in of, hey, I'm doing something
 and making my model better in one image or another.
 Yes, last question.
 (audience mumbling)
 Yes, so that would be an example of doing a new feature.
 The question here is, could you use pep for internal,
 private data?
 And that's really another reason you would customize a model,
 something that's been trained on the entire internet,
 hopefully doesn't have access to what you care about
 in your customer data, right?
 So that's another great thought there, where it's like,
 hey, I have specific internal tasks or problems
 or nomenclature, and pep could be a method
 to introduce those ideas.
 Great, okay, so to start with, super easy notebook here.
 You're going to jump into the directory there
 called One Nemo Service.
 And notebook number one, this is one dot one,
 is kind of how I'll refer to them, but 11, either way.
 You're going to open up that notebook.
 I'm going to give you about 10 minutes.
 Yeah, I'm going to give you about 10 minutes
 to run through this notebook.
 It's a pretty easy one, okay?
 So I'm going to go ahead and figure out some stuff on here
 to be able to put a timer up for you
 so you'll know what time we're finishing.
 But let's call it 10.35-ish.
 We're going to finish that.
 FYI, lunch, 11.
 So we're, you're already halfway to lunch,
 if that's critical and important to you,
 and it is to me, I'll do that in a second.
 Yeah, so 10 minutes, try to make it through this notebook.
 You'll see, we're going to kind of do this a lot today
 where this is the biggest chunk of me talking
 and you having to listen to me,
 and then I'm going to let you do a little bit of work.
 You ask questions.
 I'll kind of review what we learned
 from the notebook at the end.
 Thanks, everybody.
 Go ahead, use the cups through it.
 And.
 (people chattering)
, okay. (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (crowd murmuring)
 [BLANK_AUDIO]
