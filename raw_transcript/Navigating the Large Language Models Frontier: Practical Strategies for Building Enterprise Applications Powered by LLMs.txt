 - Received.
 Place your mobile devices on silent.
 This session will begin shortly.
 (people chattering)
, - Thank you. - Thank you.
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people coughing)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 (people chattering)
 - All right, welcome to GTC. Please take a seat.
 I see you coming in, but I think we should get started
 because we are, the clock's already started.
 Well, my name is Chinder Patel.
 I work at NVIDIA in the NVIDIA AI software team,
 and it is my pleasure to have this great panel of people
 here today.
 A few housekeeping items I need to cover first.
 This particular session will be recorded.
 It is also available online,
 so there's a livestream going on.
 The recorded version will be available to all of you
 within the next 72 hours,
 and then for the rest of the public,
 it will be available online on demand
 in a month or so.
 If you haven't downloaded the GTC app, please do.
 Please do so because it has all the latest updates
 available there.
 It will also obviously provide new sessions,
 and that would be a great place for you to provide feedback
 for this and all of the sessions.
 And we really encourage you to provide feedback
 because that is how we can make GTC even better.
 Today, from five to seven, there's Happy Hour.
 There's also the exhibit hall,
 which is just down on the other side.
 It will be open from 12 to 7 p.m.
 So let's get started.
 This particular panel session is on practical strategies
 on building enterprise applications
 powered by large language models.
 I will have about five minutes to ask questions,
 so we have questions from online as well as from in the room,
 so we'll probably have a chance to have a couple of you up here
 to ask the questions, and then the panel will answer them.
 After that, the panel will be waiting for you outside,
 so if you have additional questions, you can meet them
 and you can ask the questions over there.
 So with that, I'll introduce Parshad.
 He's the senior alliance manager at NVIDIA.
 And Parshad, take it over.
 Thank you, Titan.
 Welcome, everyone, to this panel.
 We're excited to have great discussions about frontiers
 in large language models.
 We have five speakers in this panel today.
 I'd like to introduce them shortly,
 and then we jump into questions.
 So the first speaker, Mr. Jerry Liu,
 the CEO and co-founder of Llama Index,
 the data framework for building large language model applications.
 The second speaker, Mr. Harrison Chase,
 CEO and co-founder of Linechain,
 which is a framework to make it easier to use large language models
 to develop context of our reasoning applications.
 The third speaker, Mr. Arden Jane,
 the CEO of Gleam, an AI-powered workplace search company,
 he co-founded to make it easy for people to find the information
 they need to be more productive and happier at work.
 The fourth speaker, Ms. Jane Scowcroft,
 who leads conversation on AI data strategy
 and other focus of generative AI at NVIDIA,
 Jane brings international experience in the intersection of deep research,
 breakthrough technology, policy, and new products
 across all types of organizations, startups, government, and nonprofits,
 and the industry.
 And our last speaker, Mr. Joe Comby,
 who is the Senior Director of Product Management at NVIDIA,
 where he focuses on core deep learning software
 covering large language models, speech-to-text, text-to-speech,
 recommendation systems, and inference serving.
 Welcome our speakers. Thank you.
 [applause]
 So I'd like to start the discussion by asking some questions from Jerry Liu.
 You know, retrieval of augmented generation, or RAD in short,
 has gained widespread popularity as an effective approach
 for grounding large language models with external data sources.
 However, the emergence of long-context models like Gemini
 that can process up to one million tokens
 seems to disrupt the current paradigm.
 From your perspective, what implications do these long-context elements have
 for the future of RAD systems?
 And the second question, how can enterprises stay adaptive
 as the technology landscape continues rapidly evolving in this space?
 Yeah, I like that question. We got right into it.
 This is rag-dead.
 So basically, we actually had a blog post on this a few weeks ago, actually.
 And I think this is really triggered by the emergence of Gemini 1.5 Pro,
 which, if you don't know, has a 1 to 10 million context window,
 so people were just dumping entire textbooks in there
 and then having it synthesize information and recall stuff
 from the stuff that you could dump in there.
 So it's actually, at first, very impressive.
 They did the needle in the haystack experiment,
 recall a very specific piece of information across entire books
 or entire, for instance, SEC filings.
 And so we were thinking about it.
 And basically, our position is the following.
 One is, just as a framework, we're in the business of basically
 trying to create a really nice framework for developers to use
 to connect algorithms with data.
 We're kind of agnostic to RAG.
 We spend a ton of resources on RAG, but a lot of what we want to build
 is basically whatever new use cases emerge from long context models,
 we want to be there to help provide the tools for any user
 to connect their outlines with any sort of unstructured, semi-structured data.
 That being said, I think while Gemini 1.5 Pro does eliminate certain challenges,
 and especially if you think asymptotically,
 as these context windows get longer and even infinite,
 there will certainly be things that will go away,
 but there will still be remaining challenges for enterprises to resolve.
 So let's talk about that.
 Let's talk about what are the kind of like RAG techniques
 that people might not have to think as much about anymore,
 and what are the RAG techniques that will still remain
 and then continue to be a thing going forward in the future.
 So what long context windows enable is that you probably as a developer
 in the one year frame don't need to think as much
 about really fine-grained chunking for your outlines.
 Right now, one of the biggest pain points of trying to do RAG
 is figuring out how to slice up your text into a bunch of little blocks
 of smaller chunks, right?
 And especially as costs go down and as long context models
 have longer context windows, you can start doing retrieval
 at probably the document level as opposed to the chunk level.
 And what this means is that a lot of the really precise fine-grained
 chunking techniques, you probably need less of in order for the outlet
 to actually understand the information.
 And I'd argue that's actually just a really good thing.
 The second thing is that as long context models become more of a thing
 and costs and latency go down, you probably also don't need to
 over-engineer specific retrieval algorithms just to get information
 from a single document.
 A lot of times, a strong baseline will probably be roughly
 some top-K retrieval over documents plus some basic re-ranking layer.
 I think right now, because you can only cram so much context
 and you have to be conscious of costs and latency,
 a lot of these things you basically have to figure out how to smartly
 make use of disparate context for each document to try to fit that
 in a context window.
 And as these windows get bigger and costs go down,
 you probably have to think about that a little bit less.
 So I do think in general, AI engineers will have an easier time
 building context-oriented applications over larger amounts of data.
 There are, however, some challenges to long context windows.
 One is costs and latency are still a thing.
 They will probably still be a thing for a year or two.
 And there is this idea of a KB cache to your LLN,
 which can cache the activations of the transformer layer
 so that if you upload the same documents,
 you don't have to rerun the entire thing during inference.
 That said, if your document corpus,
 and usually most enterprise document corpuses,
 are in the hundreds of megabytes to gigabytes of data,
 and the whole promise of LLNs is to basically just unlock insights
 from this entire treasure trove of data.
 And so long context windows aren't the entire answer to that solution.
 You still need to figure out a way to process your data
 to retrieve the right information to basically figure out
 what is the relevant context from my entire enterprise knowledge base
 to feed into the prompt window to answer this existing task.
 So this whole idea of retrieval will probably still remain a thing.
 The idea of hand engineering specific techniques for chunking
 will probably go down in importance.
 And I think the last thing I'll say is just on a very technical level,
 I think it's very interesting.
 I think embedding models technically have not yet caught up
 in context lines with LLNs.
 And so basically if you want to do really long context massive document retrieval,
 you're going to have to figure out interesting ways
 to try to embed and index your documents to still do vector search
 if you still want to return the entire document
 to the LLN during synthesis.
 Thank you so much, Jerry.
 So the next question I want to ask from Harrison.
 A new emerging paradigm that is going beyond RACS is agent AR.
 Having AR agents that can use RACS as tools and LLNs
 to automate reasoning, planning, and execution.
 What are the most critical gaps we need to address
 in the underlying frameworks and AR models
 to truly enable robust, reliable, and trustworthy agent AR systems?
 Yeah, that's a really good question.
 And maybe just give me a very brief overview of agent AI systems.
 So the way that I think most people think about agents today
 is there's a language model.
 It interacts with the environment in some way.
 It decides what to do.
 It decides what function to call, what tool to call.
 It gets back a response.
 And that continues to do that until it's answered the user's question
 or completed the user's objective.
 And so the central idea there is to put the language model
 at the center of the system where it can interact with other components.
 And this isn't a super new idea in the warp speed of gen AI.
 So when lane chain came out in November of 2022,
 we had some initial components in here.
 And other agent frameworks have kind of come out in the past year or so.
 The biggest one, of course, being kind of like auto GPT.
 And a lot of these take a very similar approach
 where they have the language model kind of running in a for loop
 interacting with the environment.
 Now, if you've used auto GPT, it has a tendency to go off the rails sometime
 or not accomplish tasks.
 And so maybe taking that as a baseline,
 and then we can talk about what are some of the gaps that are missing.
 I think one of the biggest gaps is at the language model level,
 and it's basically the ability to kind of plan and reason about long-term steps.
 And so I think some initial observations here,
 if you ask a language model to maybe make a single function call
 or to do one step, you can get it to do that with pretty reliable consistency.
 You still maybe need to provide some two-shot examples.
 We find that to be really helpful.
 But it can generally decide a single step to take,
 and maybe two, and maybe even three.
 But then you start getting to more and more steps,
 and you run into a bunch of challenges.
 The context window starts filling up.
 So the observations you get back might be documents from RAG,
 and it starts to take up more and more context.
 Very good question and answer about the long-context window of LLMs,
 and they are getting good at kind of like fact-based retrieval,
 but they're still not amazing, I would argue, at reasoning over large pieces of content.
 And so this idea of planning and observing over kind of like an extended period of interactions,
 that's really the main gap I see.
 I think the ideal agent is also the simplest one, where you just run it in a loop,
 it decides actions to take, it puts it back in.
 You don't have to do any prompt engineering to get it to explicitly plan steps.
 You don't have to do any prompt engineering to get it to explicitly reflect on steps.
 But those are some of the hacks that we're seeing people do in the short term to overcome these.
 So people are adding explicit planning steps to get around the issue of the LLM kind of like
 starts with a plan, but then goes off the rails.
 They're adding explicit reflection steps to get around the issues that the LLM sometimes thinks it's done,
 but it hasn't actually answered the original question.
 And so I'd say like planning is basically the number one gap.
 I think there are other things as well.
 I think maybe the second one I'd call out is a UX issue.
 I think it's still uncertain what the best way to interact with these agents is.
 There was a really great release last week, Devon, an autonomous coding agent,
 and they did a really cool UX where you could see the exact kind of flow of what the agent was doing.
 You could rewind to a particular point in time and then you could edit it.
 And while these agentic systems are not like 100% reliable,
 that rewind and edit capability is really important.
 And that's some of the functionality that we're thinking about building in
 and making first class into some of the frameworks that we're developing.
 And I think the last thing that I'll say is basically just personalization
 and kind of like learning of these systems as well.
 I think, you know, if you think about an autonomous software engineer that you ship,
 it's not going to know kind of like about the best practices at your company
 or about how you might want to do things in a particular way.
 But if you kind of like interact with it a few times,
 hopefully it can learn from that and start to incorporate that.
 And what's the right way to do that? I don't know.
 Is that few-shot examples? Is that kind of like allowing for editing of prompt instructions?
 Is that fine-tuning of models?
 I'm not entirely sure, but I think planning, UX, and then personalization
 are three of the big issues in the agentic AI that we're excited about
 over the next few months, years, however long.
 Yeah. Thank you, Hans. It was great.
 So given all of these, you know, exciting capabilities of LLNs,
 one thing that is often quite overlooked is trust and security of, you know,
 adopting these cool technologies and enterprises.
 So I want to ask from Arvind, who has, you know, years of experience in enterprise security,
 what are the most critical, you know, factors to ensure agentic AI application is trustworthy
 and is, you know, can create trust in the end users?
 Yeah. That's actually a big challenge for generative AI.
 I mean, all of us have seen, first, you know, the hallucinations, you know, that models, you know,
 can demonstrate and also the lack of predictability.
 They ask models questions and they can generate different answers every single time.
 So part of it, you know, when you think about security and trust, part of it is,
 can you actually, is the model giving you the right answers?
 You know, especially in an enterprise use case, you're going to actually do something with it.
 You know, this is a business critical scenario that you are in.
 And you have to absolutely be certain that, you know, whatever answers you get back from AI are correct.
 And then part of the security challenge is, like, how do you make sure that you're not leaking your sensitive data
 either externally or internally within your company?
 So those are the two key challenges there.
 So let's talk first about security.
 So when the models company, you know, came out and everybody was up there,
 like, you know, are we actually sending our private data to these models?
 And will this data be used in these public models in the future?
 That problem is actually largely solved at this point.
 Like, you know, as an enterprise, you just got to make sure that when you, you know, buy your licenses or services
 from these large model vendors that you're actually asking for zero-day retention
 and as a result, you know, ensure that none of your data is actually ever going to be used in training public models.
 But I think the bigger challenge for AI applications, you know, internally in an enterprise
 is how do you make sure that you, like, maintain, you know, your data governance and security needs?
 Most of the information inside the enterprise is private.
 Think about any document inside a company.
 Maybe three or four people have access to it, but not the rest of the company.
 So any AI application that you build internally, you have to make sure that it understands
 the governance models, the permissions and the ACLs that you have set in your enterprise.
 And so I think, you know, that's one of the things that you can do with, you know,
 AI-based architecture, making sure that your retrieval engine understands the user who's actually asking the question
 and then use their credentials to only retrieve information that they have access to.
 That's one of the key things that you have to implement, you know, in your technology
 to make sure that there's no leakage of information inside the company.
 And then the second thing on security that actually not enough to just obey our current governance.
 One of the things that we have heard a lot from our customers is that, you know,
 companies feel that, you know, they don't have great governance internet.
 You know, there's a lot of sensitive documents inside their company that are not adequately protected.
 Like, this might be a sensitive HR document, and it's sitting somewhere in the corners
 where nobody finds it today, so there's no issue.
 But, like, once you have the AI connected with all of this data,
 you know, at that point, like, you know, these things start to become a problem.
 And their customers, like, you know, ask sensitive questions in our system.
 For example, what's, you know, this person's salary, and, you know, suddenly they get an answer for it.
 Like, they're not in HR, they're not supposed to have access to this information.
 But what happened was that, you know, the information and the data inside the company was not adequately protected.
 So now once you start to think about deploying AI for all of your company information and data,
 you have to actually go up and beyond whatever governance controls you have.
 You know, your systems, your AI systems have to understand users, their roles,
 and sort of restrict content, like, you know, and cover for those governance gaps that you have.
 So those are some of the things, you know, on security that, you know, that you have to think about
 when you build AI applications internally.
 And then on the second question of trust and being responsible,
 the RAC has one of the key, you know, advantages of RAC was that you can actually ground the models to work on certain information.
 And, you know, you, you know, given a question that a user has,
 you retrieve the right set of information from your search or your dual technology and then make the model work on it.
 And you sort of, you know, constrain and guide the model to sort of go out of bounds of that.
 And that sort of largely works.
 So, so what we have observed is that, you know, in enterprise use cases, hallucinations are not actually the bigger problem.
 The bigger problem is actually, like, what I call garbage and garbage out.
 Like, you know, if your retrieval system is not picking the right information, you know,
 that could solve that task or answer that question that the user had,
 then the model cannot really do anything, you know, like, you know, it's being given drop, you know, poor input and it's going to have poor output.
 And that's where, like, I think, like, most of the challenges in these AI applications, you know,
 we're seeing customers run into.
 So that's where we sort of think about, like, your retrieval technology and is it capable?
 Is it, like, you know, and the products sort of go well beyond just, you know, semantic matching or vector matching.
 You have to think about, like, are you actually giving fresh and up-to-date content to these language models?
 Are you giving them content that's relevant, you know, to this particular person or the department based on what role or department they work in?
 And those are some of the other challenges that you have to solve, like, from a retrieval perspective,
 making sure that you are providing the models the right input.
 And when you do that, you know, that's sort of when you start to get this, you know, trustworthy, fresh responses back from the algorithms.
 And the last thing that you have to think about, like, you know, again, like, depending on, like, how much critical your application is,
 is, you know, don't trust the responses that are coming back from the AI.
 Because, you know, AI models are still sort of largely critical and can make mistakes.
 So you have to do, like, you know, really robust fact checking on every line of response, you know, that the AI models, you know, come back with.
 And there are technologies for that, you know, that can use the gain licenses.
 Like, for example, you can try to match, you know, the constant response of the AI, like, you know,
 embed those responses and sort of see, like, you know, if you can find references to that in the input that you provided for the model.
 So those are some of the techniques that you could work on to create a more secure and safe response.
 Thank you, Armin.
 So speaking to the point that Armin mentioned, you know, the importance of data on each model,
 how the critical role plays in retrieval of that generation,
 I want to ask questions from Jane who has a lot of experience in data strategies across different domains of general AI.
 So retrieval of the models play a crucial role in RAC systems.
 By efficiently retrieving relevant information from a large corpus of data and enhance the trustworthiness of the system.
 What are the best practices in collecting datasets for training and evaluating retrieval models?
 I'll take a quick step back and with all due respect to my esteemed colleagues in the video,
 I'm fairly sure I have the best job at the company as part of the data strategy team.
 We get to work with engineers, production engineers, researchers to define what is the best, most impactful data for whatever problem we're trying to solve.
 And as we've heard, there is so much changing at such a quick pace.
 We need to be relevant and appropriate about what data is necessary.
 I think there was always previously an adage in AI/ML, more data is better.
 I have a slightly different perspective where it's minimizing the data but maximizing the impact.
 So what is the highest quality, the most impactful kind of data?
 This is a perfect kind of role for a control freak like me.
 Because what is a model without the data from my very biased perspective?
 And so we get to work internally and then also externally with a lot of our great partners in defining what do we need to do?
 What does the AI industry, the LLM industry, the general AI industry need to do to take these technologies, these systems,
 the ability to chain everything together as far as possible to make it as easy as possible for enterprise to go that last mile?
 Because as we know, enterprise data is sacred.
 You are the only ones who know the data, the impact that it has, who has access to the data.
 This is a big topic that we've been tackling at the moment around access control for data.
 We should be able to see results from different areas or different parts of the knowledge base or the knowledge source.
 We can make decisions about that data.
 And so I think my favorite stakeholder is actually the enterprise.
 And understanding what are the pain points and how do we minimize the level of effort to make things as relevant and as useful for enterprise.
 Because these tools, these technologies are going to be game changing, are game changing, are going to be even more game changing.
 When I think about practical strategies, though, the area that I always start in is with evaluation.
 And importantly, it's something that has to be done and it can be done independent of any model, any system, any kind of implementation.
 Evaluation is you defining for your enterprise what is that gold standard, that gold set that you can evaluate any system against.
 In general, first kind of rag systems or retrieval systems, it looks like a triplet that's a prompt or a query, the response from the system, and then the references.
 So like where did I get this information? How do I justify?
 I think there's so much that we need to do in terms of reasoning, chain of thought as well that can come into that.
 But at the bare minimum, it's this kind of triplet.
 Defining internally, building that, you know, a set of a hundred, two hundred, a thousand, whatever that minimum set is for you is really, really key.
 Because it helps you understand for my enterprise, when I'm leveraging these systems, what am I asking it?
 What is my expected result? And where do I get that information from?
 And tap the shoulders of analysts that are within your organization.
 So business analysts, financial analysts, supply chain, HR, resourcing, whatever it might be, to understand what's the process they go through to make a decision, to make a recommendation.
 Because they're the ones who are kind of mimicking what Jensen talked yesterday about kind of simulations.
 They're the ones who are kind of simulating that kind of process, right, of where are they pulling information from.
 And so for me, evaluation is a place that is such an easy entry point and it's an important discussion to have before even looking at models, before even starting to play around.
 So for me, I think that the evaluation is a really, really, really key place to start.
 That is great. And also, how can incorporating metadata into these rank systems enhance the performance of retrieval models?
 So this is another area that we're very, very excited about.
 And this is where kind of long context doesn't really go far enough, right?
 Because you're putting the content in, but you're not putting any of the relevant information that might exist.
 But when you think about your own mental models, about how you find, source, compare different pieces of information within your organization,
 you're pulling in dates, authors, titles, but a whole bunch of associated metadata that puts that piece of information in context within your organization.
 By leveraging the metadata and how that piece of information sits in that structure,
 we can actually make much better, more informed recommendations about what is the important retrieval chunk,
 what is the proper re-ranking, and then what is the sort of final result that comes out from that general model.
 And I think for agentic AI, this is super important because you need to be able to understand the context of where that recommendation,
 where that action is going to be coming from.
 The metadata also comes into play for the data governance side of things. So who can have access to this information?
 If I'm allowed to have access to this pool of information, but not this pool, what kind of decisions might I be making?
 And so I think that metadata layer is really, really key.
 This is a challenge, right? Because garbage in, garbage out, there's a lot of really messy enterprise data.
 I'm sure there's quite a few of you who have seen some sequel tables where the columns are rows, or date one, date two, date three.
