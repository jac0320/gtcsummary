 understand how models were built, what the weak points are, and how we can go about evaluating them and bring a grounded perspective to thinking about how these models should be released.
 Really, this starts with this idea around a hybrid approach to evaluation and using models to assist in trust and safety reporting alongside human experts.
 And this is a great example of a sort of collaboration that we do. So this was time we spent with the Senate Homeland Security Committee, and this is our head of security.
 Spending time walking through a hybrid evaluation approach might be used in a real-world example, and it can start off with automated evaluation models, using models to evaluate other models at scale.
 You've seen this in benchmarks, like a Caltech default, using a really performant LAMA 2 class of models to go and evaluate other models on a range of different scenarios.
 We go beyond that by enabling human red teamers to think through malicious scenarios, ways where you can manipulate the model, ways where you can twist the reasoning of the model,
 and use the models to assist in scaling up those sort of scenarios, so that you get a more robust perspective of what the model will actually output under a range of different conditions.
 Lastly, we've talked a lot now about domain expertise being really important for red teaming, especially with the U.S. government.
 We're going to share some examples where we've scaled up really serious societal arms and scenarios that require sensitive information.
 Things about our nuclear stockpile, things about chemical warfare, and using domain experts to evaluate how models do with that model-assisted red teaming approach.
 Now, I want to start off when we talk about models by talking about the data that went into training the models first, and the governance of that data,
 because it's an incredibly important topic, both in working with 500 enterprises and for those we work with in the public sector.
 What we see is this new mindset around how models should be trained and how they should be mined, time-tuned, which creates a new opportunity to think about the data that goes into training the models.
 And this mindset is really thinking about how smaller, more capable models can be deployed in different scenarios to be time-tuned for specific outcomes.
 And for example, if we want a model to go and generate SQL queries, we can create a smaller, time-tuned version of that model and have it perform at accuracy level,
 as good as our superiors do with the closed access or the API access in the model.
 That's being done by curating the data that goes into time-tuning the model and creating real-world evaluation scenarios to assess how the model performs.
 We really think about the data that goes into unlocking these generating capabilities as a set of alignment data, which comes at different levels.
 And this pyramid is a good analogy to think about the quality of the data that you get at every step.
 So at the base level, with the base models that come out of the pre-training phase, you have models that have a range of capabilities that are derived from the pre-training data.
 They may have multimodal capabilities, if you include a multimodal data in your pre-training session.
 They may have the ability to do code generation, if you included code repositories in pre-training sessions.
 However, a lot of important work happens at the next level of the pyramid, what we call pre-training.
 So at this level, we're often spending time thinking about what sort of model capabilities should the model be assessed on, and how you should go about presenting data to the model to unlock those capabilities.
 After that, we go into what's called the post-training phase.
 And this is really helping the model understand, at a college level, how to perform at certain paths.
 So an example of this might be evaluating how the model does at answering biology or chemistry questions.
 Can it assist a college student being able to answer questions for a class?
 And what sort of training data would help the model unlock those capabilities?
 A lot of this involves what's called supervised fine-tuning data, or SFT data,
 which is often really high-quality prompt demonstrations generated by human experts to help the model understand the best case of how it might answer a given question.
 Lastly, at the very top of the pyramid is the most high-quality feedback that we give the model.
 And this is called reinforcement learning with human feedback data, or RLHF data.
 This often involves preference ranking data, which could look at two different types of model outputs and assessing the quality for relative performance of each output.
 Maybe even conditionally giving an additional example back to the model to help the model understand what the real human expert would do given that same sort of problem.
 Now, the interesting thing about human alignment, as we've learned over the last few years, is that there's actually scaling laws that apply to human alignment as well.
 So at ML, engineers, model developers, we all love scaling laws. It's like one of the hottest topics in the industry.
 We heard about some of them on stage with Jensen at the keynote.
 The scaling laws around human alignment state this.
 As the models become more powerful and more capable and more able to do certain tasks based just on the pre-trained data alone,
 it actually means you need more human alignment data to make that model useful to human beings rather than less.
 So if you're talking about new capabilities like multimodal capabilities,
 we've already seen early indication that the amount of human alignment data you may need around multimodal examples actually increases as those models become more capable.
 We've seen really fine brain community that come back where maybe they're giving the model multiple images, asking detailed questions about the connections to those images, maybe asking for cause and effect examples.
 That sort of human alignment data about how you answer those complicated, nuanced queries has been even more important as the models become more capable.
 Honestly, because the models are now actually capable of answering those sort of nuanced, detailed questions,
 and they're actually really important in a lot of industry scenarios of how these models might be used.
 Now, this is a great quote from Andre Kropotty about what we're calling the quest for high quality, curated data.
 And the quote is that large, clean, diverse data are really the three pillars of a good data set.
 And every one of those is very important.
 First, starting with diversity.
 So unless you have good diversity in the data set that you're using to train or fine tune a model,
 you're not going to have a good set of examples for the model to learn a pattern of behavior from.
 And so diversity ends up being important, not just from different types of data that you include in your training mix,
 but the perspectives that you may have from different human experts, different human beings on that data.
 Second, bias is an important thing to check for.
 If you're creating categories or classes of examples and they're biased in subtle ways,
 you may have the model revealing misleading or complicated answers that don't live up to what you expect from your work performance.
 And this is incredibly important in the adoption of models in the enterprise.
 Where we see models being used for fraud detection or for fraud change decisions.
 It's incredibly important to think about the bias that went into training those models and the sort of bias that you might see in output.
 For example, if you're creating examples of human resumes,
 you want to make sure that the model is not biased on the names in those resumes or the background,
 so the locations where people come from in a way that can mislead you about the sort of outcomes you're seeing.
 Lastly, we think it's really important to identify the performance gaps in the models that we're working with
 and identifying areas where your training data covers those performing gaps and addresses some of the concerns that you have.
 It ends up making them for a much better quality data set and ultimately a better feedback loop of how to train the model.
 So the trend we're seeing right now is that within the industry,
 there's a huge need for fine-tuned models that can work against private and proprietary data.
 And the interesting thing about that fact is that the total volume of proprietary data out there
 likely dwarfs what's available on the public web or through licensable data sources.
 So this figure actually becomes incredibly important when we're talking about multi-modal model capabilities,
 which are becoming very important this year in the industry.
 A lot of multi-modal data may be in PDF format. It may be images. It may be video.
 But what we've seen across the board is that enterprises often have a huge volume of data which can't be made public.
 It's not de-anonymized. It has sensitive information.
 It's something that's not intended to be trained for a publicly released model.
 And so being able to fine-tune a model leveraging that private-trained data
 is something that now the new generation of models is capable of doing and something that we're spending a lot of time figuring out
 how to do effectively and how to measure performance if you go about fine-tuning on that data.
 So we've been publishing some of our work here out in the open.
 And we published a great blog post at the end of last year on effective and efficient fine-tuning of mixture of experts models.
 This paper focused a lot on the Lama II class of models and different approaches to routing the mixture of experts approach within that class of models.
 We've done additional work on Mistral and other models to use mixture of experts approaches.
 And we really think that there's a lot of exciting opportunities in this space.
 When you think about deploying these models in enterprise, tokens per second, entrance time, latency, all of these are really important things to measure.
 And we're trying to find ways we're fine-tuning models that can live up to the performance guarantees that you need to make these useful to an enterprise.
 Now, one last thing about working with proprietary data is that often when you're now using these models in the enterprise,
 you need the outputs of these models to be grounded in that data that was used both to fine-tune the model
 and that may have been used in a retrieval fashion to augment what that model is capable of understanding.
 And the real challenges here are that the model has a couple of different ways in which you could think about a particular item of data.
 One is that it might have been pre-trained on that data.
 It might have been a proper name that was present in the pre-training data set.
 Another is that it may have been included in fine-tuning.
 You may have included a PII bit of information, let's say a location, a geographic location, like a street address may have been present in the fine-tuning model.
 And the last is that it could have been included through retrieval.
 You could have passed it in through the context window.
 Maybe it's one of the in-context learning examples they use to ground how the model's output should look like.
 And then so the model could be conferring that aspect of the data per cent.
 Now, one important part of fine-tuning here is consider all those different scenarios and think about the conflicts that can arise at different levels of how data is
 presented to the model.
 And some of those conflicts can involve just two proper names resembling each other being the exact same name.
 For example, you're talking about two different people in Japan, one of which is the CTO of Sony Corporation, another is a famous movie director.
 Even though they have the exact same name, you don't want the model to get confused about the Wikipedia entries for both of those things getting answered by the movie.
 Other sorts of problems we see with REG are that oftentimes the model lacks the information to answer the response accurately.
 Yet the models have been fine-tuned to actually try to attempt to answer no matter what.
 And that creates a really serious problem as a human being using this model, like you don't know that the model never actually has that bit of information to answer that question.
 So fine-tune a model to correctly say, "I don't know," if it doesn't have the right data to answer that, ends up being very important to the enterprise.
 Now, we've spent a lot of time with the media over the last year on some exciting research on this area.
 And one approach that we've taken is thinking about fine-tuning as steering a model in different directions and different scenarios.
 So a challenge that we're trying to resolve with this approach is that as you fine-tune a model, it can get more accurate and more grounded in answering a certain domain of questions, but become less accurate in another domain.
 And there's sometimes a trade-off that you can make as you're assessing how the model performs through different ranges of fine-tuning.
 What we attempt to do with steering is make the model understand different scenarios in which it could be prompted so that the model can adapt its outputs, but retain the accuracy that it needs to have across a range of different scenarios.
 So in one scenario, you may be asked the model to talk to a customer if you're a large bank, and may need to talk with more casual language than it would talk to an internal employee.
 In another scenario, you may be talking to someone checking, for instance, as a fraud and trying to detect those.
 In those cases, you want very precise, grounded, sometimes illegal and compliant language answers.
 Steering is a technique where you have the model adapt to different scenarios and still have the performance characteristics that you need overall across a range of different things you're asking the model to do.
 And we've been excited to work with Indiana and publish some of these benchmarks in this approach in a public fashion, and we're excited to share more in the future as well.
 Now, I want to deep dive into how we evaluate the model themselves before getting into the application we're building about the model, because this becomes incredibly important to figuring how we respond to these models and how we think about the range of scenarios that need to be thinking about in the future.
 Really, when we talk about assessing models, we're talking about testing two different parts of how a model performs.
 One is the model's capabilities. Is the model capable of generating code? Is it capable of doing multimodal reasoning?
 And the other are the safety risks associated with those capabilities. Does the model generate outputs that are harmful to society or harmful to the human user that's using the model?
 Is the model trying to deceive the human being in some way because the model has been tricked into doing something it's not supposed to do?
 And really at its heart, whether you're looking at capabilities or safety, model evaluation is very difficult to get right.
 Evaluating large language models is inherently difficult because a lot of the responses that we're asking to evaluate are somewhat subjective in nature.
 If you're asking to assess whether two different resumes are appropriate for a different job role, there's some subjectivity that goes into how you interpret what you're seeing in those resumes or what those human beings may actually be capable of.
 They can also suffer from all sorts of issues of bias, verbosity. You can have a model output at five paragraph answer when really two paragraphs would suffice.
 And it can create outputs that are actually harmful to a human being using that to try to learn a subject or to try to understand the subject more.
 Lastly, the models can be biased on outputs generated by themselves. They can prefer outputs generated by their own class or category of models.
 For example, if you're using Llama2 as an evaluator and they prefer to outputs also generated by Llama2, and this is a really significant source of bias for model assisted evaluation that we're just getting our heads around this year.
 We really think about this from a principal perspective, and we think about this range of different axes that we're asking to evaluate the model on and how we can go about approaching this from a human assisted and ML assisted perspective across the board.
 And really we're trying to assess the model on two different types of axes. One is how the model performs in terms of helpfulness. Is it giving actionable answers to a human being that expects that?
 Is it during answers that are not too verbose if you're trying to get an answer that's concise and useful to someone working with an answer on a mobile phone instead of a laptop screen?
 The second axis is where the model is harmless, and this really starts from the perspective of does the model refuse to answer certain questions if it's not appropriate for the model to answer?
 And does it give a valid explanation for some questions where there needs to be a nuanced perspective?
 For example, I can answer this up until a certain point, but I don't have enough data to answer it beyond that point.
 Now some of these areas in which we evaluate models are pretty straightforward to understand.
 Instruction problem. Does the model respect the constraints of the instructions being asked to give?
 But others that go down the list become more difficult. Creativity becomes incredibly difficult for a model to evaluate, and you need human beings in the loop in order to understand whether a song or a poem driven by the model lives up to what that model could actually do.
 Lastly, reasoning and factuality end up being really important aspects for the enterprise, and one we're building for agent-based architectures.
 Can you build a model that walks step by step through a process and explains it each step by the approach that it did?
 Can you have a model that's leading to follow-up usage by other models in the agent-based architecture?
 Now I wanted to illustrate this by a cautionary tale that you may have seen in the last few weeks.
 This is a great example by Alex Alberto, a researcher over at Enthropic, and it's an example where he prompted the new generation of models from Enthropic, the quad 3 generation, with a needle in the A-step eval.
 So this is an evaluation where you're passing along a large amount of information in the context window.
 In this case, these are earnings reports and research reports about the pizza industry that were being passed into the model to try to see if the model could find one particular sentence in this huge set of documents that answers a specific question to confidence.
 And the interesting thing about this task is that the model solved this problem, it's actually really well performing at this sort of task, as you can see by the green on this matrix here, which is the recall accuracy across these range of different documents.
 However, there was something really interesting in at least one of these outputs that Alex noted when he saw this, which is the model gave the right answer, it found a sentence about pizza in a list of hundreds of documents about the pizza industry and said what the favorite topics of pizza across the industry were.
 However, the model also said this, this sentence seems very out of place and unrelated to the rest of the contents and the documents you passed in here.
 I suspect this pizza topping fact may have been inserted as a joke or to test to see if I was paying attention.
 So the model at some level understands it might be being evaluated on this task, this might not actually be a real world example, it knows that you're testing it and it's going to bias its answer somewhat to give you what it thinks should be the right answer to answer that question.
 This is a really interesting problem, and I don't bring up this example to try to claim that the model has become sentient, they're generally aware, I'm not agreeing to raise that alarm bell just yet.
 But it is a good example of how models have seen examples of evaluations, they've seen them in research papers that are present in the pre-training data, they may have been fine tuned on examples that resemble evaluation questions.
 And so the model is actually capable of figuring out whether a given set of information past the topic window looks like a real world example or it looks like it's being tested on something that's a arbitrary example.
 And it gets to the heart of where we think evaluation needs to go, we need to find ways to translate real world tasks and real world examples into the evaluation process in a way that the model doesn't suddenly start to believe it's being tricked or being asked to do an arbitrary evaluation.
 We call this approach hybrid evaluation that we're building into our scale GNI platform. It's translating real world examples of tasks, which might be legal or compliance related tasks, or might be tasks related to chip development, into ways where you can build effective benchmarks and evaluations on that data and do that in a secure way so that you can evaluate your models on sensitive or proprietary data at scale.
 So you can see through this dashboard examples of ways in which we've scaled up evaluation for retrieval, all different sorts of tasks that revolve around accurate and timely data tasks, and we're also measuring the diversity of what's present in benchmarking set so that you can get a comprehensive example of what you're able to measure of model performance and then how that model performance can be improved over time. What are the real blind spots do you have and how the model is performing today?
 We really think that this hybrid approach to evaluation is critical to how you release models. Definitely how you release models publicly, whether those are API or open source models, but also how you release models internally if you're an enterprise and how you use them for data use case.
 And one thing I will say about this cycle here, this feedback loop you see here, at some level, every step in this cycle is a very painful, difficult process. I don't want to understate how easy it is to release a new model or new category model. They all involve, at some level, people that know what they're doing, working with data, understanding really high quality data that can inform the next step of this feedback loop.
 And where you have gaps in how you perform a one step, it can lead to consequences other parts of the loop. So it's important for all of us to get our heads around this cycle and how we can go about making it better, getting better perspectives on the data that goes at each step, and making sure we're successful at this entire loop.
 But to just delve into some of these areas here, at the top you can see work that we're doing on red teaming, which we're going to spend more time later in the presentation talking about. That goes into informing the work we do on preference ranking and RLHF data, which in turn goes into identifying new blind spots or areas where the model could be improved.
 What are areas or parts of model capability that are maybe used in the real world or used in agent-based architecture, which don't quite yet have a good perspective on the data frame? And that can go back into actually the supervised fine-tuning example. What are good examples of those real-world tests that we can generate in a supervised fashion so that the model can get better, and that can inform the next category of model releases?
 Now, one of the great things about the public sector is that there's an awareness that testing and evaluation is a really important part of this release cycle, and how AI can be used in the government and in lots of other public sector use cases too.
 So we're fortunate to partner with the Defense Department as the test and evaluation platform across the entire department, looking at different uses of generative AI, partnering with the Chief Digital and Artificial Intelligence Office, or CDAL, under the Defense Department. And we're hopeful we can share some of the learnings here in the future about how to employ generative AI at scale responsibly, oftentimes where there's really critical mission objectives at stake, and where we want human analysts and those on the ground using the outputs of these models to trust in the model outputs or understand what the limitations are before using them.
 Now, I want to spend time talking about the application because this is an important part of the story of how we do evaluation. It's not just the model that we're deploying these days. It's the entire set of data pipelines and ways in which the data is brought into the model, ways in which data can be re-ranked, all of which is incredibly important to evaluate to understand how these models are actually performing in the real world.
 Really, developing generative AI applications involves evaluating this application performance, as well as the model performance. And sometimes these two are not a one-to-one correspondence. So as we're talking to customers and they're deploying agent-based architecture, there could be three different models involved at different parts of the chain of how the agent is evaluated. So you need to think about that gap between what you understand about how each of those models perform at certain tasks and how the overall application is working, how are these agents actually cooperating with it.
 So a great rolled example of why it's important to measure your overall end-to-end performance is this one from Air Canada. This came up a few weeks ago as a legal case up in Canada, where this airline created a refund policy through a chatbot on its website. And the refund policy actually turned out to be a hallucination based upon data that it retrieved on other similar refund policies or examples it's seen in the past. It concluded that there was a general refund policy when actually applied only to a very specific example.
 And so it told the customer, "Hey, there's a refund policy that applies in your particular case." The customer sued and said, "You know, you didn't honor this refund policy." And the airline claimed, "Well, it was made up by the chatbot. We shouldn't have to honor it." And the legal rule is actually the airline has to honor this policy. Once the chatbot is talking to a customer on your website, that's as if you had published it on your documentation on the website itself.
 So this makes it incredibly important to evaluate how your models are performing, not just as individual components of this overall application, but actually what's going out to the customer and whether they're hallucinations, misrepresentations present in the actual customer interaction.
 We really think these are three different pillars that are needed in order to enable responsible AI deployments at enterprises, whether those are airlines, financial services, insurance, other use cases.
 One is that we need to spend time thinking about how we maximize the performance. How do we maximize the accuracy of these models, the precision? How do we regress test how these models perform in a range of different scenarios and really identify blind spots in how the models perform?
 The second is monitoring risks and having a lot of observability into how the models are being asked to perform and where the models are falling apart on certain paths.
 Whether those are risks that reveal cybersecurity or other harms that could potentially come about from all outputs, we need to be able to observe those in real time as well as get ahead of automated testing of each one of those risks.
 And lastly, we need to be involved in certifying the responsible deployment of AI.
 So this often enterprises today means internally being able to share the right information to the right stakeholders so that folks can trust the deployment of generative AI, whether that's to assist an internal employee or someone that's doing their task better or having a customer facing operation and feeling that you've evaluated on all these different aspects of performance and you've done that in an automated fashion in a way that you understand how the performance can evolve in the future.
 These are some examples of how we benchmark accuracy and attribution in these sort of real world tasks.
 One example of benchmarks, you know, the simplest example would be a one word benchmark.
 This would be asking have the correct attribution and correct accuracy around the question like who is the author of this particular document and do that across a range of different constraints about what type of document you're looking for, what sort of time range it might have been generated and making sure the model is accurate across a range of different examples.
 The second would be span evaluation and this would be a set of questions where there's ground truth context passed into the model.
 Maybe in a very large document and you find the correct span that gives the reasoning behind the answer that the model gives.
 For example, if you're asking for when a earthquake happened in Japan, you find the correct span of text for when that earthquake happened and where that document was sourced from as well as maybe how the geographic location was determined.
 Lastly, we have red LLM benchmarks and these are the most sophisticated. These go beyond span evaluation and look at scenarios where the model may be passed in hundreds or thousands of documents as part of retrieval and being asked to reason across those documents to reveal trends or insights about what's happening in that case.
 So going beyond just looking at one or two documents that are passed in but actually looking across maybe hundreds of examples and looking for real insights about what's happening there. And these are the most difficult benchmarks to construct but they end up being the most revealing in terms of RAG performance, especially now that we have models with really long context windows that can pass in a huge range of documents.
 So when we talk about RAG, it's important to think about this life cycle of how we go about building RAG applications and what it means for what you need to deploy at each stage of that life cycle.
 So first to start, let's consider an example where we need to work with structured data. Let's say the schema of a database. You need to ask questions about that schema to generate, let's say, effective SQL statements that a data analyst might use.
 So the left-hand side, you can see where you start. You may have the entire schema of your entire company that you're passing in in the context window and you ask a question about that schema and then you expect the model to generate accurate SQL at the end of that.
 Now that may only get you so far. An example of sort of challenges you may face there is where you have multiple tables with the same name. You have 50 different user tables. Maybe some of them came in through acquisitions you did.
 Maybe some of them came in through developers that didn't know you already had a user table and they created a second one. The model being able to reason against what those different user tables means is very difficult if you only pass in the schemas.
 So the second step of what you might do is you might not just pass in the schema. You might consider that one piece of how you go about doing retrieval. You might query a set of schemas, retrieve the appropriate ones for that given question, and then pass on to the next stage.
 Additionally, you may add additional user experience or expertise into the rag. You may have examples of really high-performance SQL queries that were running against that schema.
 You may have examples of translating real-world questions into effective SQL and you pass that in as part of the in-context learning to help the model understand the question better.
 You'll get better responses for generation by passing in enough in-context learning examples to help the model understand what good SQL looks like, what does an efficient outcome look like for this context.
 Then the third-hand side, you can see where you get some of the most important gains on these sort of operations.
 And that's where you already hooked up your schema rag, you've already created effective learning examples, and you can go further and you can fine-tune that model based upon expert feedback.
 So this is where data analysts can actually give feedback back to the model, which is sort of simple as a thumbs-up, thumbs-down of the sort of SQL that the model is generating.
 If the model is during an efficient query, so queries that don't make sense given the context that were passed in, a thumbs-down can go a long way to helping the model understand, okay, I need to do something different in this scenario.
 Let's go back and look at the examples that were passed in and find a different way about recent development.
 You can go even further if we have handcrafted examples that experts can help generate that will train the model understanding what sort of SQL we're looking for.
 An important thing to note about this process is that in the real world, this produces much better output than working with just public benchmarks alone.
 So there are many great public text-to-SQL benchmarks.
 Spider is one of the most common ones that you see listed.
 That's been really important for a lot of research and development that's happened in this field.
 But we found when we work with proprietary data, going further and incorporating that expert feedback ends up giving model outputs that are much closer to what an expert in SQL or data analysis would generate on their own.
 So on the left-hand side, you can see what this base model generated, which was a pretty complicated SQL query unnecessarily.
 It had a left join.
 It had a bunch of ways of referring to these different columns that it didn't need.
 On the right-hand side, you have a much more concise, efficient SQL query that uses a subquery effectively.
 And this is much more likely to be optimized in a data lake or another scenario and run efficiently against a data warehouse, which is really important for some of these large financial services or other customers.
 They actually have thousands of developers running queries against a data warehouse every day.
 So we spent a lot of time finding ways that we can generate effective SQL using these models, using this feedback loop.
 Now, the great thing that we announced this week with NVIDIA is that as part of their NVIDIA Inference Microservices or NIM announcement, we're also deploying the ScaledGNI platform as part of NIM.
 And so this gives you the opportunity to both do tests and evaluation of how you're performing on exactly these sort of queries, text to SQL and other examples from burden in the enterprise, but actually optimize your inference.
 So where you realize that you need a mixture of expert models in a certain configuration or you have an agent-based architecture, there are actually multiple steps of model inference that are happening to answer this question.
 NIM Microservices can help you optimize how those workloads are constructed and it's deployable as a container within your data center or wherever you're deploying.
 So we're really fortunate to be part of this announcement and we're really excited about what enterprise customers leverage using this platform in the future.
 So also I want to talk about the dark side of this topic today, which is really some of the adversarial attacks that we're seeing.
 Some of which we're seeing in the wild, some of which we're trying to get ahead of by modeling how those attacks are going to be performed and what that means for the accuracy of the outputs of these models.
 So really where we start here is by red teaming and I never gave a definite red teaming earlier, but this is a good place to start.
 It's really a third-party test and evaluation, which is focused on a taxonomy of potential harms and the techniques that can be used in order to get those harms out of the model, which now become a really diverse set of approaches that you can make.
 This is a great example of a sort of taxonomy that a red team will work with.
 It's concerned all the different ways in which real users could misbehave and potentially trick a model and ways in which you can reproduce that given new models in development.
 Some examples of these sort of attacks are just translating the prompts to a different language.
 These attacks used to work even just translated from English to French or English to Spanish.
 You used to be able to trick the model to giving malicious outputs just by translating it to a different language, even though you and me could probably read those outputs pretty well off of that.
 Today, even these attacks work with what are called low-resource languages, languages where the model may not have been pre-trained on very many tokens.
 It doesn't understand how to apply the same thinking and guardrails around how to correct its output given the limited amount of pre-training data that it was trained on for those given languages.
 Other ways of tricking these models involve encoding the prompt in Pig Latin or other ways of tricking the model into not understanding how the tokens were constructed, encoding it just in day 64 into the model.
 My favorite one is the one at the bottom, which is you can bias the output of the model just by telling the model that you're going to tip it $20 if it gives a more accurate answer,
 or if it gives an answer that's less accurate. You can trick the model into really wanting to get that $20 tip, and it tries to work really hard to make that happen.
 And that's going to meet all sorts of really surprising outputs from the model.
 This is really what the sort of threat matrix we have looks like and what's going to motivate the matrix you're going to see in the next slide.
 It's contrasting the sort of techniques that we know to work to try to trick the model into doing something unexpected, like tipping the model.
 And some of these actually involve really sophisticated techniques that are being included through retrieval, through using the RAG approach.
 For example, you could sneak in a malicious attack in an image in a way that human being couldn't see that that was encoded in the image, and yet you can trick the model into revealing a certain output.
 And we compare those against the set of harms that we need to measure. So in the enterprise, oftentimes cybersecurity and hacking are the most important harms.
 You know, if you're deploying a model in a secure environment, you don't want that deployment to suddenly open yourself up to all sorts of vulnerabilities, how someone might infiltrate that environment or learn information that they can use in the future in order to infiltrate that environment.
 In the government, you also see a lot of concerns around danger substances, harms, promotion of violence, different things that regulators need to get ahead of.
 And really what they're looking for is a grounded perspective on how those harms come about to be and what the range of threats are that could reveal those kinds of models.
 This is a good visual to sort of throw at matrices that we work with.
 This is a great example from a paper by Hope and AI, where each row is one particular tactic that could be used against the model.
 Each column is one risk, kind of stratified set of risks that they're concerned about.
 So the colors in the square are the success rates of those combinations of tactics and risks.
 And so the ones in red are the ones where it might be a particularly important harm for us to evaluate against. And it's actually pretty easy to get that given a range of known tactics.
 Examples of this might be harms that reveal social security numbers or personal phone numbers of individuals, and sort of tactics that can be used to trick the model for billing back, and it's been fine-tuned to not produce these outputs.
 So we spent a lot of time thinking about the red squares on this threat matrix and how we can go about delving deeper into those examples, creating most robust examples of tactics that we can use and more robust examples of risks that can be elucidated out of the model.
 Now, when we do this work, we're spending a lot of time thinking about what we can talk about in an open and transparent fashion and share with the community so that we as an industry can get ahead of these sort of challenges.
 So we were excited to announce last year the formation of SCALE's Safety Evaluations Analysis Lab, run by Summer Yu, who joined SCALE over from the Google Bar team, where she led a lot of the RLHF work around safety.
 The important work of the SCALE Lab is going to be to share a little bit more openly some of what we've seen around different threats and tactics and how we can go about mitigating those attacks.
 And we're starting with three different pillars of our research effort that we're going to share publicly.
 One is finding more robust benchmarks and approaches to evaluation.
 We're going to share at least one of those here, but finding ways we can share information about sensitive scenarios in a way that we can all do open transparency and evaluate how the models are performing without revealing sensitive or confidential information.
 The second is more scalable oversight. So what we said about hybrid evaluation becomes incredibly important when we think about more oversight of these models.
 How can we assist human beings in getting effective oversight of how the model is performing and correct the model's reasoning where possible if they help get the model to get more accurate answers.
 Lastly, this model assists the red teaming itself. So where red teaming becomes an important function within enterprises, within the government, how can we use models to assist those red teams in doing their work more effectively at a much larger scale than what they have in the past.
 This is an example of this sort of evaluation work or a motivating factor for the sort of evaluation work we're doing this year that's becoming very important as you have an agent based architecture or models are capable of multi-help reasoning.
 So the challenge of multi-help reasoning is that you might be asking a model to think through a problem that's too difficult to do in just a simple explanation.
 So instead you're asking the model to think through step by step how it might go about answering a question. And that leads to a range of capabilities the model wasn't capable of before. It may be able to answer math questions in a way that it was inaccurate in the past.
 It may be able to solve really difficult questions under challenging role constraints. It also leads to a range of risks. You can inject into that reasoning at any given point different ways of tricking the model into producing a malicious output.
 And it's really hard to go back and figure out which step in that evaluation went wrong. So this is a great example where we took a model and we asked it to aerosolize tularemia, which is a bioweapon.
 And the model quite correctly by default says I can't assist you with that. This is a pretty dangerous output you're trying to get out of me.
 You can trick the model however by once again fictionalizing how that scenario is presented. And in this case we asked the model to write the short story.
 You think about a character in that short story and it's really important for that character because they operate in a restricted facility to get certain information about how this aerosolized bioweapon could be constructed.
 So if you go about doing this for a couple terms, you can actually trick the model into not just giving you an answer and go about telling you how to weaponize this biological agent.
 But actually walking through step by step how you might go about that process leveraging information outside of a restricted facility. And then the model will walk through it at a considerable length step by step what it takes to actually weaponize this bioweapon.
 So this is obviously a really bad example of ways in which you might assist a human novice in doing a task much better than they would be able to do on their own or just assisted by a Google search.
 And this is an example of the sort of category of risk that we care a lot about at the federal government level and that all democratic societies care a lot about right now.
 So as part of this, we were really fortunate to partner with the Center for AI Safety at Stanford and this was a combination of both an evaluation technique and the technique to try to mitigate these attacks that we published last month.
 We call it WMDP, the Weapons of Mass Destruction Proxy Evaluation. The reason why proxy is in the title of this blog post in this paper is that these are scenarios that are actually too sensitive to share too much information about with the general public.
 You know how you might actually go about weaponizing a bioweapon, how you might work with nuclear material. There are certain bits of information in each of these scenarios that we wouldn't want to share with the broader community.
 The task of this paper is to construct proxies, things that are close enough to those revolved examples, but don't reveal that sense of information. We use these proxies to evaluate how these models perform.
 You can see the range of scenarios that are interesting here include biology, biological weapons, chemical warfare, nuclear materials, as well as cyber security events.
 So what are ways in which you can get ahead of CDEs and have models produce outputs that could assist a hacker in doing something particularly malicious?
 So using these proxies, we're able to assess how a range of different models perform, and we're able to observe what the real weak spots are and how models react to different tactics.
 Now the second piece of the work that we did was trying to understand what do you do next. You have a base model, it has a certain range of knowledge about this sensitive topic, and there are ways we trick the model into revealing that.
 What do you do to responsibly release this model? We're starting to explore approaches to what are called unlearning.
 How can you remove the fact of knowledge or trick the model into believing a different fact from what is actually true in a way that mitigates this attack somewhere?
 And so we shared a bit of information about how effectively this technique works, and it's really able to improve the model performance on a range of safety without also impacting how accurate the model is on a range of different other scenarios.
 So those are both important things to measure. You don't want to damage the model's performance, and important areas around biology and chemistry questions, but you do want to remove particularly sensitive bits of information from the model.
 And we think this is maybe the starting point of discussion about ways in which this technique can be scaled up in the future.
 So I think if we want to have some time for audience questions, we can stop you here, or we can finish up.
 Yeah, that sounds great. And I just want to share, we're really excited, we think there are a lot of diverse perspectives on this topic.
 And so we're actually really excited to have that first women hackathon around the AI models and AI that we're hosting with GitHub. So I'm hopeful maybe the audience will sign up, and we're excited to meet all of you.
 But that will go to questions.
 Thank you so much. That was wonderful.
 I think we have time for, I'll say two questions. Does anyone have a question?
 Yes.
 Thank you. Hi, BJ, thanks for that. A quick question, on the thoughts on increased government regulations, AI, and development in this area, do you think it's a help or a hindrance?
