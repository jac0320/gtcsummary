0:00
competition that that's really one of the first silent models they're gonna hear from context. So, Chris, we've talked a little bit about birds and how that compares to some of the other ones that we're using. Yeah,

0:10
certainly. So there's a lot of love language models out there even more so than, than the chatbots. And it gets really confusing. They basically fall into three families. There are models like GBT, which stands for a generative pre trained transformer. There are models like Bert, which is the fiber actual encoding representative transformer and the models that are full architecture transformers like T five and different between the groups. The first major difference is how they're pre trained, right? So before you find to a model and your specific task is pre trained on billions of texts to get a general understanding of language, and Bert is pre trained by showing a lot of text and then randomly words are hidden. And then Bert needs to use the birth model like, like her need to do the words before and after the hidden words to try to guess what the what the hidden word is. This is an auto encoding task. And at such Burg understand vocabulary very well structure semantics now, GBT like models went during their pre training, basically a lot of text and they need to predict the next word. So as such, they're very good at flow and what comes next. And then, in addition to the differences in pre training, there's also differences in the architecture. So a full transformer has an encoder and a decoder. And this is group three models like T five. Now for it's just an encoder. So input text and it goes through a series of self attention layers and outcomes. A mathematical vector called an embedding, that embedding represents the text. Now GPT is just a decoder that you put in an embedding, and then after a series of Wired outcomes text, so you can see there's lots of different LMS there's a lot of different differences and as such, they're all they all excel at different tasks.

2:11
So there's constantly going to be the different need for encoder and decoder type of models, just depending on the application. Maybe talk a little bit about what are some of those applications when you use the encoder versus decoder type of models? Sure.

2:26
Taking overt zero San Carlo competition, where it was used. One of the competitions go is to evaluate students summary the other competitions go is to evaluate complexity or the past ages so so both tasks require to evaluate and classifies and sentences. I think this deal or is a good example of a use case of birth because is very good at clarifying that. Right? But GPT is used for generating sentences. So, so it's like a chatbot. I think, for me, I'm using CBT for generating a simple goal when I say can you show me the example of the pytorch add on GPD returns an example. So I often hear people say they don't belong to any coding without CPT. So. So those, that DBT is very different.

3:46
Certainly, there's applications for both and the cool thing is it's not just limited to the LLM space where you can actually apply these other lens and other areas. My background is envision and I'm seeing some really cool stuff happening in Division space, as we're using language models. Or maybe you can talk a little bit about that. What are you seeing in terms of that? Yeah,

4:06
so definitely on the left, I've had a huge impact in vision and in particular in the way that we interact with our vision systems, right. So before MLMs make this you know, big slash, we were not even thinking about interacting with our vision systems using natural language right. So this was kind of made possible by by play which was the first or one of the first algorithms that said, Oh, how about we align the text modality with the image modality, right? So Chris explained before like how to obtain this embedding from text, and now the idea of clip would be to obtain an embedding from an image and kind of put these two together in the same embedding space is representing the same object, for example, you know, you have the text dog and you have an image of a dog. You want to put these two embeddings closer together. And well, how do you how do you train such a system right? So you need a bunch of images where there's corresponding captions, so a caption that actually explains the content of the image. And then you train the system to align the embeddings. So what is cool now is that you can go from one modality to the other and you can do really nice things. You can talk to your vision system using natural language, which has really, you know, allowed us to think bigger, how to how to apply our vision systems to much more for example, categories, you know, think beyond the cars and pedestrians that we're detecting that we're segmenting and really thinking big in terms of natural language. So I think the perspective has really changed with

5:49
this idea of bringing embeddings from different modalities into a common embedding space is just opens up so many possibilities and very powerful, what capabilities recently.

6:01
So first, we're interested in perception, as I said before, and L lens allowed us to do what we now call open world scene understanding for example. So you take the task of semantic segmentation right before what we used to do is grant you know a certain number of classes were interested in if you're in interested in autonomous vehicles who want to be tagged and segment, pedestrians, cars, road etc. So the way we speak set of classes, and when we're training our systems to segment you know, once you get an image segment and assign a label, but there will be strict set of labels, right. Nowadays with other lamps. The perspective has changed, right? I mean, before it was okay, how do I scale up such a system to you know, the infinite number of objects that we can find in the world? It was not a clear path forward. And now we're going to learn to actually see a path forward, right? I mean, the idea is that you actually use prompts you use your natural language, to express what you want to find in the image. And the vision system needs to segment anything that you probably write fire hydrants, dogs, house, whatever, not only a set of predefined classes. So I think this was a way of doing actually this open world. Semantic segmentation or scene understanding so completely different game to what we're doing. before. And then of course, like LLS have also changed the way we do for example, generative AI, right. So we have now things like that may or may journey that leverage this alignment capabilities of K that I was talking about before. And so for example, you have Dalyan that takes now these text embedding and usually using a diffusion model, it generates an image that represents what you describe in the text right? So here probably seen these demos where you kind of right, you know, there's a polar bear in on a skateboard in Times Square and get this nicely generated image of exactly what you have described. So I think this opened up endless possibilities for designers, for artists and in general, for the public to interact with this vision systems because now everything is through natural language. So I think this opens up tons of possibilities.

8:26
Yeah, for those of us working on competitions we're always looking for you know, what is that next thing and you know, what is the next edge of some of those those things capabilities you're talking about are really exciting. What do you think smash like what what are the next frontiers that we're talking about here with vision and LLS

8:43
we finally start exploring text and image right so but there's tons of other modalities without going too much further away. We have been used with see now things like Sora, for example, that is generating videos from text, but there's still tons to explore. Right? So there's the question of you know, how temporary for hearing those videos are, or, for example, the captions that are used to train these models because it's the same idea. As with fade right you want to align a video with the caption explaining the content of the video, but the question is, if this caption actually explains only what is in the video in terms of objects, or also describes motion describes actions. So I think there's a whole new, like research field for exploring what kind of captions we use to train the systems and how temporarily procure in our videos are going to be there. So our workers going to appear now in this area thing. And then there's also the whole field work. I mean, we have other centers we have, for example, LiDAR, and we also want to align geometric features with language with images, right. So I think there's a chance to explore in different modalities. We have been working, for example, on Lidar and find two objects in the LIDAR space using geometrical features using shape features. So I think it's going to be super exciting because now we're going to be able to generate, for example, full objects in 3d using text prompts. So there's tons and tons that is going to appear I think, in the upcoming news.

10:21
Yeah, yeah, really exciting stuff. You know, starting to bring it into the competition space a little bit. Yeah, it wasn't that long ago were the things that you have Wow, those little bit were things like retriever. So you can just retrieve images or retrieve tax and get commonalities. But now generally, I that's that's become we've been able to move far beyond that. And actually, we can combine the two concepts and so there's a thing now called rag. Everybody's talking about rabbits. Chris, why don't you think this site ran a little bit telephone? What is rag and how's it used? Okay,

10:55
so rag is a really cool technique that extends the capabilities at all and it stands for retrieval on degeneration. So if you ask a basic chatbot a question, then it's going to answer that question from its memory, what what it already knows. When you use rag, you have an LLM and a set of documents. So then you ask a question and the first step is we search all the documents for chunks of text that relate to the question, and then we give both the question and all those helping tons of text to the LLM looks at it all and then it gives an answer. And this happens all without without even knowing it. But as such, the answer comes back and it's so much more accurate. And I had a chance to experience this in a recent cattle competition, called the LM science exam. And we were challenged to build a system that can answer multiple choice science exam questions. And we were limited because limited how big the language model can be. And there was also time constraints and resource constraints. So as such, we couldn't submit a model say, as big as chat GBT, which may already have a lot of the knowledge and its memory, but we had to sort of submit smaller models. So the solutions that won this competition we're wrapping, and specifically, people were submitting models, and at the same time, they'd submit a set of documents specifically, they submit all of all 6 million Wikipedia articles. They submit it all together. And then what their code would do is when it was when it was about to answer a science exam question doing first scan all 6 million articles in the blink of an eye and find any texts that relate to the question, and then it would feed that help that helpful information plus the question to the LM and then it would get back an answer. I witnessed this firsthand because on my computer, I would just make I just made challenging questions, right. I would make a question on quantum physics about specific detail or a number and think no way would it find it? But sure enough, you know, in the blink of an eye, it would come back with the answer and it was something like I forget 97 98% Correct. So it's truly incredible what these practices can do. I know what's impressive thing is, all of this is happening behind the scenes, right? You're just asking a question, and the answers are coming back. And it's doing the retrieval and all that kind of stuff. And it's just all in the blink of an eye really amazing. Yeah,

13:26
yeah. For those of you who might be interested in finding out more about that or receive this this in action correspond with some really great notebooks out for some of the highest and highest voted ones in Tableau. A few months back during during this competition, so you can go and check those out and see how we train Wragge. Have we been friends with Greg but really, really good stuff Kazuki you know, so Chris talked about a couple of things there. He talked about retrieval time we talked about LLM student Federation. unbalanced. is one more important than the other, like, getting the trade off between retrieval and get alone. Yeah.

14:03
Let me talk about this topic per topic for us, Ryan and iTune. There are some papers just comparing about it. So the boast almost all papers whose route is better as Antun he calls because the fine tuning is very difficult method to apply due to the catastrophic forgetting. That means when you want to train new things, subscribe, right so news of us so you can do that. But that model often forget all the things. So more than that, Rob is very cost effective. Comparing two by two because fine tuning is a lot of computing resource. So yeah. So but I think it's worse to try to fine tune when you want specialized understanding. Also, I think, yeah, I think we should find a safe spot between saving money and making requirements.

15:32
So basically, Brad is something that can make elevens even better than the old itself and based on what you're saying it would be cheaper as well when you're not having to find good models and get additional data, and it'll be more efficient. So So that's, that's obviously very powerful. But if you know somebody, of course we're interested in as the applications of that so so anyway, what what are you seeing in terms of different applications? for that?

15:53
Yeah. So I think there are two kinds of interesting applications. So the first is to protect privacy. So we all have a lot of private data, either personal or enterprise. So which we don't want to share online. And what we can do is to bring our land to a local controlled environments like we deploy the open source and recreate our vector database like an embedding model. And yeah, specifically like rack system, connecting our local arbitrary data to this local defined. So this is the top to your data experience. So we leverage the capability of allowing while protecting the privacy of the data. We actually have two demos you can interact with. On the second floor, the demo booth we have the chat with RTX. So basically, it's a new look just to deploy on the Windows laptop. So you could it could talk to some PDF file some other kinds of files using large language models. Another demo is talk to your data with Nemo agent. So whenever you have a question, so if there's an agent which can route the question, to a unstructured text agent, or to a structured support, trigger, and sensitize the answer and get back to you. So I think basically, these are privacy protecting kind of demos. Second kind of applications I think is to enhance like the recency of the of the use cases. For example, like a news or a finance agent or the search on Power Search and also co pilots. So reaching out in process, like real time streaming data, and, you know, and help us accomplish accomplish tasks like replying an email helping me writing a short summary of the conference or, you know, writing code. So yeah, I think those are the interesting applications. Yeah,

18:00
yeah. Yeah, the applications are just limitless. So you know, we've talked about applications or fingers, LLM 's and grads and this embedding space between vision and LLM and some hot areas. You know, interested I know we all are, well, how can you take these things and actually apply them and the competition space? So you know, as such, with these technologies, it seems like competitions are starting to change a little bit. Like for example, you know, we're seeing LLM competitions where, you know, there's no data provided or one data point you got to generate your own data. We're starting to see see changes that you have, what other changes are you seeing in the competition space?

18:36
Yeah, so just like you mentioned, so I think a very interesting trend in the power competitions is there are more and more competitions, which don't provide any training data at all or for like very little training data, which is not enough to train a powerful like predictive model. So what the challenge here is it asked all the participants to come up with novel ideas and solutions to collect your own data, curate your own train data. This is actually a very critical step for any machine. learning tasks. But, you know, previously, on Pavel at least, so the training data is fixed. So and there's, it's very hard or impossible to expand the training data. But now, we're seeing more and more use cases where you know, participants leverage LM to generate training data and the which creates actually a great competition or advantage to win a competition. So yeah, so actually, this is also very cost effective. So comparing comparing to like handling so I expect two more such competitions. And I think this bill is actually quite useful for other tasks outside competitions. Yeah,

19:54
yeah, I totally agree. Yeah, another area where we're seeing the application and some of these things that we weren't seeing before the common space is maybe REXIS. And Chris, I know you've done a lot of work in rectus before you had a chance to use Hello, hello, Francis.

20:10
Yeah, we have so as Ellen's are being developed, we're actually seeing them improve all arrays of AI and Laura had spoke how it's helping with vision. But another example is recommender systems. Right? So recommender systems are you on one online shopping site, and they'll suggest something you might like or a streaming video website and it suggests movies. The way recommender systems work is there's users and items and attempts to recommend an item the user is going to like, and typical ways of solving this are you take a look at the items that a user previously engaged with. And then you could find items that are similar to those items. Or you could look at each user and find other users that are similar to that user and then see what items they like. Or lastly, we find patterns between users and the items they engage with. The way l ones help is if you remember, we had mentioned how a model like Bert can encode a block of text so items can be represented by their font, text description, and we can take that description and then we can encode it into an embedding. And that is like a point in space a little dot. And when you encode all of the items, you have all these dots and then we can find which items have similar by just finding which dots are the closest right, so that now gives us a new way to find similar items. Likewise, you can apply that to users. And lastly, by using these embeddings these dots, you can actually find patterns between users and embeddings. In this pattern could be either the items in this embedding space so using LM is really helping us make more accurate recommenders.

21:58
I think if you're able to use this recent KDD Cup competition, right, maybe you can tell us about that. Yeah,

22:04
so we did so. Recently, I teamed up with a bunch of co workers and we entered the prestigious annual KDD Cup, which that was in 2023. And the task was hosted by Amazon and the task was to build three recommender systems so when you visit so that you can visit the Amazon website in different countries and they can do what the websites in different languages and the tasks where we had to build a recommender system for languages where we have lots of data that we had to build recommender system for underrepresented languages, not a lot of data. And then lastly, we had to build a recommender system which recommend products that do not exist yet. So yeah, interesting chapter. So our so our solution used large language models, and specifically we use embeddings to find similar items. And then furthermore, embeddings allowed us to do something else, which is when we found patterns in the languages which had lots of data via transfer learning or translation because we're working in an embedding language space when we're able to transfer those patterns to apply them to the recommender system for the underrepresented languages. And that gave us a huge edge there. And then in the third task, and you kind of use model wiper, which is the encoder for that. And then for the third task, where we have to generate generate, generate potential items that don't even exist yet. We use models like GPT, which would you know, basically start with an embedding of items that users like, and then it'll generate text descriptions of products that don't even exist. So using so using language models allowed us and then we also combine that with with classical techniques, and that allowed us to make very accurate models. We actually the NVIDIA team actually won first place and it was like three main competitions that we won first place and every single competition. I was like, get ready to clap we.

24:17
are super excited about that. It was a great demonstration of just the power of Allen's helping out with other forms of AI.

24:25
Yes, that's a great example of how you know some of these new technologies are coming in and can be applied not only in the real world, like some of the applications we talked about, but also the competition so clearly seeing changes in that space. So so this will be where is this how to where to where what do you see as in the future of competitions? How might they look different in the future?

24:44
Yeah, I see the camera that I have the who would be more comfortable to for humanoid fighters. They can speed up their application process by taking over augmentation. Suggesting reverse. In other words, they can move focusing. They can focus on more essential tasks, which is exactly what the organizer looking for. So yeah, I think I think more than that, the mastermind models will be more accurate and robust using that high quality data. Also, I think it makes CV and LB more reliable.

25:43
Yeah, yeah. Which goes back to what you were just talking about about the problem of data and now we need to do more data annotation and with the generation so certainly not that should be a change that we should be looking out for. So great. Well, so the term a lot of topics today. You know, some of them are the latest technologies, how we're using them, how we apply them, how they can be used in competitions, but we'd love to hear from you any questions that you have for us about any of these topics or anything beyond. We'd be happy to take questions

26:17
is when people first of all, thank you for the awesome panel. The question was about the future of machinery learning competitions in the past. If you participated in machine learning competitions, there was a chance you would contribute to the state of the art research, Aleks net would be a perfect example. And to do that, the barrier to entry was pretty low. You just need the computer with the GPU and the best basically had to be smart. That's it. Now cartilage research state of the art research required to train large models, which is at least a few million dollars and the cluster of computers and not everyone in this room has access to that kind of resources. So do you think that in the future, the machine learning competitions will still provide the venue for discovering cartilage breakthroughs and state of the art developments or it will become kind of marginalized and mostly the venue for recruitment and kind of place for people to enjoy? No one is?

27:25
Sure, yeah, they also are with them and maybe somebody else that wants to contribute so so there's a self regulating factor involved there, which is the amount of compute that you have for inference, okay. So you can go off and train these advanced models, but the way competitions are working today is they're mostly code competitions. So you have to commit your code to an inference server that has a limited compute envelope. So what we're seeing is a lot of underneath and variations on how you can compress these models that you can quantize them how you can get the drop because limited envelope. And I think that's the factor that normalizes the playing field a little bit and not thinking just about who's got the most compute because it was about that and you just had to submit a static CSV file with your with your solution that I think the premise of your question it would be exactly right because that this will just go to who had the most impeach away. But that's not the case. And we're seeing some actually, I think, some really innovative things even beyond necessarily the scope or the intent of the actual competition that's going into this efficiency problem, because everybody's trying to take advantage of the latest and greatest and state of the art but how you can compress that into a limited can do envelope that everybody has access to that becomes almost a challenge in and of itself.

28:39
So I think, I think even now so often, machine learning competitions can still contribute to the state of the art research. I think two examples are first a mixture of experts. So if you if you take a look at the honeymoon phase open the award. Many of the top entries are actually created by mixing several language models. So in an innovative way, so it's actually pretty, it's not that high compute intensive as one assumes. So it can be done on the on the laptop or even on a single GPU is possible. And, you know, it's like an ensemble of a second sample is so there are approaches like the QR answer the contest, low rank adapters, so you know, it's also like you're just training a very small adapter. So even though they are have billions of parameters, the adapters actually have it's just megabytes in terms of bytes. So in some cases it can really enhance the capability of an LM so in a low cost. Thank you. We have the next question.

29:51
Great talk. Slider questions about third party competition. He has a mic up if I can take him down and he was going to use us to make recommendations on new products i i didn't really understand the jump between the menus and the recommendations that you expect.

Transcribed by https://otter.ai