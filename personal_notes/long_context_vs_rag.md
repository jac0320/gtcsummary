# Long-Context vs. RAG

Everyone is doing RAG. Literally, everyone. RAG is a model that is designed to retrieve the section information from a long and dedicated context, and then use it to generate the answer. Think of it as an open-book exam. You have a book that contains all the information you need to answer the question. You read the book, then you answer the question. 
                    
In my opinion, the key to a successful RAG model is the quality of the context. And for a business model, it is about the proprietary data that each company has - the documentation. Documentation quality and quantitive is a necessary condition for a successful RAG. Sufficiently, you will also need a good generative algorithm to generate the answer, which incorporates context retrieval tuning, structured LLM prompt engineering, and strong validation schemas. There is no shortage of papers on how to do this since the framework is cost-effective and easy to implement. In the technical talks, I have seen quite a few companies trying to apply RAG internally. Companies like LlamaIndex, Perplexity, and Glen all build part of their business pillars based on a RAG framework. I don't doubt the potential of RAG, but is RAG the only way to go?

No. Long-context model stands as another viable option. Long context is a model that is designed to take a long context and generate the answer (Google has the >1M token model). Think of it as a closed-book exam. You have a book that contains all the information you need to answer the question. You "memorize" the book, and then you answer the question. As these models tend to be a lot harder and more to train, the key to a long-context model is your machine learning engineers + enough capital. It can outperform RAG on certain tasks like handling complex queries and constructing detailed narratives.
        
Is it an either-or situation? Not necessarily. Some studies found RAG combined with a 32k-token LLM can outperform providing full context directly to the LLM. In my chat with few friends, all agreed that RAG is not going away any time soon. The understanding of both RAG and the long-context model is still early. The best track is to keep trying and learning.
        
[Written by Site ðŸ¤“]