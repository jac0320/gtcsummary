{"docstore/metadata": {"f2d9f122-955d-4f44-a3e0-0cab5ce873d7": {"doc_hash": "8e7eb1abc6d49c42ddf8c41eea305a0e521be8c7a6edceefc21978e69cdda6e5"}, "b6529b78-4cf3-40cf-a2a0-014f4c728d36": {"doc_hash": "dce826e59d86124d5942cb377980b8e148e7662fc937bd4073b3b76b407a266f"}, "c74ae174-88e0-4d82-ac13-3b651f59a9b9": {"doc_hash": "b476630c874f03d6e12059c875b341f19fe29fb9bb874f3685c8695caebacfc8"}, "54a1ce89-03a3-4d25-a880-61d655249636": {"doc_hash": "0d7ecfbbbf1fa39bbfef2140d7f2c387c18f1e7d8d98f190b4e2315c59380f6e"}, "aaa85b62-7ea2-454e-8bc1-e02fa54da3be": {"doc_hash": "7bc113f4bc242e52c0d2b318ded6a215d6b2fd7bb919abe81f8550d67ad7d74e"}, "d8c74385-0b45-4c5f-8f3e-b1ee60ad0abf": {"doc_hash": "dbbe736c0acac5f272c6dba71577305de971f15be50b76539480ca721803e89a"}, "881cc7df-79a3-4f3f-87b4-a510705da3bb": {"doc_hash": "199869dbba14fcb26ccecea936ed30699085b0a9b9752f3a486b35ce1fd661af"}, "b79e63f1-1dba-49b1-be20-1c194b7900a1": {"doc_hash": "a4090f266e442502afab676c8db2e036a93c7087b89acad245bd6873a1909f7c", "ref_doc_id": "f2d9f122-955d-4f44-a3e0-0cab5ce873d7"}, "a3228cc8-e87f-413a-8e94-2ce311e2cabc": {"doc_hash": "1eee4419a09e0233cf6a5df7b6d2b0cce36683c9c8a0b06bbd7964b09a5f22f2", "ref_doc_id": "b6529b78-4cf3-40cf-a2a0-014f4c728d36"}, "828fff43-c592-4df7-b9f5-a8adb7b635fc": {"doc_hash": "af5243158169c19d8b9f931d22abb48ad03d9f76bb19d592669ae948017f59f4", "ref_doc_id": "c74ae174-88e0-4d82-ac13-3b651f59a9b9"}, "78402ff2-c57f-40aa-a672-876806883cc8": {"doc_hash": "e89b8b68750d7d2fbf5a118e724e9178ebbde5f4ebc7aa5852e43ebb44e84128", "ref_doc_id": "54a1ce89-03a3-4d25-a880-61d655249636"}, "60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe": {"doc_hash": "b828ac385bed8e77763cf4db915a9b51b986dc31cddefc1acd94964939b34b9c", "ref_doc_id": "aaa85b62-7ea2-454e-8bc1-e02fa54da3be"}, "6c89ddb2-9f57-4803-bea7-41c1becb94c1": {"doc_hash": "78f7c8c33c3a747dd90fbb0eed9f54840942387d25f7357b233bb6f36f3725a6", "ref_doc_id": "d8c74385-0b45-4c5f-8f3e-b1ee60ad0abf"}, "39295a5a-0bdf-437d-ae73-560299fe35e5": {"doc_hash": "bcfd2f460f59f0b31449b6981a3d94b3d5077333e082daebd750a8756bdcc5f1", "ref_doc_id": "881cc7df-79a3-4f3f-87b4-a510705da3bb"}}, "docstore/data": {"b79e63f1-1dba-49b1-be20-1c194b7900a1": {"__data__": {"id_": "b79e63f1-1dba-49b1-be20-1c194b7900a1", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/democratizing_ai.md", "file_name": "democratizing_ai.md", "file_size": 2050, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2d9f122-955d-4f44-a3e0-0cab5ce873d7", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/democratizing_ai.md", "file_name": "democratizing_ai.md", "file_size": 2050, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "8e7eb1abc6d49c42ddf8c41eea305a0e521be8c7a6edceefc21978e69cdda6e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3228cc8-e87f-413a-8e94-2ce311e2cabc", "node_type": "1", "metadata": {}, "hash": "494579dd729dfecabc320655662cbcfef8284f2f0fce9f8bb94308e25f0191de", "class_name": "RelatedNodeInfo"}}, "text": "Democratizing AI\n\nIronically, the first person to mention the term \"democratizing AI\" was OpenAI's COO Brad Lightcap. He emphasized the importance of enabling a wide range of users to leverage AI technologies for diverse applications. To me, that almost sounds like \"We should scoop more users from Google into our closed environment\". On the other hand, Mistral, which company open-sources a high-quality smaller talked about how to give AI to users' hands without mentioning \"Democratizing AI\" once for 45 minutes.\n\nJokes aside, my takeaways on Democratizing AI is about two things:\n1. Enable more people to use and benefit from AI in a non-harms way\n2. Don't have AI controlled by a few powerful entities\nAt least, I am convinced that this will be my mission in helping people build AI products.\n\nOften the time, I ran into such discussion on how can AI solve this question. And I'd always answer, it may not. But, let's think it through. What is the rule of thumb that you may consider AI:\n1. **Does your problem have some sort of pattern?** For example, is your task somehow repetitive but not the same every time you do it? Like reply an email, classifying a document, or categorizing a file into the Box folder.\n2. **Does your decision require some logic that cannot be explicitly written?** For example, the email you are reading does not always follow the same pattern, but you can tell if it is spam or not. Or, the document you are reading does not always have the same structure, but you can tell if it is a legal document or not.\n\nif you answer yes/maybe to both questions, then you may consider using AI. And if you don't know how to use it, I can help people learn about what AI can do and how to leverage it based on my knowledge and experience. I won't be able to solve all the problems, but I can help you think through the problem and see if you can build an AI solution. Maybe my future wish is to give people the platform to build AI without knowing how to code. But, that is a long way to go. Or is it?\n\n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3228cc8-e87f-413a-8e94-2ce311e2cabc": {"__data__": {"id_": "a3228cc8-e87f-413a-8e94-2ce311e2cabc", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/long_context_vs_rag.md", "file_name": "long_context_vs_rag.md", "file_size": 2251, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6529b78-4cf3-40cf-a2a0-014f4c728d36", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/long_context_vs_rag.md", "file_name": "long_context_vs_rag.md", "file_size": 2251, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "hash": "dce826e59d86124d5942cb377980b8e148e7662fc937bd4073b3b76b407a266f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b79e63f1-1dba-49b1-be20-1c194b7900a1", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/democratizing_ai.md", "file_name": "democratizing_ai.md", "file_size": 2050, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "a4090f266e442502afab676c8db2e036a93c7087b89acad245bd6873a1909f7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "828fff43-c592-4df7-b9f5-a8adb7b635fc", "node_type": "1", "metadata": {}, "hash": "ab8dd8ed603b9c9520204145e6fc186a636123618b64b2eb2aa0d9c7381ad9b6", "class_name": "RelatedNodeInfo"}}, "text": "Long-Context vs. RAG\n\nEveryone is doing RAG. Literally, everyone. RAG is a model that is designed to retrieve the section information from a long and dedicated context, and then use it to generate the answer. Think of it as an open-book exam. You have a book that contains all the information you need to answer the question. You read the book, then you answer the question. \n                    \nIn my opinion, the key to a successful RAG model is the quality of the context. And for a business model, it is about the proprietary data that each company has - the documentation. Documentation quality and quantitive is a necessary condition for a successful RAG. Sufficiently, you will also need a good generative algorithm to generate the answer, which incorporates context retrieval tuning, structured LLM prompt engineering, and strong validation schemas. There is no shortage of papers on how to do this since the framework is cost-effective and easy to implement. In the technical talks, I have seen quite a few companies trying to apply RAG internally. Companies like LlamaIndex, Perplexity, and Glen all build part of their business pillars based on a RAG framework. I don't doubt the potential of RAG, but is RAG the only way to go?\n\nNo. Long-context model stands as another viable option. Long context is a model that is designed to take a long context and generate the answer (Google has the >1M token model). Think of it as a closed-book exam. You have a book that contains all the information you need to answer the question. You \"memorize\" the book, and then you answer the question. As these models tend to be a lot harder and more to train, the key to a long-context model is your machine learning engineers + enough capital. It can outperform RAG on certain tasks like handling complex queries and constructing detailed narratives.\n        \nIs it an either-or situation? Not necessarily. Some studies found RAG combined with a 32k-token LLM can outperform providing full context directly to the LLM. In my chat with few friends, all agreed that RAG is not going away any time soon. The understanding of both RAG and the long-context model is still early. The best track is to keep trying and learning.\n        \n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "828fff43-c592-4df7-b9f5-a8adb7b635fc": {"__data__": {"id_": "828fff43-c592-4df7-b9f5-a8adb7b635fc", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/retrieval_vs_generation.md", "file_name": "retrieval_vs_generation.md", "file_size": 2041, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c74ae174-88e0-4d82-ac13-3b651f59a9b9", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/retrieval_vs_generation.md", "file_name": "retrieval_vs_generation.md", "file_size": 2041, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "hash": "b476630c874f03d6e12059c875b341f19fe29fb9bb874f3685c8695caebacfc8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3228cc8-e87f-413a-8e94-2ce311e2cabc", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/long_context_vs_rag.md", "file_name": "long_context_vs_rag.md", "file_size": 2251, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "hash": "1eee4419a09e0233cf6a5df7b6d2b0cce36683c9c8a0b06bbd7964b09a5f22f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78402ff2-c57f-40aa-a672-876806883cc8", "node_type": "1", "metadata": {}, "hash": "949bc0a0e235f38b4173aeca16468af379d04e9b3d7ba6ded2e8fc95c015d065", "class_name": "RelatedNodeInfo"}}, "text": "Retrieval vs. Generation\n\nThis was mentioned in the keynote by Jensen Huang. He said\n                    \n> The way that computing was done is Retrieval, you would grab your phone and touch something. And some signals go off basically\nto some source somewhere that is PRE-RECORDED content somebody wrote, like a story, image, or video. Then Pre-recorded content is streamed back to the phone and recomposed in a way based on a recommender system to present the information to you. The future of the vast majority of the content will not be retrieved and the reason is those pre-recorded content was made by somebody \nwho doesn't understand the context which is the reason why we retrieve so much content. If you can work with an AI that understands the context, like who you are, for what reason you're fetching this information, and produce the information for you just the way you like it. The amount of energy we save, the amount of resource we save, the amount of wasted time we save will be tremendous.\"\n        \nTake the following example when I want to know which fasteners (diameter and length) to use for an outdoor deck with 2x6 redwood:\n\n1. My first try was to look it up on Google, but I ended up on several DIY websites. The overwhelming number of ads made it tough to find the information I needed.\n2. I tried watching a tutorial on YouTube. It was a 10-minute video. Nah, who knows if it has my answer or not? I am too impatient.\n3. Then I hopped on perplexity.ai and asked the same questions, and I got the AI-generated answer in less than 5 seconds (with references).\n\n!Test image\n\nWhat struck me was how the AI seemed to intuitively understand what I was looking for amidst a sea of diverse information. While I still value the depth and quality of pre-recorded tutorials and guides, I'm increasingly convinced of the potential of generative AI. It's about connecting people with the right information in the right context - and that's where I put my money at.\n\n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78402ff2-c57f-40aa-a672-876806883cc8": {"__data__": {"id_": "78402ff2-c57f-40aa-a672-876806883cc8", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_economics_of_ai.md", "file_name": "the_economics_of_ai.md", "file_size": 2849, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54a1ce89-03a3-4d25-a880-61d655249636", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_economics_of_ai.md", "file_name": "the_economics_of_ai.md", "file_size": 2849, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "0d7ecfbbbf1fa39bbfef2140d7f2c387c18f1e7d8d98f190b4e2315c59380f6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "828fff43-c592-4df7-b9f5-a8adb7b635fc", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/retrieval_vs_generation.md", "file_name": "retrieval_vs_generation.md", "file_size": 2041, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "hash": "af5243158169c19d8b9f931d22abb48ad03d9f76bb19d592669ae948017f59f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe", "node_type": "1", "metadata": {}, "hash": "cedf240fdbff0abb5b3fb2fe04056e59b8727545ec82c2cdeb5a22aecc818902", "class_name": "RelatedNodeInfo"}}, "text": "The Economics of AI\n\nI was chatting with bria.ai, which is a visual generative AI platform for creators, and we got into this discussion about how much to train a model. They have a multimodal model that is claimed to be trained from the ground up. For that, it took 280 A100 for 2 weeks to train the model. And if you go to lambdalabs.com to rent an A100, it will cost you \\$2.5 per hr. So, the total cost to train the model once is \\$2.5 per hr x 280 x 24 x 14 = \\$235k\n            \nSo, let's do some math. If you've got 10k users forking over $10 each month, you'd break even on just the training cost in a smidge over two months. That's not even touching the iceberg of data costs, salaries, marketing... you get the point.\n        \nSo what'd a start-up do?:\n1. Hack up the price? Sure, if you want to put the breaks on growth and enjoy the melodious screams of your VCs.\n2. Figure out more growth as MidJourney riding the AI hype with their 1.5M-2.6M active users. Tempting, with all that AI mania money floating around.\n3. Or maybe, just maybe, use some of that sweet investor cash to bring in the brainiacs who can fine-tune the model, cutting costs without skimping on quality. Sounds smart, right?\n\nI could be naive but I'd bet option 2 is the easiest way to go now with the all AI hype that floods \\$\\$\\$ into the AI startups. But eventually, option 3 is the option that leads you into the final round of the sustainable business game.\n        \nAnd let's not forget the AI value debate. Sure, for complex, creative tasks, AI's a no-brainer, outperforming traditional code by achieving something that regular logic just couldn't handle at scale. But for the simple stuff? It's a harder sell. Why go AI for a task as simple as 1+1 (using \\$\\$ inference) when a basic function would do the trick unless you're keen on splurging on software engineering talent?\n        \nYet, there's a twist. Even for these 'simple' tasks, when you bundle them up and look at the big picture, AI starts making a whole lot of sense. What matters is the value of integrated automation. Your SWEs built a good abstraction and architecture for a platform to be scalable. AI can help augment/maintain the system with less cost and less time from people. And this is where the value of AI comes in for even simpler tasks. Independently, simpler tasks are not worth the cost of AI, but when you integrate them, the table can be turned. \n\nSo, it's not just about the hefty upfront cost of training an AI model. It's about weighing the ongoing costs (infrastructure, maintenance, talent, etc.) against the value AI brings in terms of customizable, integrated automation. Don't fire all your SWEs. Have AI augment their lives and support them in maintaining the system to free up their tedious on-call live at least.\n                            \n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe": {"__data__": {"id_": "60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_need_for_scalable_inference.md", "file_name": "the_need_for_scalable_inference.md", "file_size": 2185, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaa85b62-7ea2-454e-8bc1-e02fa54da3be", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_need_for_scalable_inference.md", "file_name": "the_need_for_scalable_inference.md", "file_size": 2185, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "7bc113f4bc242e52c0d2b318ded6a215d6b2fd7bb919abe81f8550d67ad7d74e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78402ff2-c57f-40aa-a672-876806883cc8", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_economics_of_ai.md", "file_name": "the_economics_of_ai.md", "file_size": 2849, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "e89b8b68750d7d2fbf5a118e724e9178ebbde5f4ebc7aa5852e43ebb44e84128", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c89ddb2-9f57-4803-bea7-41c1becb94c1", "node_type": "1", "metadata": {}, "hash": "4a635d995385c23f26edce81a9d033aaa1d995a1c75c21f754b311640943c54c", "class_name": "RelatedNodeInfo"}}, "text": "The Need for Scalable Inference\n\nNvidia is really up their game with their FP4/FP6/FP8 performance with Blackwell GPU. feeling a bit puzzled by all these \"FPs\"? Don't worry; it's simpler than it sounds.\n            \nThink of FP-X as a sort of 'size' for the data points the GPU handles - kind of like choosing the right-sized container for your leftovers. The \"X\" in FP-X tells you how big that container is. A bigger number means you're getting more detail (precision) but at the cost of needing more space and time (memory bandwidth and slower processing). On the flip side, a smaller number means less detail, but you save on space and speed things up.\n                \nNow, it is important to note that we are talking about inference here, not training. Inference is the process of using a trained model to make predictions on new data. And hence, the majority of the applications, are driven by a bunch of inference. Hence, the speed of inference is crucial for real-time applications. For example:\n        \n* If you want to deploy a large model on a robot for it to complete a series of tasks based on a high-level commander, you want the inference to be as fast as possible. \n* The self-driving vehicle needs to make real-time decisions based on the input from the sensors. And you don't want your model still running after running over a pedestrian. The faster the inference, the safer this vehicle maneuvers. \n* Phone support agent system using LLM: you want your support agent model to have fast inference to respond to the customer on the phone in a very natural and human-like way.\n        \nBut here's the catch: speed isn't everything. As AI gets more advanced and takes on more tasks, it's going to gobble up more power. Imagine having a top-notch AI support agent that answers customers flawlessly but drains so much power that the costs go through the roof - not exactly ideal for business.\n        \nHence, the crucial demand for scalable inference is to have a:\n* Fast Computation (driven by cloud/local architecture, algorithm, and hardware)\n* Cheaper Operations (energy, infra, and maintenance)\n\nSo who do you think runs a better game here?\n\n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c89ddb2-9f57-4803-bea7-41c1becb94c1": {"__data__": {"id_": "6c89ddb2-9f57-4803-bea7-41c1becb94c1", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_world_of_agents.md", "file_name": "the_world_of_agents.md", "file_size": 2526, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8c74385-0b45-4c5f-8f3e-b1ee60ad0abf", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_world_of_agents.md", "file_name": "the_world_of_agents.md", "file_size": 2526, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "dbbe736c0acac5f272c6dba71577305de971f15be50b76539480ca721803e89a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_need_for_scalable_inference.md", "file_name": "the_need_for_scalable_inference.md", "file_size": 2185, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "b828ac385bed8e77763cf4db915a9b51b986dc31cddefc1acd94964939b34b9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39295a5a-0bdf-437d-ae73-560299fe35e5", "node_type": "1", "metadata": {}, "hash": "8d70a05c19692e72dc59e8d8b0a3a469165c356fbb5847a3fc175750efd916f4", "class_name": "RelatedNodeInfo"}}, "text": "The World of Agents\n\nTwo days before I wrote this sentence down, this paper called More Agents Is All You Need came out. Besides these, there are about 7 blogs in my reading list talking about Agents. There are also people using AI agents build Operating Systems. And I just finished talking to a start-up that leverages agents to automate website browsing flows. Last weekend, I boarded my wife to death while discussing how I am thinking about a critic agent that leverages bigger models to tame lower-level agents to prompt task success rate. Hopefully, you get the idea at this point.\n                    \nBut, interestingly, when you hop on perplexity and ask what is an AI agent it will tell you:\n> Based on the search results, there does not appear to be a clear, defined \"AI agent framework\" being discussed. The sources cover various topics related to AI, intelligent agents, and their potential applications, but do not describe a specific framework or approach.\n\nMy take on an AI agent is an inference unit that can perform one or a series of tasks subject to a pre-defined goal. Keeping it at a high level, the AI agent is software that can do things for you. It can be as simple as a chatbot that can answer your questions, or as complex as a robot that can do your laundry. The key is to define the **goal** and **decompose** that goal into logical steps (note that the logical steps are not fixed, as there can be many ways to complete the goal. You can think of an agent designed solely to generate a series of tasks. And this other agent is designed to attribute these tasks to dedicated agents. Then some agents are critical of the workflow and may ask certain parts to be re-done and observe an alternative. And you have other agents helping you summarize and validate the outcome. As the tasks differ, the models/prompts/logic designed for each agent can be different. Here, there is your army of minions, please be the best villain and go dominate the world. \n        \nAt this point, you don't want all of your agents to sit on top of an OpenAI API as it can be \\$\\$\\$ when you turn on the machine. It becomes fascinating to leverage a variety of different models, fine-tuned/pre-trained on specific tasks, to drive down your cost of operation. And to let this system run, you will also need scalable inference... wait a second... am I recalling my previous section now? It must be important.\n        \n[Written by Site \ud83e\udd13]", "start_char_idx": 2, "end_char_idx": 2444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39295a5a-0bdf-437d-ae73-560299fe35e5": {"__data__": {"id_": "39295a5a-0bdf-437d-ae73-560299fe35e5", "embedding": null, "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/what_the_heck_is_this_NIM.md", "file_name": "what_the_heck_is_this_NIM.md", "file_size": 1639, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "881cc7df-79a3-4f3f-87b4-a510705da3bb", "node_type": "4", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/what_the_heck_is_this_NIM.md", "file_name": "what_the_heck_is_this_NIM.md", "file_size": 1639, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}, "hash": "199869dbba14fcb26ccecea936ed30699085b0a9b9752f3a486b35ce1fd661af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c89ddb2-9f57-4803-bea7-41c1becb94c1", "node_type": "1", "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_world_of_agents.md", "file_name": "the_world_of_agents.md", "file_size": 2526, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}, "hash": "78f7c8c33c3a747dd90fbb0eed9f54840942387d25f7357b233bb6f36f3725a6", "class_name": "RelatedNodeInfo"}}, "text": "What the Heck Is This NIM?\n\nNIM is everything you'd want to enterprise AI. It optimizes a bunch of things for you. It is like a magic box that you can put your AI model in and it will run faster and cheaper.... as long as you buy the hardware from NVIDIA. \n\n!Test image\n\nFeel free to read below if you want to continue [Written by Site \ud83e\udd13 until here and summarized by AI \ud83e\udd16 below]:\n\nNIM is a powerful tool from NVIDIA that helps organizations accelerate their journey to production AI. It is designed to bridge the gap between the complex world of AI development and the operational needs of enterprises.\n        \nNIM provides optimized inference microservices that allow developers to access AI models through industry-standard APIs. This simplifies the development and deployment of AI applications, enabling rapid scaling within enterprises.\n        \nNIM packages domain-specific NVIDIA CUDA libraries and specialized code tailored to various domains like language, speech, video processing, healthcare, and more. This ensures the AI applications are accurate and relevant to their specific use cases.\n        \nNIM leverages optimized inference engines for each model and hardware setup, providing the best possible latency and throughput on accelerated infrastructure. This reduces the cost of running inference workloads as they scale. \n\nNIM is part of the NVIDIA AI Enterprise software platform, which provides enterprise-grade AI capabilities. It allows developers to experiment with NIM microservices and deploy production-grade NIM microservices on various NVIDIA-powered environments.", "start_char_idx": 2, "end_char_idx": 1594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"f2d9f122-955d-4f44-a3e0-0cab5ce873d7": {"node_ids": ["b79e63f1-1dba-49b1-be20-1c194b7900a1"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/democratizing_ai.md", "file_name": "democratizing_ai.md", "file_size": 2050, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}}, "b6529b78-4cf3-40cf-a2a0-014f4c728d36": {"node_ids": ["a3228cc8-e87f-413a-8e94-2ce311e2cabc"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/long_context_vs_rag.md", "file_name": "long_context_vs_rag.md", "file_size": 2251, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}}, "c74ae174-88e0-4d82-ac13-3b651f59a9b9": {"node_ids": ["828fff43-c592-4df7-b9f5-a8adb7b635fc"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/retrieval_vs_generation.md", "file_name": "retrieval_vs_generation.md", "file_size": 2041, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}}, "54a1ce89-03a3-4d25-a880-61d655249636": {"node_ids": ["78402ff2-c57f-40aa-a672-876806883cc8"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_economics_of_ai.md", "file_name": "the_economics_of_ai.md", "file_size": 2849, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}}, "aaa85b62-7ea2-454e-8bc1-e02fa54da3be": {"node_ids": ["60fd7b63-4d1a-48a2-8aa0-b2ffa24722fe"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_need_for_scalable_inference.md", "file_name": "the_need_for_scalable_inference.md", "file_size": 2185, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}}, "d8c74385-0b45-4c5f-8f3e-b1ee60ad0abf": {"node_ids": ["6c89ddb2-9f57-4803-bea7-41c1becb94c1"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/the_world_of_agents.md", "file_name": "the_world_of_agents.md", "file_size": 2526, "creation_date": "2024-04-14", "last_modified_date": "2024-04-14"}}, "881cc7df-79a3-4f3f-87b4-a510705da3bb": {"node_ids": ["39295a5a-0bdf-437d-ae73-560299fe35e5"], "metadata": {"file_path": "/Users/sitewang/gtc_summary/personal_notes/what_the_heck_is_this_NIM.md", "file_name": "what_the_heck_is_this_NIM.md", "file_size": 1639, "creation_date": "2024-04-17", "last_modified_date": "2024-04-17"}}}}