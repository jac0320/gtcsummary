### Model Development for Bias, Factuality, and Attribution - GTC 2024 Summary Outline
Vijay Karunamurthy, Field Chief Technology Officer, Scale AI

#### Introduction
- Emphasis on understanding model development to evaluate weaknesses, biases, and factual accuracy.
- Introduction of a hybrid approach combining automated model evaluations with human expertise.

#### Hybrid Evaluation Approach
- Example of collaboration with the Senate Homeland Security Committee.
- Combination of automated models and human experts for robust evaluation.
- Importance of domain expertise in evaluating sensitive scenarios like nuclear stockpile and chemical warfare.

#### Data Governance and Training
- Discussion on the governance of training data and its significance.
- Introduction of smaller, more capable models tuned for specific outcomes.
- The concept of alignment data and its impact on model capabilities.
- Stages of model training: pre-training, fine-tuning, and reinforcement learning with human feedback.

#### Scaling Human Alignment
- Importance of increasing human alignment data as models become more capable.
- Discussion on the necessity for diverse, bias-free, and comprehensive datasets.

#### Proprietary Data and Fine-Tuning
- The significance of fine-tuning models on proprietary data for enterprise applications.
- Challenges in ensuring models are grounded in the data used for training and retrieval.

#### Model Evaluation Challenges
- The inherent difficulties in evaluating large language models.
- The role of subjectivity, bias, and verbosity in model outputs.

#### Real-World Evaluation and Applications
- The need for real-world task translation into model evaluations.
- Examples of model applications in various sectors including defense and enterprise.
- Discussion on adversarial attacks and red teaming to test model robustness.

#### Responsible AI Deployment
- Framework for maximizing model performance, monitoring risks, and certifying responsible AI deployment.
- Case study on Air Canada's chatbot and the legal implications of model outputs.

#### Red Teaming and Adversarial Attacks
- Definition and importance of red teaming in identifying potential harms.
- Taxonomy of harms and techniques used to exploit model vulnerabilities.

#### Safety Evaluations and Unlearning Techniques
- Introduction of the SCALE Safety Evaluations Analysis Lab.
- Exploration of unlearning techniques to mitigate sensitive knowledge in models.

#### Community Engagement
- Announcement of the first women hackathon around AI safety hosted with GitHub.

#### Conclusion
- Open discussion on the impact of increased government regulations on AI development.