 This session is expected to be standing room only, and we need to fill every seat.
 So, if there is an empty seat between you and the person next to you,
 please make your way towards the center so all the available seats are towards the outside.
 Thank you again, and we'll be starting shortly.
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 Oh, there he is.
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 [SIDE CONVERSATION]
 Welcome to GTC.
 This session will begin shortly.
 Please take this opportunity to silence your cell phones.
 Thank you.
 [SIDE CONVERSATION]
 Good morning, everyone.
 Hi, I'm Wallace Mills.
 I'm a senior strategist for executive thought leadership
 here at Nvidia.
 Welcome.
 A few things before we get started.
 Just wanted you to know this session recording
 will be available online within 72 hours.
 And then within a month, it will be
 available on Nvidia on demand.
 Just wanted to remind you to download the Nvidia GTC app
 if you haven't already.
 And that's where you'll find the latest updates, session
 catalog, and surveys for your sessions.
 Also, we invite you to explore the exhibit hall here
 on level two.
 And that will be open today at 12.
 You don't want to miss that.
 That being said, I am particularly
 thrilled to introduce this session.
 I have had the pleasure of putting together the Business
 Insights track in partnership with many leaders
 across Nvidia.
 And so we get to kick it off with this conversation,
 this much anticipated conversation.
 And while many people are just beginning to or just
 starting to ride that wave of generative AI,
 and true Nvidia fashion, we are already
 here to talk about what's next.
 So if you would, please welcome our Vice President
 of Enterprise Computing, Manuvir Das.
 [APPLAUSE]
 Here at Nvidia, he leads the teams
 working to democratize AI by bringing
 full-stack accelerated computing to every enterprise customer.
 Manuvir has over 30 years of experience
 in the technology industry.
 And prior to joining us here at Nvidia in 2019,
 he held a range of senior roles at Dell and Microsoft,
 where he helped create the Azure Cloud computing platform
 and was an affiliate professor at the University
 of Washington.
 Welcome, Manuvir.
 Thank you.
 That is so embarrassing.
 This session is not about me at all.
 Thank you all for being here this morning.
 It's a real pleasure to see you.
 Hope you enjoyed the keynote yesterday, all the announcements
 from Jensen.
 One of the things he mentioned at the keynote
 was our first DGX system, the DGX-1.
 And he talked about delivering it personally
 to this startup called OpenAI.
 And what this group of people has
 done over the last few years is just absolutely incredible,
 right, isn't it?
 So we're very fortunate today to kick this off in a session
 where I'm joined by Brad Lycapp.
 He's the Chief Operating Officer of OpenAI.
 He's also a new Blue Devil.
 And I'm just giving him a hard time, because I'm a Badger.
 And it turns out the bracket just came out for the tournament.
 And the Badgers and the Blue Devils
 will probably meet up in the second round.
 So I'm not going to be a friend of his then, but I am today.
 So the interesting thing about Brad
 is he's obviously got a great role at OpenAI.
 He's also been called Sam Altman's secret weapon,
 the person he really relies upon.
 So I'm sure he'll have a lot of interesting things
 to share with us.
 So Brad, why don't you come on up?
 [APPLAUSE]
 Everybody gets to see us walk up here.
 It's going to be a bad time.
 Thanks for making it back.
 Get the fancy mugs of that.
 OK.
 So Brad, why don't you tell us a little bit
 about your role at OpenAI, what you do on a daily basis,
 and maybe a little bit about what keeps you up at night?
 Sure.
 Well, thank you for having me.
 It's great to be here.
 This is my first JTC.
 So we'll see if we're back next year and what that will bring.
 But yeah, so I am our COO at OpenAI.
 I spend a lot of time thinking about,
 how do we bring what we build in our research lab
 to life for customers and users and partners?
 And usually, people say, well, what does that entail?
 And I kind of say it entails everything.
 Other than actually doing the research,
 they don't let me touch the computers.
 I just pay for them.
 But I spend most of my time with our customers
 and trying to figure out how this technology is going
 to get integrated into the world.
 And what keeps me up at night?
 There's not much, I would say, right now that keeps me up
 at night other than Slack.
 But I think the next few years are really
 going to be quite interesting.
 I think we are still on the flat part of the curve.
 And the way we see it is this is like the edge
 of the first inning.
 And so as this technology gets built and developed
 and as we scale these systems up,
 we think the capabilities are going to be really amazing.
 Yeah, you know, what's interesting is I think a lot of people
 think of OpenAI as strategy, and think of it
 as the average consumer going and experiencing
 the technology.
 But of course, you've worked a lot with enterprise companies.
 And most customers that we talk to as a video,
 they build drags of one kind or another in their company
 by now.
 And how does everybody do that?
 They call it OpenAI to do that.
 So I think both myself and the audience
 are very curious to hear from you a little bit about what
 has that experience been like?
 And I think you're actually quite involved yourself
 in working with enterprise customers, right?
 So tell us a little bit about how that's all been going.
 Yeah.
 Well, it's funny.
 When we launched Hatch GPT, use obviously took off.
 And it was a product that wasn't launched for the enterprise.
 And we spent about six months just
 trying to figure out what the hell was going on
 and trying to make sure we had enough GPUs to accommodate
 our growth.
 But we spent really the last six months of last year
 actually starting to realize that there
 was a legitimate and growing set of applications
 in the enterprise that people were bringing Hatch GPT in for.
 And it's why we ended up launching Hatch GPT enterprise
 and ultimately our team product, which was a smaller team's
 product.
 But there was a real pull that we felt from not just SMBs
 in the market, but even the Fortune 500.
 We currently have over 90% of the Fortune 500
 using Hatch GPT in some form.
 We're trying to bring them all along on that capital E
 enterprise version of the product.
 But it has real pull.
 And it has real fit there.
 And the amazing thing about it is it's very horizontal.
 And so every function at the company has, to our knowledge,
 found some way to make the technology useful for them.
 And what's amazing is we didn't have to build a lot of really
 kind of vertical use cases or applications.
 It just kind of works.
 And so if you're on the finance team
 and you're analyzing a lot of data
 and you're trying to do reconciliations and all
 of your tax analysis, you can drop big spreadsheets
 in the Hatch GPT and just ask it questions
 and ask it to do the reconciliation.
 And it will kind of just do it.
 It'll turn your HR people into data scientists
 if they need to be.
 And so you've got applications like this
 that people just kind of don't fit with.
 And we're trying to go and build even better versions
 of the tools for them.
 It is amazing, because you're right.
 I think it's been surprising to people
 just how good the technology is.
 Now, what we see, Brad, when we talk to enterprise customers
 is the use case that we see the most traction with
 is just assistance.
 You've got your free intern.
 And no matter what job function you're in,
 you build a chatbot that does the work that you do.
 And you get your 80% answer to get going with.
 And then you finish it up.
 Is that something that you guys are seeing, too?
 Yeah, in some use cases, there's a little bit
 of that last mile engineering.
 And we have a team that can help customers with that.
 And so we try and do that work in a very hands-on way.
 I think some of that will start to fade
 as the models get better.
 And so there's two things we see.
 Partly is solving for where the model still
 has deficiencies in its capability.
 And then partly is just trying to rig up
 all the context that the model needs to be able to do a job.
 I don't know that the second part will go away.
 The world is very large and messy.
 But I think the first part will--
 people will really feel accelerated
 as the models get better.
 Yeah.
 OK, so obviously, Brad, you guys have these great models,
 the various flavors of GPT, the Power Chat GPT.
 There's a whole tools ecosystem that
 has sprung up around OpenAI, helping people use this stuff.
 I'm curious, for your company, do you
 see part of your mission and your role
 to be a full platform for application developers who
 use this kind of technology now, or do you just
 want to be the provider of the whole model service?
 I think both, if that makes sense.
 The way that we look at it is everything is just
 an abstraction on the intelligence.
 And I think it's just how many layers of abstraction
 do we want to build.
 But we'll build anything that I think
 accelerates the world's ability to start
 to kind of hold the technology and pull intelligence
 into all the nooks and crannies where we think it should be.
 One, I would say, fairly humbling part of my role
 is you start to realize how big the world is
 and how many places there are that we
 could apply this technology.
 And for every ounce of energy I would spend thinking about,
 should we go build something specific as a first party
 application?
 And I kind of remind myself that there's someone out there
 who cares a lot more about a specific problem
 than we ever will, and that that's true 99% of the time.
 And so how do you build a tool set that
 allows for them to go build the technologies and the tools
 and applications that they want to build?
 And then what are the things that we focus on
 as the kind of primitive pieces, the foundational layers that
 will enable them, and also create great user experiences?
 It's interesting because, in a way,
 we are going through the same journey that NVIDIA
 went through in its history in the last few years, which
 is we have a motto at NVIDIA that we
 like to spend our time doing the things that nobody else can do
 and the things that the others can do we let them do.
 Because you feel this sort of responsibility
 that you have an instrument in your hands.
 And your job is to make that instrument as good as possible
 and with as much reach as possible
 and let other people build around it.
 And you've got this amazing instrument, right?
 And I'm sure you feel the sense of responsibility
 that, like you said, you can impact the whole world
 with this instrument.
 So I think it's a very powerful thing.
 And the other thing I was thinking about was,
 Jensen said it in his speech yesterday, too,
 when he was talking about the world's industries,
 and $100 trillion of industries.
 Just obviously, with your background,
 I'm sure you think about that.
 Because in the tech world, for a long time,
 the tech world has been about cost, right?
 Every company has to have an IT department.
 There's a budget for that.
 And it's all about how do I reduce cost, you know?
 Every new technology is disrupted,
 because it's like I'll make something cheaper to do.
 But I think in the domain that you're in,
 and we believe that we're in, it's
 really about new opportunities, new value for companies, right?
 I mean, nobody ever said GDP has to remain flat.
 It's allowed for things to grow.
 So do you guys see it the same way?
 Yeah, we do.
 I think if you kind of look fundamentally
 at what the technology really is,
 it's kind of just this phased scale-up of the ability
 to offload certain tasks to models
 that can learn, that have general learning capability,
 and can get better, predictably better, both with scale,
 but also with more information, more context,
 and more capability.
 And I think that's the exciting part for us,
 is from an enterprise perspective,
 you think about how complex large businesses really are,
 and how much learning there is to be able to say,
 you know what, for this specific thing,
 we actually can offload parts of this workflow
 to an AI that can not only do it at a baseline level,
 but actually can start to do it better over time,
 and increasingly kind of own parts
 of that entire value chain.
 And it just allows people to focus on other things,
 and that's what we see in practice is instead of spending
 two hours sitting there tearing your hair out,
 trying to get the revenue reconciliation to work,
 an AI can kind of explore it and figure it out for you,
 and you just kind of throw compute at the problem,
 and all of a sudden it's solved.
 And that same person that would have otherwise
 spent that time can just go spend their time
 thinking about something more important.
 I say this because I also manage finance, and--
 - Yeah, yeah, I notice how all these examples
 go by the finance in some way.
 - It's on the frame.
 - I'm sure your team is using ChatGPT.
 - They do, yes.
 - That must be.
 I think we all now have problems, right?
 We've got ChatGPT, I mean, that's where I go.
 You know, there's a lot of people here, Brad,
 who come from an enterprise background
 at this conference and in this room, you know,
 and I think the question that's on a lot of people's minds
 is there's all the, you know, the knowledge of the world
 and the internet, et cetera, that obviously your models
 have done a great job of absorbing.
 And then every company has its own sort of repository
 of knowledge in lots of different places.
 And, you know, various people have different angles
 as to how to approach that, obviously there's rags,
 you know, with Nvidia we do a lot of fine-tuning.
 I'm curious as to, for OpenAI, right,
 what is your vision of how enterprise companies
 should really incorporate all of the data
 that they have into the AI process?
 - Yeah, this is one of the, it's the question I get the most.
 This is probably the thing right now
 that I think is kind of the least-solved problem,
 which is to be expected.
 I think we're really early in this phase shift
 and you've got this kind of core technology
 that people are able to poke at and use,
 but the kind of pipelining and the rigging
 of all the infrastructure and systems
 will take some time.
 But I think that that's where,
 what we're starting to see right now
 is people are able to marry
 really interesting repositories of data
 with identification of clear use cases,
 with understanding of how the model can be applied
 to both of those things.
 And you kind of tie those three things together
 and you can get some really good outcomes.
 So an example of this recently that we worked on is,
 we worked with Klarna to work on a customer support use case.
 And Klarna is a very forward-thinking company on AI,
 so they've been kind of doing this for a while.
 But they took, I think, the right approach,
 which was they really started
 with a very specific implementation of the technology
 where they constrained the problem.
 So it was a, they looked at a very small part
 of the workflow with a very specific dataset
 and a very specific implementation of the model.
 They kind of got that piece to work
 and then they kind of just built from there.
 And now it's handling a large swath of the work
 and saving them many, many hours.
 And I think that's kind of the approach we guide toward
 is don't try and overshoot.
 So don't try and swallow the ocean from day one.
 Don't undershoot, meaning don't lack ambition.
 But start with something
 that you can constrain the problem on,
 get it to work, and then scale it up.
 - Yeah, you know, that point you just made,
 I've seen a few interviews you've done, Brad,
 where you've talked about this more than once,
 that you have these meetings with companies
 where they think that somehow genia
 is going to magically make them a better company, right?
 And change their position in the market.
 Whereas it's better to just start with specific use cases,
 get some value out of those, and then go from there, right?
 - Yeah.
 - So as a piece of advice to people in companies
 who are sort of just getting going,
 for example, if I look at Nvidia, Brad,
 we have now, you know, depending on how you count,
 a couple hundred of these rags, chatbots that we're running,
 you know, inside Nvidia for different kind of purposes,
 and we kind of got there organically.
 For somebody who's starting off now,
 would you, how would you recommend that they begin?
 Should, because there's a lot of interest in the companies,
 right, so you're not going to see just one or two,
 they're going to sprout around the company.
 Do you think they should spend some time first,
 sort of thinking through how it's all done
 and picking sort of one way to go,
 or do you think it's better for them to just sprout organically
 and see what happens?
 - Yeah, well, to your earlier point,
 yeah, we spent most of 2023,
 I used to tell our team, we don't really do sales,
 we do therapy.
 We would have, like, somebody come in,
 and it would be like, you know,
 usually a C-level person that was sitting in our conference
 room about five minutes into the meeting,
 like, they would be kind of, like,
 professing all of their problems
 and things that they were worried about,
 and they're like, could AI fix all these things for me,
 and my board wants me to ship something next quarter,
 and usually we have to, like, talk them off the ledge
 a little bit, and, like, get them some water
 and have them come down.
 (laughing)
 Once we get-- - Was there any jobs
 for the session?
 - Not enough.
 But once we get to a real part of the conversation,
 yeah, you know, our perspective on this is
 to kind of do what we, what I just mentioned,
 is really think about, you know,
 where are there places in your business
 where you feel operationally like there's an opportunity
 to improve how you run?
 For a lot of people, that happens to be customer support.
 That's the thing that we hear probably the most frequently.
 No one likes the quality of their customer support
 experience, they spend a lot of money on it,
 it never quite works, it's the thing they hear
 the most customer complaints about.
 And so it happens to be a place that--
 - Now that's pretty horizontal, right?
 Because that's a place to lots of admissions, yeah.
 - Yep, but we tend to recommend kind of a multi-pronged
 approach, so identify two or three areas
 where you have, you know, a real gnarly problem,
 but where, again, you can kind of constrain the problem.
 So support is this workflow that is this kind of,
 it's multiple tasks strung together
 with varying levels of kind of human involvement
 and human engagement, and a lot of data,
 and more context helps, right?
 So you can look for these core ingredients of having,
 again, going back to the kind of the pyramid of like,
 of data, of process, and of model capability,
 and you can figure out what's that first implementation
 look like, and then how do you scale it up from there.
 And so picking a few of those types of projects
 is these kind of more bespoke, platform-based projects.
 And then the other thing we recommend, really,
 is going back to ChatGPT itself as a product,
 is giving, starting to give your teams access
 to the technology.
 This was not something we really were actively thoughtful
 about middle of last year, but as we've deployed ChatGPT,
 as we've had a chance to talk to companies
 that are using it, democratizing access to the tool,
 and just giving people an opportunity to use it.
 It does not have to be in a particularly complex form
 or develop form, but just giving people a chance to say,
 I know what work I have to do.
 I can poke around with this thing enough
 to be able to kind of explore what it's capable of doing,
 and I'll figure out how to find value in its capability,
 helping me do my work better.
 And that happens very organically,
 and it happens all the time.
 And I think companies kind of forget that.
 They want to have this very manicured AI strategy,
 and there's this big company rollout,
 and they want these proprietary chat bots,
 and I think 90% of the value just comes from,
 right now at least, is coming from just giving people
 access to the tools and not thinking too much about it.
 - I think that's a fair point, because the value
 when you try the first time is so obvious
 that you're willing to work through it, right?
 I mean, that's a big thing.
 So Brad, on that front,
 working with these enterprise companies
 in different use cases, you've also rolled out
 your custom model with the GPTs now, right?
 That is very easy for people to build.
 So can you tell the audience a little bit about what that is
 and why you went down that road and how that's going?
 - Yeah, I'll try and contextualize it maybe
 in kind of a broader picture of our strategy.
 So we have this very core intelligence in GPT-4
 and whatever comes next.
 And a lot of where we're spending our time
 is starting to think about how can people make
 that technology, or that those models feel more personal
 to what they're doing, more task-specific,
 improve their performance on any given thing.
 And so a lot of the work we've done in the last few months,
 GPTs, custom models, has been in that direction.
 So you can think of actually GPTs and custom models
 as opposite ends of the kind of customization spectrum.
 GPTs are like that simple, easy way to take chat GPT
 and just basically kind of create a slice of chat GPT use
 that is specific to a given task.
 So if you want to have a model of the chat GPT
 remember certain information,
 be able to call on certain outside data,
 be able to kind of access a PDF or a spreadsheet,
 have a certain personality, be able to use certain tools
 in a predictable, repeatable way.
 You just kind of ask for it.
 You actually can configure a GPT
 without even having to build it.
 You just describe what you want
 and it'll go off and do it.
 And we see that, a huge pull for that
 in the enterprise actually.
 And it's not surprising because people start to figure out
 that these are the workflows
 for which I can use the technology.
 So I'm just gonna go code each of these
 and you can just call it.
 The custom model side is like the full Monte
 other end of the spectrum.
 That is basically us taking GPT-4 or any other model
 and fully trying to figure out how to customize it
 for a specific use case and maximize its performance
 in that use case.
 We do that on a more limited basis.
 Obviously it's time and resource intensive for us
 but we've had tremendous success early on.
 We're still kind of experimenting with this
 but success early on improving the model's capability
 in any number of domains.
 - You know, it is fascinating
 because obviously you guys really started this whole journey
 with a very large capable model
 that is just so surprisingly good at so many things, right?
 And it just gets better.
 And then at the same time, if I look at the last year,
 there's this ecosystem of models that have sprung up
 and they're not as capable as the models
 that you have inside your services at OpenAI.
 But with different ways they're getting better, right?
 And they're specialized at certain things.
 And so how do you see, you know,
 whether it's the larger model is getting larger,
 larger or the smaller models,
 do you see a role for both within an enterprise
 or do you think just the one large model
 used in lots of different ways?
 - Yeah, we do see roles for all.
 I think, you know, the same way,
 my mental model, by the way, on how to think about
 kind of enterprise AI deployment
 is to try and map it as closely as I can
 to how the kind of the modern enterprise is constructed
 from a human capital perspective.
 So the same way that you wouldn't wanna hire,
 you know, 25,000 PhDs to run your company
 because it would be overkill for what it is that you do.
 You may only need five or 10 or whatever you may need.
 You wouldn't wanna take GPTX
 or whatever the kind of latest model is
 to every single problem.
 You may want a diversity of models
 that have specializations of different things
 that are kind of fine tuned for different use cases.
 I suspect over time the models
 will just get generally better.
 So the need to kind of like iterate on them
 and fine tune them and try and make them really good
 at any specific thing, that would dissipate a little bit.
 But you definitely don't need a like four class model
 to solve some of the things that you would need.
 And so one of the things we're actually working on
 is trying to figure out ways to allow people
 to kind of more dynamically pull models in
 for any given use case
 so that they can kind of like distribute that,
 distribute the intelligence a little more.
 But yeah, no, I think, you know,
 you kind of have your intern level model.
 You have your, you know, mid-level manager model.
 You have your senior executive model.
 You have your subject matter expert model.
 - And there's a place for each.
 - And it'll be a diversity of things, but.
 - It actually raises an interesting question.
 You know, I'm sure the audience is thinking
 about asking this question too,
 given who you are and what you do.
 You know, if I were to say on the spectrum of,
 you know, one to 10 capabilities of models,
 where do you think we are today?
 Are we like one out of 10 or are we seven out of 10?
 What do you think?
 - Yeah, you know, I was gonna make one more point
 on the last comment I just made,
 which is the interesting thing about what we do
 and the challenge from where I sit
 in how we deploy the technology in the enterprise
 or anywhere is you have that kind of map of human capital
 and you're trying to kind of map the human capital
 to the model capability basically.
 But the thing that's changing constantly
 is the model capability, that kind of window
 is moving every six months.
 And so all of a sudden, you know,
 the model that was your intern model,
 six months later, it started to look a little bit like
 your, you know, mid-level VP models.
 And that mid-level VP model is starting to look
 a little bit like your, you know,
 your senior director model.
 - Can you just diss the whole bunch of VPs?
 (laughing)
 - Any VPs.
 These are crude analogies.
 But so that's an interesting phenomenon.
 And something that companies have to manage dynamically
 is, and I think it's a net good thing, it's surplus.
 And so, but we spend a lot of time with companies
 thinking about what are we bringing to bear
 by any given problem, and should we be rethinking
 that combination of things as our model capabilities?
 - Yeah, yeah.
 And you'd imagine that, you know,
 it's sort of the new form of IT, right?
 Because in every company, some humans have to figure
 all this stuff out and think through, you know,
 what's being used, right?
 You know, early Brad, when you were talking about the,
 you know, the initial adoption and dealing with some things,
 it just reminds me of, you know, when the iPhone came out
 and there's this general belief that,
 oh, it's good for consumers, but companies are gonna have
 a hard time adopting iPhone because it doesn't have
 this control and that control, and how will IT understand it,
 and doesn't that sound silly now, right?
 So I think let's transition a little bit then
 to what's next, right?
 So one of the things that I see, Brad, in talking to
 some of the more advanced customers,
 and customers are further along in their journey,
 is that they're kind of starting to move this,
 this transition from, a lot of this has been about
 some form of information retrieval.
 At the end of the day, what I'm doing is I've got
 some information and I'm trying to, you know,
 search through it in some fashion, and now the question is,
 can I use this technology more as an agent,
 where I try to do things in my company, right?
 I try to run processes, I try to call into things,
 make actions happen.
 Do you see that in your interactions with people,
 and how do you, where do you think the technology is?
 Because if I've got an assistant and I'm looking at the output,
 you know, there's a human in the loop.
 But if I'm making the thing take actions for me,
 I have to trust it a little further, right?
 So how do you see that?
 - Yeah, you know, this is what I'm excited about,
 is this is in many ways kind of how we think at OpenAI
 about what this technology is useful for
 and how it should be used.
 And in some ways, like we laugh a little bit at like,
 the way that kind of AI implementations work today
 in many cases is like, it is these kind of information
 retrieval based things.
 And like, they're like the world's worst databases
 in some ways.
 They're really slow, they're really expensive,
 they're not 100% accurate, you know, they're getting better.
 But, and so why would you use them as a database?
 Or why would you use them for some sort of kind of
 like high precision recall?
 It feels like a strange way to use these things.
 You know, no judgment.
 But the way that we're really excited
 to see these systems evolve is as reasoning agents.
 So how do you actually basically take the model's
 core capability to extract information from something,
 think about that information,
 and then basically take some sort of, you know,
 be able to kind of like synthesize some sort of insight
 and take action based on that insight.
 So there's two things that have to happen there.
 One is the model's reasoning capability has to improve.
 And two is you have to give it some ability
 to have actuators, to basically take action
 to figure out how to end the world.
 And I think those are kind of the next two waves
 that we're gonna start to see merge is we suspect
 that reasoning is kind of the next area
 that we will start to see the model, you know,
 kind of access of improvement really accelerate on.
 And then also being able to think about
 how do you actually be able to give models the ability
 to work through multi-step problems.
 So I'll give you an example in like healthcare, for example.
 If you can point a model at a medical record and say,
 okay, it can extract the information
 from this medical record,
 today it can do very basic operations,
 it could maybe summarize that information,
 and it could, you know, it could update that information
 based on some sort of input.
 But can you get it to think about that information?
 And if it can think about that information,
 can it actually draw some insights about that information
 in a way that might inform some second step
 or some third step after that?
 - It could help with follow-up with the patient,
 it could help with diagnosis of the disease,
 it can help with placing an order for a prescription.
 It could then complete the loop
 and actually talk to the patient about the prescription
 or when to pick it up from where and how it's dosed
 and when they should take it,
 and then also remind them later on
 that they should be taking it a week later.
 So that's the way that we think about these systems
 on a multi-year basis.
 - And do you think that that's gonna happen
 because the core model is going to become better at that
 or do you see an approach where there's a separate model
 or a separate system that is more built for reasoning
 that complements the existing models?
 - I think today's systems are actually already pretty good.
 And if you, you know, if you go to GPT-4
 and ask it to reason about that hypothetical situation
 and explain its thinking kind of step-by-step,
 it will explain it to you that way.
 And so the action path is known to the model.
 It's now just a question of can it take each step
 of that action path and identify the actual specific thing
 it should go off and do,
 and does it have access to what it needs to do with that?
 - Yeah, that's, it's great to hear you say that
 because we definitely see that beginning to happen
 and obviously the more you all are working with that
 the better it is for everyone, right?
 So, so Brad, I mean, what, from your point of view now,
 obviously we talk about agents a little bit,
 but from your point of view,
 if you had to step back as your company,
 what do you think, if you think of the next, say,
 let's pick one year, three years, five years, okay, maybe,
 in those sort of time frames,
 what do you think is the big thing,
 the big shifts that you're working on
 that that could really change the landscape
 for how people use this technology?
 - I can't tell you that.
 (laughing)
 Yeah, no, I think, well, there are some things I can't say,
 but the things I can say, so, you know,
 I expect, we don't think that there's any,
 we're anywhere near as ceiling on,
 just core capability improvement in the models.
 We think there's a lot of room in future scale-ups,
 and so we're very excited about that.
 I think we're trying to understand
 how to move the models along axes
 that are not just kind of like raw IQ,
 and I think we feel really good
 about where that work is going.
 And then I think from where I sit,
 there's that question that we kind of addressed earlier of,
 what are going to be the standards and frameworks
 and tools through which the world starts
 to kind of like rig up the information
 required for these systems to be useful
 in production and in a deployed setting,
 and so part of it is just building the thing,
 and then part of it is making sure
 that we've got a place and a way to deploy the technology
 that can actually make it useful in production, so.
 - And it was definitely not a fair question,
 and I think you did a very good job handling it,
 and I don't make predictions anymore.
 - Let me ask you the question in a different way.
 Obviously, as a company, you can have a focus on
 just improving the technology as a whole, as you're doing.
 You can have a focus on enterprise customers,
 the industries of the world, the commerce of the world.
 There's a lot of opportunity.
 So what is your mindset and focus?
 Do you feel it's your mission to democratize this
 across all the enterprise companies in the world
 to help them all get to a better point?
 Are you more just focused on the individual
 consumer use cases?
 Because obviously, that's a big benefit to the world, too.
 - Yeah, our mission is literally to ensure
 that the benefits of the technology are broadly distributed,
 and so how do we think about enacting that?
 Well, one is making sure that people
 are able to build on top of it,
 and for the reasons I mentioned earlier
 about how big and messy the world is,
 we're gonna need to do that anyway.
 I think the surfaces will change,
 the abstraction layers will change,
 but the core of it is we will just try and build ways
 for people to use the tools effectively, successfully,
 wherever they wanna use them.
 Greg Rockman, our co-founder, has a nice phrase in this,
 is how do you think about a world where you have AI
 that baked into the economy?
 And the baked-in part is when you kind of decompose
 what that means, and probably reading a little too much
 in his analogy, is you've got all these ingredients
 and you've kinda gotta mix them up
 and then kinda let it sit, and it starts to work.
 And we think about it a lot that way, too,
 is how do we actually go put the technology in place
 and bring these other ingredients to bear
 in a way that once they're mixed up,
 things just start to work differently?
 And so that's how we spend a lot of our time
 trying to enact our mission.
 And then obviously, from a consumer perspective,
 we look at it similarly.
 Chatt GPT is just an abstraction in our own API,
 so we just took a model, said make it better
 at talking to people, and we served it and it worked.
 But it's just a way for people to access it
 that is not through API.
 - Yeah.
 So, Brad, I think it was, what was it,
 back in the '30s or so, '22, when you released Chatt GPT,
 it's gotta have surprised even you what's happened, right?
 Just the level of interest, the uptake, I mean,
 it's a new thing and it just, it just,
 people just got it right away, right?
 Because it was so easy to see what it's impact was, right?
 So, I'm just wondering if you do a little bit
 of a retrospective for us, you know,
 it's been a little more than a year.
 What's your take?
 Has it surprised you in every way?
 Are there any, you know, if you could look back,
 are there any things that could have gone differently,
 any, you know, choices you would have made differently?
 - What about our GPUs, probably.
 No, I think it did surprise us.
 I'll speak kind of on my own behalf here
 a little more than on the company's behalf,
 but we, yeah, you know, we actually thought,
 we did not think that GPT-3 as a model class
 was the model that had kind of crossed the chasm
 in terms of its usefulness in for, you know,
 whether consumer applications or enterprise applications.
 We thought actually GPT-4 would be kind of the first model
 that had crossed that chasm.
 And so we, a lot of our planning processes
 had aligned around the launch of GPT-4,
 which was in March of 2023, so a year ago.
 But we had finished training GPT-4 months before that.
 So we started training GPT-4 in the middle of '22.
 And so we're now kind of two years from that date.
 And so we, yeah, we kind of thought four would be the moment.
 We had to scramble a little bit to accommodate,
 you know, what everyone wanted a little earlier than planned.
 But it's been amazing to see.
 And I think it speaks to something that I think will be true
 in any capacity, regardless of whether you're an enterprise,
 a developer, an individual, which is A,
 the technology has this very innately human characteristic
 to it, is you can kind of hand it to someone
 who's like five years old or 95 years old.
 They can both find a way to use the technology.
 It's very natural.
 And I think that's really important.
 And so how do we push the systems to continue
 to improve on that access, too?
 And then two is being able to kind of lower
 the barrier access.
 And so making sure that it is accessible
 to people around the world.
 That was kind of a thing that we thought we really got right
 with Tachi Petit was making it free.
 And the stories we hear from people in far flungs
 of the world for things that we could only dream up.
 - You can only imagine, right?
 - Yeah, exactly, yeah.
 - You know, that the font you said about it's so human,
 you know, that's something that's quite close
 to a video too, Brad, because, you know, we,
 yes, we do AI, but we're also sort of the graphics company
 a little bit, right?
 And so we really see a lot of opportunity where that,
 firstly, the text interface is so much more human
 than writing code.
 But the audio interface, the visual interface,
 having, you know, we call these avatars,
 where you basically feel like you're talking
 to another entity.
 And of course, at the end of the day,
 there's other AIs that are converting that into text
 that is then going into a regular chat or what have you.
 Do you think that is an opportunity for this technology
 to really expand its reach, you know, on a planetary scale
 because it makes it much easier for humans to interact?
 But do you think that should be a good area
 of research and progress?
 - Yeah, I think that someone born today,
 the relationship they have with computers
 will be kind of unrecognizable
 to anyone sitting in this room.
 They won't know a world where you have to navigate
 like this kind of graphical user interface
 and these hamburger menus and these click down things
 and there's a field, text field you have to fill something in
 and you have to hit submit and then it sends you an email
 and you have to go over to your inbox
 and check if the confirmation email was sent.
 And like, whatever these like miserable situations are
 that we found ourselves in and like,
 I appreciate that it's make do with the tools we have.
 I just think we'll be completely foreign to someone born,
 born today in 10 years, 20 years.
 It reminds me of, I have kids of a few different ages
 and they sort of span the advent of the iPad era
 and my elder boys, I remember this moment where
 they were young and for each of them,
 they were sitting on my lap and I'm working as we all do
 and they're pressing the keys
 and they're trying to press the keys on the keyboard
 to participate with that and then my daughter,
 when she reached that stage and she was two years old,
 what she was trying to do was she took her hand
 and she tried to move it across my screen on the laptop
 because that's the interface she used, right?
 She didn't know the keys were on the box.
 So I think those interfaces are gonna be quite different
 as we go forward.
 - Yeah, in 10 years, hand your kid a laptop from, you know,
 2020 or whatever and watch them talk at it
 and wait for a response and not get one.
 - Not get one, no, exactly, we can do something like that
 and the amazing thing is that your company
 and hopefully our company will have been able to feel
 like we had something to do with that, right?
 So, yeah, it's quite amazing.
 Brad, I think you're, you know,
 I think I speak for everyone in the audience
 when we say, you know, we're very appreciative
 of everything that we've done.
 Can't wait to see you all do that for the world
 and we'll be here watching.
