0:00
are lots of work running Android. And I'm assuming you're somewhat familiar with the Peter architecture. If you're if you're here at this point, then literally thinking about like a thread and anything popped in is a hugely parallel issue. And so the first thing you need to be thinking about whereby confronted with a new problem, they want us to think about how are we going to solve it? It's a powerful machine. You need to be thinking about how to program this. Let's talk about how to parallel algorithms and that might be interesting but the point is, is a lot of time is called model parallelism. technicalities. Power, there's all these different things. But really, when it comes down to it, there's only two people randomly come up with it is that then they're shaped in detail to the same group, fundamentally there's two types of problems and the one about how I break my data down across threads, and the other one, which is how I break my work down and really keep up with the on the right. And you think GPU has nothing to do with it on the left. There's really thrown a quarter million threads that this machine has started to go fast. And so that's not what we're gonna be talking about today. And it's not entirely because the future model and again, I'm assuming if you're here at the ninja edition, or at least somewhat familiar with Walker is going to break things down into 210 References down to a grid of work. That you brought in threads, and then read through the wall. And so the nature of Judo is that it's intrinsically both task parallel and stage. And this is both why three receptacle is powerful, and it's also why you don't have to, you can just sit there and talk to them all the time. And then you're not a ninja if you're doing great, and we'll get into some of the performance. And so, to run at that peak performance, you really have to think about what you do itself is doing when a manager what how, how does it map the work onto those threads? And how can you exploit that take advantage of it, use the knowledge to do more. Right, there's a whole other section of things about how to manage the memory better. For pretty much the last three talks can be learned more about the memory, observing about memory. So now I'm switching over to have actually run. There are a lot of little details. And this is not necessarily going to heat up the performance of machine. But if you get them right, it can really make a difference. So let's start with data parallelism. Because that's something that's very easy to understand. Think about that even that possibly not working with it. Right. So I've got these three tasks that you'd see in the pipeline as much hard work and you have to have twice as much work there. And so when you sit there and you run it at one time, the takes five minutes. She takes two minutes, sometimes this this special program to run but by and large, it mostly does. But obviously, it's not invented by that will be bothered to tell you this, because if we look at the problem when I write what happens if I now run a VNC on two GPUs, or at a different point in time. Well, the key thing is a lack of the right size, then joining for half of my double size review. I'm not completely sure, but actually this happens more than you think. I gotta explain why is that this this this happened enough. It's worth thinking about. Because the rules put out the rules of any sequential program says A must finish before being again, even though there's room to be right because the last bit of ad producing the data, the first round of events, and so the second half of the years I will lose the rules you can't beat if you can't find the recipe and so be the same thing you know, be five years bet so if you want to be two, and I get to be in training before I'm gonna be five comes and drops off machine as well. So at this point, my time is still running slow as the slowest threaded be fine, we're just going to be the rest of my GP literally I thought it would be run for a second I've wasted property to you at once. It was 10 that was that was wasted. So it's it really matters because it isn't just the steadily worse. This is something which can turn out to be an asset for significant unless he comes home and tells him to tell the machine back again because he has to get an estimate change that size and this is a prime example. So I've got plans to give you one coupon faster. And this is an experience a lot of people haven't that property growth, necessarily growth before. So the problem that many reasons why this can happen, but one of the key things I want to talk about is the way organizations and so what I mean by a wave here is when you want to create a program in flux, which is exactly the number of blocks that will fill GPU that you might run 1000 block for 10,000 blocks right may run very large programs are there many ways in your program, or only one wave is exactly what fulfill the machine and so it's my opinion, it's a one wave of walks every trench that they run for one second or whatever the time is, and then they're done. But it's aimed just slightly bigger wave. Then obviously the final example, what's going to happen is that last couple of units of a are very gentle machines, the rest of the 99 cent aisle and that one sounds good to run for whatever time it is. And this is as big as a major source of make up. Every time I do provide a way of voting believable Read, Write my chapter this is your burning a killer lots of better information. And again, it's mostly shot Yeah, right. And so with monetization is generally random, right? Unless you intentionally size your blocks, your grid size to particular size, your buffer overrun in some way and then it's hard to predict if you run on different sides and BTUs and map the fact that you need berries. So on average, obviously, your way of quantization is going to affect you. So the very tail of your bread is going to be wasting half as if you for whatever length of time it runs. And that is if we never should go to lunch. Right? So what really is happening in my brain in my mind is that my version just the case where a machine factory wants a most waste of time. That'd be a waste of time and then see it you can see it's going to have a way to do that unless you've done it intentionally. It's not going to provide that perfectly. Right so this is where this is where the organization and thinking about it over a lot over a long period of time, a lot of loss to different kinds of shorter makes a huge difference. Right? So if you have a lot of waves, a little bit at the end is going to be much difference, but it's shockingly common, particularly now. But you only have to speed up your wave. For example, the Python program 10 years ago, the number of vessels on the ship was too deep. Now, when 10 times that, right, so something that ran for, you know, hadn't had that run around for 10 waves, a tiny little bit of the distance 5% Something like that. And now I'm not even up to 50% overhead, because maybe she's bigger unless a lot more work into it. The program that was running beautifully it was a while ago and they just have very mentioned que se last thing. You get so many I see a lot of candles out. And inside CUDA I'm one of the things we pay an enormous amount of attention to is the latencies and the overheads of launching and running these things. Because it is now so common. She should go away we're not going to wait. When we used to the GPU by the way was originally designed with the assumption that there will be 100 waves of rocks and every point everybody wants to happen yet. Right That's incorrect. So already the design point is not one of the programs not running. So the answer is wasn't true ninja says you should wait. If I brought an image like flour up because if I break this down, as you can do a Foursquare image end up representing 16 by 16 tiles. It's very more natural. We all do this. We take our data and we subdivide it down into units would make sense for our data. And then we don't have any voxel hybrid and never be we end up with national waves. The actual thing you shouldn't be doing furniture, you should be mapping your data to your threat model first event. Right? And so, in principle, I didn't have the opportunity to use 122 s&p Five one fitness in public in one way I need 11 and a half tiles where I actually can't have 11 and a half tiles. And so I ended up with 12 child labor that were wasted on that. It might be super interesting to do that. I can alter the size of these tiles on the floor even tiles later without having to waste way too much. But you can do this do this. Do this man. It is absolutely worth it. Also, something new to offer onto the grid striking for people where you we launch on the module keep iterating certainly rather than starting up and shutting down. You can't always do this, by the way, but sometimes I have a constraint share power to side on my title and therefore I cannot randomly fly by that. Right so it's not sure what but they are always

9:59
right there because you tried a few pathological cases where it is not meant to do this. But honestly, it is always better to figure out how much you're going to have if you're programmed to take it but if you have not provided training on how to talk to him for granted and we certainly parameterize your programs. Minor changes make it harder to play the whole bunch of things to make it work. But if this is something which is running format times you can listen to the tape just like getting right and so both thing probably will never get you to 100 Right. So you have to consciously project and then finally the best part of why they are powerful is once we get right to the question, we're going to move on to the other parts of task power. There's a lot more to be said about this is much much harder, but much more productive which by the way cheaper so why is it helpful or illegitimate? Right? When I got down, I was injured working. I went all the way back to me. If I had a second task, an unrelated thing x y Zed instead of A, B and C. Well, it depends on age so I can't use my laptop I've been using less x of course it's in a minute so I can just fill the rest of the gap completely. Okay. Lesson number one or two and go and learn strings and go more students. Right. This is one of the medical reasons why right and so obviously, when a fish back, running is so big and you got to run to make finishes. And then he fills in the gap where Angela will be out pretty much right and that works out very well. And if I look at the time that these things are taking sequential A B and C with little with Wave quantization gaps formed by x, y and Zed run longer than if I have these two things running in parallel to each other. And one of the interesting things about this picture right? I run my end to end time is faster and that means the throughput or machine is higher. But the trying to get to the end of C is longer where latency is and this is why we want to do throughput machine. But as compared to CPU and lengthy machines and video production, the GPUs in the mind pull up multiple threads as fast as possible and amazing as possible. So the future is designed to not really care how C itself finishes. It's looking for both of the things to finish overall in shorter amount of time. And that and that's always the the main point of trying to get latency out of the GPU. That's the problem. Quick make it easy visual experience. Right I can have stream of ABC X, Y and Zed. Furthermore, we're going to go into Linda's is going to mention one streaming tendency the theory is a great, great dependencies. We go through the graph of the dependency cycle what logically speaking was a basic or maybe advanced, right, but that's not the point of this talk. I'm going to talk to you about reasonable ways. This isn't quite the reason for why but obviously, if you can do this, we have two separate sets that says a word nbcf always wanted to be as good as you can and it's pretty easy to do. What happens if we don't have to? What am I I need a lot of independent work to be able to run shell scripts. What I don't have now I have to magic it up. How do I find the dependencies to create in dependency whether you they want to do I how do I create cost power of one. The obvious thing to do is to try to buy a big chunk of work to do I got to half the work right if you go ahead and work out into the brain I have two half gigabytes of work. plenty of work to do nice and easy my number right now, this works wonderfully. If the left hand side, a is completed on the right hand side. Right, which is the case for what would why would why our algorithms were either the one where they were asked we have as much wine on the bottle now you only depend on one predecessor. So there's a straight line dependency. It provided me with where I want to fly through basically anywhere one, and that's absolutely fantastic, except almost nothing. And by the way, there are a few things that are interesting and good enough but largely speaking all the interesting algorithms. Right. I'm speaking many of them have one central area where I'm trying to look left and right and revolutions and thoughts, the derivatives market and they think we all want to do want to teach you very much. And so then I can't be split into two. And I can't just put it in two for obvious reasons, right? Because if I'm going to try and build one unit to see if I'm going to find out depend on five units a B and those five minutes to be will depend on five other units a lead to a nine inch band or a five mil right down the middle. Oh my god like now. I need access like I got a calculator. A but that's living in Barcelona to be deconstructed into a two was run and then then see halfway to be would be to anyway because you get horrible by managing sets of dependencies. I had to do the span out. And then frameworks, compilers or software useless for you because it is hard heart and it's kind of a one hour with helpful but just you know even the three things there it can be very, very difficult to operate and then you do of course you dodged by if you don't do this by saying let's reduce the size of what we're doing to a single word which is which Trump has done downside but at least the voice of horrible fencing off my back. I think we're going to keep more of our data within a given topic topic. Right so if they want to keep the St. Paul Weiss but B depends on a it could be depends on five minutes away. He has to be shorter right? He was rather uncertain not till it's because to be able to stay within whatever I was doing. I can't quite as much be required machine for exactly the same reason. So where I want you to fall because when I think for like Well, now it's even a point, right? Obviously that means that I now have to produce one copy of every table my final data. When she had met with the API, I now need three charts to cover my face, which led up to two I have no more work and there's tougher overlap. Right? There is overlap because we've been looking at more stuff. And if you look at them together that we bind them together, so the units of C are next to each other. There's a lot of extra computation. redundant computation is done. And so I traded independence and not having to deal with the policy, which within western North Carolina or different power system for over computation, which is actually not that bad. There's not that bad, because I've got lots of compute power that I can do with it as well. But also because we practice. The main problem is not 24 slides. I wouldn't do this why? And these are just the little attributes of calculating a couple of elements out of bounds of less than one. So functionally speaking, I can do this with relatively little cost and isolate isolating my car now to not have profitability, which gives me all of those settling things I've talked about before. And so that's very appealing. That's a report by the way. It's a trivial the local software is more intelligent, the driver figure out my find out and how I built the proportions of different data values. And then the problem really involves walking all algorithms. The problem is that when I split them into everything that's when I get to the bottom of a column, calculate my redundant compute. My rent, compute is the entire thing. There's no point spending it and creating more work for myself. I'm not saving anything, because by definition, all the wall everything has to be recompute on West planet twice to wipe it off. So I can't take one of these algorithms and tribulation, many of these types of items like Fourier transform is an example of such a thing where you want to switch to actually turn a 1d and 2d Fourier transform. And it's still efficient to do this. But it's not something you can split your data to figure out the overlap and loss right now. You're actually talking about that a structural change the program, which are the ultimate algorithms, a lot of power programming and actually have to figure out these algorithms and has been and spread them across the world.

19:25
And so the challenge is that they break pipeline and when they break pipelining, what I have is my 14 year old box example I got some things to look for potential thing for the company for maritime I was committed to the tree branch Lead Pipeline. So I was in my pipeline for A B and C. When I bought the whole nine pipeline stops the thought I see if I get this church which should be broken up around the album around the ground that the the outward icons that appear on the walls. And so I've got nice, short units of efficiency, and then synchronization. And this starts to look a lot like what we call model power, which is something that the AI community and not just in terms of traffic in terms of stripping across the GPUs. And the idea is that if I got really complicated from end to end, this was like transforming our whole magic wand. Remember, it'll transform paper, but it's so complicated. Figuring out the fine detail overlaps and planning really hard. And so what we generally do is you you either break into the foundry or you we break it up into a bunch of pieces, when you run the pieces are separate tasks, and they run independently for a while. And that's totally great. Actually, that's a very good way of taking somebody very complicated as well. So it's worth actually mentioned in thinking about this problem as well. Because if I look at what I've done here and I just literally sliced into even pieces, I've ended up with their task three is massive bottleneck. It's got like four different interdependencies coming in and that is going to be expensive, because suddenly everybody's busy waiting shortly when someone depends on their output, or it's not ingesting baked yet, and it's not. And so it's really worth looking at your algorithms here and analyzing where the dependencies go, and I can color it and I can split it apart and I can discover that actually I've got various independent pieces with a single connection between right that is, in my mind my task parallel work, suddenly instead of having lots of dependencies, if I bring my task down in this way, there's only one place where the blue side is dependent on the green side. And that's, that's really helpful because now the number of times I've got one thing waited for others review so I can start working around and optimizing around this particular with this particular party. Now obviously, I can take this, this this structure, I can break it down even further away, whatever we're left with the right level one versus four. But if I need to, I can I can split it further. I can hierarchically take two things overlapping divide them again, but I'd really want to be thinking about that but there's a line in yellow between two and three, where I still got a lot more than more than that and work running. You want the independent work that gets your overlap. Right. And this is true whether these tasks are running on a single GPU or multiple GPUs this way of breaking work. Down. Is, is really, this is the way that you should live. So the problem with top panel I mean, they're very, very important about reading and making happen. Once you've got it running well. You've got lots of things going on, which you don't want. And we've got lots of things going on share one tonight that deal with each other for resources. Now, if you've seen any of my previous talks, I'm going to go about memory. Memory efficiency is something that can cost your performance right and so memory results are precious preparation went through rather than nothing, like failure to progress. And so we've got two tasks, and then getting the resources effectively my cash, which is an enormously important resource is hard and fast, right? They kind of work in sand patch, where task one is now 25 megabytes in the GPU, instead of 50. And that can have a really big impact. And it can really have a big impact and really, really, really want to use the cache. I guess I put it on there because we don't want to use my hands. But when did you find we really need to catch because a Catholic hash has tripled the bandwidth of main memory and two and a half times latency reduction. And overall if you have a task that has a very actively loading storing memory and a high cache hit rate, the difference between operating out of cash and operating out of memory can absolutely be ignored. So cash is unbelievably important. And I just handle it by going past. Right? I started as a side note when I was thinking about cash because isn't it struck me it was like quite a long time ago when he finished running the program from diversion into cash. If I got my task A B and C and I'm operating on my on my picture here, you know, everybody generally writes that program. So you start at the top left hand side and you do a rotation for vessel during my breakdown my work into blockchain as the name blockchain has read my work down like what you don't do that raise my hand wash your breath to read from material perspective. I start from the top left. I go down to the bottom right. And then I didn't seem to be traveling very much remain has done its best per last Rose. It's my passion, my passion for whatever life throws wherever and be turns around. It's not the topic and very often it is dependent on a sequence of tasks, right? Presumably a to produce a negative B needs to consume a reasonable and does a computation right not to memory B immediately read this again, right? That's why programs do and it's it reads in the wrong thing by reading the wrong thing. But there's a benefit of reading the first stuff, write included relative to start at the bottom. And so if I want a sequence of operations, A, B and C, and we always write on panels to start at the top left and right, the first one is not the top left, and then this one's for the bottom. Right. It's beautiful. I instantly get massive improvement through bandwidth, lower latencies. A lot of the cost of running the CPU one of the shortcomings that we're talking about is one way to work out. A lot of the cost of the slide was to move my head out of memory. First next last band cache and that's fantastic. By the time the next waves come in my cache, I start to feel I got enough threads ready to cover the latencies in my cache. So just simply making these blocks from the bottom it is so powerful and it's so incredibly easy, right? We all write code that says my neighbor is actually thread eight he must walk anytime flopped, and this is this is the beginning line like he was like all right, all right, I think do the basic plan index don't just change it to take a beat up I need to go negative on even though even though we can't watch it cost me nothing we couldn't program all said that everybody is allowed to run your block in reverse order random order sequentially. And oftentimes, the block isn't completely independently of work. The general program is unchanged. Functionally, there is no functional damage you can do and it was just one line of code. You can take advantage of the fact you got to make my sintashta Starting the AppSumo reducing the cost of the bundle between AMD and now it's no wonder that task parallelism so we don't have a problem at all. But the power of simply don't read the data into a top left every time makes all the difference. In the world. Okay, so we're going to walk down walk conditioning, that's one of my favorite. Obviously, if you only have a roommate, if you have somebody else, just think about that. It's worth it. But that leaves to leave the obvious thought, right if most of my programs I lose my memory bandwidth, right on the LSU cache, to run 10 My father 45 memory regions and programs. I know how to split my program is lost a lot of tasks already. We've gone through the gymnastics you have to do with programs for that when we're not making money but be fantastic. Right? So if my task was smaller at the time then and then be able to cash in and then it sounds like a crazy idea because if I run out with gigabytes and 50 megabytes isn't very much, and so you'd be surprised that it's actually possible, quite a lot of progress. Then I can split my data into these cashable chunks and create one chunk, right? So I got past 123 and four running on my on my PC, my picture. And so what happens is, they go into the cache, and they run in the cache. And so task eight runs first. And in this case, I'm not using task parallelism, right. In this case, my task is running because I want to like cache myself, my God, this might factor 10 fruit but I didn't have the money out of it. Is it still worth it is worth more than 100 pounds. Right? And then I lose my speed. And then I run to see and then I run once again quite a bit. So my four times raise your question, right instead of in parallel. And so this is time this amount of time and it's a technique of your all day well

29:15
first off, and it takes takes a lot to make this work. This is challenging to take program that was not written with this in mind. And to turn it into something that can do this is usually a very significant structural change. First, of course, you haven't figured out all of that. Creating tough power out of nowhere stuff that we've learned magic that we don't talk for, right? Well, that's across dependencies, all that stuff. That's hard. And then you've got to figure out how to take functions. And it's always worth it. Because I have the divided memory and I've opted to maximize cash. I've got a full job and unfortunately, it's not clear that that's going to work. And these tasks, they're going to be small and I'm going to double my salary. I'm not going to take full advantage of all my compute resources. But because most applications in Maryland, this ends up being a win in Norman's number of tests. The challenge is of course, that we've gone through and waste contamination. And that brings us back to the beginning. Not the end of it. So we can do minus 10. We can keep an edge. We can say okay, I'm going to tie myself into the cache. But then I'm going to split everything into so I got two concurrent things running so I even I don't have my way of quantization. Now of course challenges by not finding the half the driveway even smaller tire. But now my Overland is completely sold. I'm running out of cash. I finally got my cash piles working my way organizations, my debit card was working and everything is probably working together for that record. Record washer getting with this query is broken into both tasks. And David Powell taking full advantage of both of those things. So, to recap, there's really two types of tasks that take part in GPU run. We looked at wave quantization, which is really the challenge for data parallel work if you're just launching a work blindly onto the threads, saying just do it in bulk power away, which is a great way to progress nicely. Yes, we've maintained well the answer is that you really need to be thinking about finding a theme. So it's one way it's reading inside that way, rather than just mapping your threads to your data. Right mapping data is your friend that was really the takeaway about fundamentals and then that's not going to cover every base, we had to switch to cash power. And the thing about task parallelism, you've got to work out with complex dependencies. And that is, by far the hardest part of the problem is figuring out this is the first picture, but it's putting things assembled by a factor of five. The reality is that all these various different things we have to do is to count and that is why this is brilliant digitizing. It seems obvious, easy when reading conceptually, or practically functionally speaking, we've got to think about remembering habits. Right? And there's always going to be things which stop you from doing all algorithms which are really so important. Things like number 14, when you make a talk by law of water, and when people found all these great ways and five times giving an interesting bit of paper and not having to hide it again. Because it's such a heavy data that these things are getting in the way it's very difficult to make this work and so really, this leads us to the model parallel types of different ways to have chunks of work they can paralyze and then a big fancy that another chunk and another big kind of thing. And that's how you got another bigger project. You're never going to get single different pipeline work. Right. I have a little aside on my cache of like, just just key spot where running running across from the top left, which was nice, simple effectively you got to do but it's not quite the same because it actually isn't part of the idea of running everything. pipelined pipe your pastor tiling a problem in cash if you solved the second row you solved the problem of these evidences that exists between the length of your program, the ability to time and cash, shockingly, works more times than you would think. And then of course, you can go crazy as crazy as you want. You can start breaking these things and destroy pieces that I think is working great for different pipelines at the same time to try and get different kinds of utilization review it, there's you can take these types of techniques as far as you're able to give them a compact infrastructure program. But really it's funny because I started with talking when I you know I when I when I get disqualified over the course of the year I write down all these things that I'm like, Oh, I have this idea around that idea, and I'll talk about this and that. And I always wait for the things that I like and wait. But actually I sat down for this one and ended up going different directions because I was thinking about not just writing stuff like algorithmic techniques, but we're actually thinking about how things rather than it ended up being much more of a talk about how the GPU standards work. What about tricks that you can do to make our application function as one of this optimization optimization. So this is already preparing for running rate optimization for March. Over time thing about writing is probably version one. Let's talk about big lists. Because I didn't get to and so next year, I'm going to give you a reply to it or something like that. But that's all I got. Thanks very much

34:52
Stephen, thank you very much. Now brings us to the q&a section. There are microphones at the center right here in that aisle. Here.

Transcribed by https://otter.ai