 But before I present the company and where we are differentiating and the kind of models
 that we intend to build and that we have already built, I wanted to give a couple of our vision
 of what happened in the last few years for language models and how that evolved in between
 2023, 2020 and 2019 to where we are studying today with basically assistant that can be deployed in laptops.
 I will try to explain how I saw it unfold because I think it's a pretty interesting story.
 So if you look at basically what kind of LLM you could take into a useful assistant,
 which is, as a scientist you can say that it needs to be higher than 60% on the memory,
 below that it's not useful enough and it's a bit frustrating, beyond that you start to be interacting
 with an assistant or a companion which is good enough and clever enough.
 If you try to look at the sizes of the model that we saw magic this kind of performance,
 so in 2019 actually GPT-3 wasn't at 60% of the memory which was good enough, not far away.
 I was at the time in 2020 at DeepMind where we did a paper group offer which was actually bigger
 and that actually reached the 60% of the memory threshold.
 And then at the time in 2020 there was somehow a race to make it bigger and bigger.
 But that involved the infrastructure challenges that were associated to it.
 So you had to do bike riding, you had to do tensor parallelism, parallelism in every direction.
 And we went to the point where I think that was NDDM that released the 530 billion parameters model.
 And that's not only in 2022.
 The same kind of performance was achieved by a model which was much, much smaller.
 And I'll explain how we actually got to that.
 So that was at DeepMind.
 And so at that point in time I think there was a realization that we didn't need to go to operating models
 and that it was probably not such a great idea.
 And if you look at what happened in 2022, you see that happens.
 Very linked, I guess, to the fact that Chichilla paper actually corrected the scaling loss.
 And then we kind of continued in that direction in 2023.
 We have Lama, Lama 2, and many people at the start of the day.
 And at that point you see this interrogation box that basically says that the field entered into some form of capacity
 and the technology used by the different providers.
 And this is something that kind of kick-started, started the reason why we created it.
 And what we focused on is to try â€“ like the goal we started at the beginning when we started the company
 was to get to 60% of the value with a model that ran on a laptop.
 And that's how we got to Mistral 7b.
 And so what's interesting there is that there's basically a factor of 40 in between Gopher,
 which was the first model that achieved that performance, and Mistral 7b.
 So that was the premise of how we started.
 And I think that I'll explain how I believe this is opening a lot of avenues for interesting implications.
 So it's kind of a biased slide because it ends with Mistral 7b and it starts with 8x7b, but there is a trend there.
 And I'll try to explain exactly how that happens from the scientific perspective.
 So in 2020, the trend was to make those models infinitely big.
 And I guess the starting point for that was a paper from Kaplan and now 2020
 that came with this conclusion that if you were doing a 10x training, if you had a 10x training budget,
 so if you were to go from one week to 10 weeks training for a cluster,
 well you should allocate that to five times the model size and two times the data.
 And as it turns out, the compute that you spend is basically the multiplication of two factors,
 so the size of the model and the size of the data.
 And the conclusion of Kaplan was to say, well if you go to infinity,
 you need to have an infinitely big model that is trained on an amount of data which is much smaller.
 So this premise pushed basically the entire field to train models that were bigger and bigger on the same amount of tokens,
 which happened to be 300 billion tokens.
 So for two years, everybody were training models on 300 billion tokens.
 And kind of, yeah, slow down the field a bit.
 Because at some point, if you train your model on 300 billion tokens, you start saturating.
 You don't see enough tokens. Start over-picking on that side.
 And so that was the problem that we started to solve back in '20, at the end of '21.
 And so we noticed this problem, and I guess the answer to this problem was you needed to train on more than 300 billion tokens.
 And the way we approached the problem at the time was to say, okay, you have a certain compute budget.
 The compute budget is the number of parameters times the number of values.
 And so how do you actually make an appropriate trade-off in between scaling your compute,
 scaling your model size, and scaling the amount of data?
 And as it turns out, and it's kind of obvious on the inside, but it wasn't at the time,
 is that if you do have a 10-times-percent budget, you should basically multiply by square root of 10, the model size,
 and square root of 10, the data.
 And that's actually the only setting in which it doesn't explode when you go to utility.
 So that basically tells you that if you have an infinite amount of compute, you should scale at the same speed as the size of the model,
 the capacity of your representation, and the amount of data points in which you train.
 That means that your compression factor should somehow stay relatively constant.
 And as it turns out, it's approximately like the optimal compression factor is around 30 tokens per market.
 So if you have a certain compute budget, whatever you do, and it kind of depends also on the quality of the data,
 you need to run at 30 tokens per market.
 So that was a good insight.
 But somehow it also directed the field toward a bad representation where everybody started to think that the model should be chinchilla-optimal.
 That was a term coined, I guess, on Twitter.
 Except that if you're chinchilla-optimal with a 7B parameter model, you're basically training on 200 billion tokens.
 And the model that you end up with is like a jet level on MLU, and it doesn't look that good.
 So compute optimality is not great. Training compute optimality is not great if you want to make models that can run on laptops.
 And so I think something that then, like approximately eight months later, there was the LAMA paper, the LAMA effort from Meta,
 that actually showed, I guess some people knew it before, but that showed to the world that you should actually train your model probably more if the model was small.
 So LAMA 7B was probably the first 7B model to be decent, that you had a good performance.
 And the reason why it was good performance is that it was trained like three times more than the compute optimality regime that Chinchilla was advocating for.
 And so I guess what it was saying is that if you have a 10-time training budget, you should consider your inference budget when you make your choices in terms of architecture.
 So if you only consider your training budget, then you should be training something.
 Well, that gives you like 30 tokens per parameter.
 But if you take into account the fact that you're going to deploy your model on many things, well, you should try and make your model smaller.
 So the more you train, the more tokens you see during training, the smaller the model can be for the same kind of performance.
 So you invest some training time to compress your model more, and that's, I guess, part of the recipe that you find to get to 100.
 And so in that sense, I think it's useful to consider this kind of graphs where you compare the model size to the amount of training costs that you should be applying.
 And here what you see, the line, like single line, is basically points that have the same loss.
 And the loss is indicative of the performance.
 And what we call the Chi-Chi-Lao-Ti-Bao at the time is basically the point where you have a noise on that, and these were the Chi-Chi-Lao-Ti-Bao models.
 And then if you say that, OK, I have this model, but I would like to make it smaller.
 And so I want my model to be the same loss as the red one.
 I want it to be smaller.
 I'm going to pay more and more compute.
 So as you move on the way, I see there you're paying more compute.
 So you're paying more Zoloft or you're buying more GPUs.
 And at the end of the day, here you can make a very strong improvement.
 You can move from something which is 100 billion pounds to something that is approximately 10 million pounds.
 So there's a constant compression there.
 And I think that's a very interesting phenomenon.
 It's very predictable in a sense.
 You can measure it.
 You can measure it empirically.
 And I think that's a very important graph because that sets the economics of this variable, basically.
 You need to invest a certain amount of compute to get to a certain size, a certain compression level.
 And that's what you should be optimizing for when you're running information.
 And so I think for a mathematician in the room, the loss here is basically defined by a certain term that you can't get under, which is called the natural density of the text.
 Then you have two terms.
 You have a term that depends on the size of the model.
 So the larger the model, the more representation capacity you have.
 So you can lower the loss by increasing your representation capacity.
 But then you're paying something that is linked to the fact that you're doing stochastic gradient descent,
 and which is basically a stochastic vector that is dependent on the number of things.
 And you have these two things that tells you that to lower the loss, you can either increase the weight or you can increase the number of things.
 And that's by just closing this kind of, yeah, this kind of loss, you end up with such a graph.
 And that's the one that sets the choices you should be making.
 So if you put that into practice, that's really what we did when we started the company too.
 You end up with a model that can be much better than LMA270.
 And so that's how we got to this size 7B.
 We really pushed the compression as much as we could.
 And we ended up with a model that was outperforming LMA230B at the time.
 And then we continued in that direction to get the model, which was called Bistral,
 in December, running only 12 billion parameters, anti-parameters, and having the same kind of performance than LMA270.
 So by pushing this compression and also changing architecture, we went to basically gain a factor of five compared to the existing state of the art.
 And when you gain a factor of five, it's pretty cool because you can deploy it on laptops and you can basically run it five times more.
 So that's an important parameter to take into account.
 And it also explains the movement that we observed in various companies in space.
 So that's what we did. That's the kind of observation that we had when we started the company.
 We wanted it to be efficient and we wanted it to be open.
 So that's released in style 7B in June.
 We released Bistral 87B in December.
 And we're continuing today to progress in the open source world to make the models that are better and better.
 We've also opened, for the optimized models, some commercial models around the platform that sells them.
 It's very similar, I guess, to what many people are doing, except that we propose a platform that is portable, that can be transparent, transparently deployed.
 So that means that we can bring the platform, we can bring the model weights for them to be modified, for our customers to modify them to find them.
 And we have made a variety of strategic partnerships with cloud providers and also with NVIDIA, something that we announced that Jameson announced on Monday.
 And I think the mission that we have is to bring the AI to everybody.
 So make it small enough so that it runs on laptops, make it good enough so that it reaches the best reasoning performances,
 and also make it portable enough so that enterprises, developers can basically work where they're used to work.
 So that can be on cloud, that can be on desktop, that can be on private cloud, that can be on private if needed.
 And oftentimes it is, as usually what you do with NVIDIA, you work on your proprietary data.
 And so there's a lot of compliance aspects to it that we think should, that has pushed us to make the platform.
 So we do offer high models for all use cases and this is it.
 So Mixtral 7B, Mixtral, but we also have the small model, which is actually performing better than Mixtral and lower latency.
 And we have Mixtral, which is currently within the top tier model.
 And we've worked, so beyond the scientific effort that we did until December, we started to work on adding new capacities to the model.
 So it is a multi-lingual, it has function coding.
 Function coding is actually quite important in today's application when you make agents, when you make your model interact with databases, interact with tools.
 We have this JSON load that allows to force the model to output something that you can then use to call an API.
 And we've optimized it for high use cases, so we're optimizing for the model to be able to think about the entire context.
 We also have an ambient model that we did in December, which used to be state-of-the-art, which is still pretty much state-of-the-art.
 It's very fast to be killed even more than in the large language models.
 And yeah, everything is available for our platform, and it's progressively available from a variety of partners.
 It's also available on AI.
 So yeah, I guess that's what I already said.
 And some of, yeah, as I said, some of the performance we are optimizing including for Mr. Large over the latency, over the capacity of the model for, well, to make it small enough to, we have a certain level of performance today.
 Mr. Large has quite low latency compared to others, and we will continue bringing this impression aspect to our technology.
 Because we think that this is the way in which complex applications can be made.
 The reason why latency matters is that once you reach a certain level of latency, so through better hardware, better models, you can start thinking of how your AI can come from the background.
 So instead of having just a system about talking to people, you can have a system that thinks on themselves at a sufficient speed so that you can create applications for the user experience.
 This is really something that we're optimizing.
 And we optimize our privacy, as I said, to track our inputs and optimize our security.
 The system deployment, VPC deployment is basically as secure as you can get.
 We have optimized for guardrailing, so we can define what is appropriate for your specific use case and set basically the guardrails we need in front and we ensure that the dues are respected.
 And because we have been creating our data pretty well, we have had to raise from bias and control, so that our models are demonstrated to be top-of-the-line performance when it comes to gender, religion, politics, and ethnicities biases.
 And so I guess that's basically our core proposition.
 We intend to be the leader in open source, and so we're currently serving the best open source models, and our intention is to continue doing so in the coming months.
 We have better models coming, we have new capacities coming, utility quality is something that is quite important to us as we're pushing forward.
 And as I said, the idea of being open and having very strong open source models is also to enable our customers to modify the model and to fine-tune them to their use cases so that they get better performance and so that the model actually adapts and gets better over time.
 So this is also something that is coming to our platform and to which we will bring our expertise and AI expertise. The IT being to lower the bar of admission to all the fine-tuning models, and so this is coming.
 We're not announcing dates, but this is coming pretty soon.
 So thank you everyone, and I'm happy to take any questions.
 Thank you very much, Arthur. We do have time for a few questions. If you have a question, please step to the microphone that's in the aisle over on the left.
 Are you guys using any synthetically generated data in your models or model building?
 So, yes, we do to a certain extent.
 I thought so.
 So I want to bring something back to a paper you worked on the Retro Transformer DeepLine. You guys were doing retrieval argument in Generation before it was cool. I was wondering if you, and it was integrated stuff in that cross-section.
